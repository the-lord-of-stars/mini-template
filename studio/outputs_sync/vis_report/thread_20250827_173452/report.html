<!DOCTYPE html>
<html lang='en'>
<head>
  <meta charset='utf-8'>
  <meta name='viewport' content='width=device-width, initial-scale=1.0'>
  <title>Research Report: Automated Visualization</title>
  <script src='https://cdn.jsdelivr.net/npm/vega@5'></script>
  <script src='https://cdn.jsdelivr.net/npm/vega-lite@5'></script>
  <script src='https://cdn.jsdelivr.net/npm/vega-embed@6'></script>
  <script src='https://cdn.tailwindcss.com'></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          typography: ({ theme }) => ({
            DEFAULT: { css: { color: theme('colors.gray.800') } }
          })
        }
      }
    }
  </script>
</head>
<body class='bg-white text-gray-800'>
  <div id='progress-bar' class='fixed top-0 left-0 h-1 bg-black z-[9999]' style='width:0%'></div>  <nav class='sticky top-0 bg-white border-b border-gray-200 z-50'>
    <div class='w-[95%] mx-auto px-6 py-3 flex justify-between items-center'>
      <a href='#' class='font-bold text-lg'>Agentic VIS Report</a>
      <div class='hidden md:flex items-center space-x-10 text-sm'>
        <a href='https://www.visagent.org/' target='_blank' class='hover:text-primary-600 text-lg'>Challenge</a>
        <a href='https://github.com/the-lord-of-stars/mini-template' target='_blank'>
          <img src='https://cdnjs.cloudflare.com/ajax/libs/simple-icons/9.16.0/github.svg' alt='GitHub' class='w-6 h-6'>
        </a>
      </div>
      <button id='menu-btn' class='md:hidden flex items-center focus:outline-none'>
        <svg xmlns='http://www.w3.org/2000/svg' class='w-6 h-6' fill='none' viewBox='0 0 24 24' stroke='currentColor'>
          <path stroke-linecap='round' stroke-linejoin='round' stroke-width='2' d='M4 6h16M4 12h16M4 18h16' />
        </svg>
      </button>
    </div>
    <div id='mobile-menu' class='hidden md:hidden bg-white border-t border-gray-200'>
      <div class='px-6 py-3 flex flex-col space-y-4'>
        <a href='https://www.visagent.org/' target='_blank' class='hover:text-primary-600 text-lg'>Official Challenge Website</a>
        <a href='https://github.com/the-lord-of-stars/mini-template' target='_blank' class='flex items-center space-x-4'>
          <img src='https://cdnjs.cloudflare.com/ajax/libs/simple-icons/9.16.0/github.svg' alt='GitHub' class='w-6 h-6'>
          <span class='text-lg'>Project GitHub Repo</span>
        </a>
      </div>
    </div>
  </nav>
  <script>
    const btn = document.getElementById('menu-btn');
    const menu = document.getElementById('mobile-menu');
    btn.addEventListener('click', () => { menu.classList.toggle('hidden'); });
  </script>
  <nav class='hidden lg:block fixed top-20 left-6 w-[15%] h-[80vh] overflow-y-auto
               p-4 text-sm'
       aria-label='Table of contents'>
    <h2 class='font-semibold text-gray-900 mb-3'>Table of contents</h2>
    <ol id='toc-list' class='space-y-2'></ol>
  </nav>
  <main class='w-[90%] max-w-3xl mx-auto px-6 py-12 lg:w-[75%] lg:ml-[20%] lg:max-w-none lg:mx-0'>
    <header class='text-center mb-12'>
      <h1 class='text-4xl font-bold text-gray-900 mb-4'>What happened to research on automated visualization?</h1>
       <p class='text-lg text-gray-600'>Generated on 2025-08-27 </p>
    </header>
    <article class='prose prose-lg prose-gray'>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    1. Executive summary & key findings
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">We identified AutoVis papers by matching keywords (automatic vis | automated vis | visualization recommendation | mixed initiative | mixed-initiative | visualization generation | vis generation | agent) across titles, abstracts and author keywords. Across the Vis corpus subset we analyzed, automated-visualization work is not a niche: it makes up the vast majority of the selected records and shows clear inflection points around mid-2010s and a concentration of high-impact contributions thereafter. The dominant practical takeaway for researchers is that AutoVis has moved from early system prototypes and rule-based engines toward mixed-initiative tooling, recommendation systems and learning-based automation (including reinforcement learning and, more recently, large language models), and that impact is concentrated in a handful of milestone systems (e.g., Voyager, Draco) that combined clear technical contributions with usable mixed-initiative interfaces.</p>
    </div>
  </div>
</section>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    2. Data, definitions & methods
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">The dataset contains 83 Vis papers with 21 attributes per row: bibliographic fields (title, DOI, link, conference, year, first/last page), textual content (abstract, author keywords), author names and affiliations (raw and deduplicated), and several impact/reproducibility indicators (AminerCitationCount, CitationCount_CrossRef, Downloads_Xplore, Award, GraphicsReplicabilityStamp). AutoVis identification used a keyword filter applied to Title, Abstract and AuthorKeywords; after that we deduplicated author name variants, normalized years to integers, and validated DOI and link coverage. We excluded nothing by default but flagged fields with high missingness (see coverage). Primary metrics for analysis are annual paper counts and shares, citation counts (CrossRef and Aminer), downloads, awards, and any replicability stamps; derived metrics include annualized citations/downloads (total divided by paper age) and composite importance scores for timeline annotation. Planned analyses use time series (stacked areas and share lines), topic extraction (keyword trends, LDA/NMF topic mixtures, Sankey transitions), author-level and institution-level summaries, co-authorship networks and centrality measures, and citation-impact scatterplots to inspect lifecycle and influence.</p>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-2-1'></div>
            
  <div id="vis_60b8559f"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "layer": [{"mark": {"type": "bar"}, "encoding": {"color": {"field": "percent_missing", "scale": {"scheme": "redyellowblue"}, "title": "% missing", "type": "quantitative"}, "tooltip": [{"field": "attribute", "title": "Attribute", "type": "nominal"}, {"field": "non_missing", "title": "Non-missing", "type": "quantitative"}, {"field": "unique", "title": "Unique values", "type": "quantitative"}, {"field": "percent_missing", "format": ".1f", "title": "% missing", "type": "quantitative"}], "x": {"field": "non_missing", "title": "Non-missing count", "type": "quantitative"}, "y": {"field": "attribute", "sort": {"field": "non_missing", "order": "descending"}, "title": "Attribute", "type": "nominal"}}}, {"mark": {"type": "text", "align": "left", "baseline": "middle", "color": "black", "dx": 3}, "encoding": {"text": {"field": "non_missing", "type": "quantitative"}, "x": {"field": "non_missing", "type": "quantitative"}, "y": {"field": "attribute", "sort": {"field": "non_missing", "order": "descending"}, "type": "nominal"}}}], "data": {"name": "data-4ecec1b01ee8b7c100fbf97c0c21b6f3"}, "height": 600, "title": "Attribute coverage and completeness (non-missing, unique values, % missing)", "width": 800, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-4ecec1b01ee8b7c100fbf97c0c21b6f3": [{"attribute": "Conference", "non_missing": 83, "unique": 4, "percent_missing": 0.0}, {"attribute": "Abstract", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "Downloads_Xplore", "non_missing": 83, "unique": 82, "percent_missing": 0.0}, {"attribute": "PubsCited_CrossRef", "non_missing": 83, "unique": 54, "percent_missing": 0.0}, {"attribute": "CitationCount_CrossRef", "non_missing": 83, "unique": 44, "percent_missing": 0.0}, {"attribute": "AuthorAffiliation", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "Year", "non_missing": 83, "unique": 21, "percent_missing": 0.0}, {"attribute": "AuthorNames-Deduped", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "AuthorNames", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "PaperType", "non_missing": 83, "unique": 3, "percent_missing": 0.0}, {"attribute": "FirstPage", "non_missing": 83, "unique": 80, "percent_missing": 0.0}, {"attribute": "Link", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "DOI", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "Title", "non_missing": 83, "unique": 83, "percent_missing": 0.0}, {"attribute": "AutoVis_Flag", "non_missing": 83, "unique": 1, "percent_missing": 0.0}, {"attribute": "LastPage", "non_missing": 82, "unique": 82, "percent_missing": 1.2048192771084338}, {"attribute": "InternalReferences", "non_missing": 79, "unique": 79, "percent_missing": 4.819277108433735}, {"attribute": "AuthorKeywords", "non_missing": 77, "unique": 77, "percent_missing": 7.228915662650602}, {"attribute": "AminerCitationCount", "non_missing": 58, "unique": 41, "percent_missing": 30.120481927710845}, {"attribute": "Award", "non_missing": 10, "unique": 2, "percent_missing": 87.95180722891565}, {"attribute": "GraphicsReplicabilityStamp", "non_missing": 0, "unique": 0, "percent_missing": 100.0}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_60b8559f", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The attribute coverage chart shows that the dataset is largely complete for core bibliographic and textual fields: Conference, Year, Title, DOI, Link, Abstract and author names are fully present (non-missing = 83). Metrics that are less complete include AminerCitationCount (only 58 non-missing, ~30% missing) and AuthorKeywords (77 non-missing, ~7% missing). Critically, Award is extremely sparse (only 10 non-missing, ~88% missing) and GraphicsReplicabilityStamp is entirely missing (100% missing). This means analyses that rely on award signals or explicit reproducibility stamps will be unreliable without supplementation; citation and download fields are usable but Aminer counts require special handling for missing values.</p>
        </p>
      </div>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-2-2'></div>
            
  <div id="vis_2a1c8c37"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "vconcat": [{"data": {"name": "data-142cf199ba1cb88f1b2e430186721d41"}, "mark": {"type": "bar"}, "encoding": {"color": {"field": "AutoVis_label", "title": "Label", "type": "nominal"}, "tooltip": [{"field": "Year", "type": "ordinal"}, {"field": "AutoVis_label", "type": "nominal"}, {"field": "count", "type": "quantitative"}, {"field": "proportion", "format": ".2f", "type": "quantitative"}], "x": {"field": "Year", "sort": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], "title": "Year", "type": "ordinal"}, "y": {"field": "count", "title": "Number of papers", "type": "quantitative"}}, "height": 250, "title": "Papers per Year: AutoVis vs Other"}, {"data": {"name": "data-6f4163941e583b936f3cb443c528dd2d"}, "mark": {"type": "line", "point": true}, "encoding": {"tooltip": [{"field": "Year", "type": "ordinal"}, {"field": "proportion", "format": ".2f", "type": "quantitative"}], "x": {"field": "Year", "sort": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], "title": "Year", "type": "ordinal"}, "y": {"field": "proportion", "title": "Proportion AutoVis", "type": "quantitative"}}, "height": 130, "title": "Proportion of AutoVis papers over time"}, {"data": {"name": "data-cc85e1563b5c13c0c1943ee1f7c0abbc"}, "mark": {"type": "bar"}, "encoding": {"tooltip": [{"field": "keyword", "type": "nominal"}, {"field": "count", "type": "quantitative"}], "x": {"field": "count", "title": "Count", "type": "quantitative"}, "y": {"field": "keyword", "sort": "-x", "title": "Author keyword", "type": "nominal"}}, "height": 300, "title": "Top AuthorKeywords within AutoVis subset"}], "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-142cf199ba1cb88f1b2e430186721d41": [{"Year": 1995, "AutoVis_label": "AutoVis", "count": 2, "total": 2, "proportion": 1.0}, {"Year": 2004, "AutoVis_label": "AutoVis", "count": 2, "total": 2, "proportion": 1.0}, {"Year": 2006, "AutoVis_label": "Other", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2007, "AutoVis_label": "Other", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2008, "AutoVis_label": "Other", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2009, "AutoVis_label": "AutoVis", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2010, "AutoVis_label": "Other", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2011, "AutoVis_label": "Other", "count": 2, "total": 2, "proportion": 1.0}, {"Year": 2012, "AutoVis_label": "AutoVis", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2013, "AutoVis_label": "Other", "count": 1, "total": 1, "proportion": 1.0}, {"Year": 2014, "AutoVis_label": "AutoVis", "count": 2, "total": 4, "proportion": 0.5}, {"Year": 2014, "AutoVis_label": "Other", "count": 2, "total": 4, "proportion": 0.5}, {"Year": 2015, "AutoVis_label": "AutoVis", "count": 4, "total": 4, "proportion": 1.0}, {"Year": 2016, "AutoVis_label": "AutoVis", "count": 3, "total": 6, "proportion": 0.5}, {"Year": 2016, "AutoVis_label": "Other", "count": 3, "total": 6, "proportion": 0.5}, {"Year": 2017, "AutoVis_label": "Other", "count": 2, "total": 2, "proportion": 1.0}, {"Year": 2018, "AutoVis_label": "AutoVis", "count": 3, "total": 4, "proportion": 0.75}, {"Year": 2018, "AutoVis_label": "Other", "count": 1, "total": 4, "proportion": 0.25}, {"Year": 2019, "AutoVis_label": "AutoVis", "count": 1, "total": 3, "proportion": 0.3333333333333333}, {"Year": 2019, "AutoVis_label": "Other", "count": 2, "total": 3, "proportion": 0.6666666666666666}, {"Year": 2020, "AutoVis_label": "AutoVis", "count": 4, "total": 7, "proportion": 0.5714285714285714}, {"Year": 2020, "AutoVis_label": "Other", "count": 3, "total": 7, "proportion": 0.42857142857142855}, {"Year": 2021, "AutoVis_label": "AutoVis", "count": 9, "total": 16, "proportion": 0.5625}, {"Year": 2021, "AutoVis_label": "Other", "count": 7, "total": 16, "proportion": 0.4375}, {"Year": 2022, "AutoVis_label": "AutoVis", "count": 1, "total": 5, "proportion": 0.2}, {"Year": 2022, "AutoVis_label": "Other", "count": 4, "total": 5, "proportion": 0.8}, {"Year": 2023, "AutoVis_label": "AutoVis", "count": 6, "total": 12, "proportion": 0.5}, {"Year": 2023, "AutoVis_label": "Other", "count": 6, "total": 12, "proportion": 0.5}, {"Year": 2024, "AutoVis_label": "AutoVis", "count": 1, "total": 7, "proportion": 0.14285714285714285}, {"Year": 2024, "AutoVis_label": "Other", "count": 6, "total": 7, "proportion": 0.8571428571428571}], "data-6f4163941e583b936f3cb443c528dd2d": [{"Year": 1995, "proportion": 1.0}, {"Year": 2004, "proportion": 1.0}, {"Year": 2009, "proportion": 1.0}, {"Year": 2012, "proportion": 1.0}, {"Year": 2014, "proportion": 0.5}, {"Year": 2015, "proportion": 1.0}, {"Year": 2016, "proportion": 0.5}, {"Year": 2018, "proportion": 0.75}, {"Year": 2019, "proportion": 0.3333333333333333}, {"Year": 2020, "proportion": 0.5714285714285714}, {"Year": 2021, "proportion": 0.5625}, {"Year": 2022, "proportion": 0.2}, {"Year": 2023, "proportion": 0.5}, {"Year": 2024, "proportion": 0.14285714285714285}], "data-cc85e1563b5c13c0c1943ee1f7c0abbc": [{"keyword": "visualization", "count": 8}, {"keyword": "automatic visualization", "count": 7}, {"keyword": "visualization recommendation", "count": 4}, {"keyword": "visual analytics", "count": 2}, {"keyword": "automated visualization design", "count": 2}, {"keyword": "responsive visualization", "count": 2}, {"keyword": "narrative visualization", "count": 2}, {"keyword": "mixed-initiative visual analytics", "count": 2}, {"keyword": "glyph-based visualization", "count": 2}, {"keyword": "information visualization", "count": 2}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_2a1c8c37", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The multi-part figure contrasting AutoVis vs other Vis papers shows that nearly all selected records match the AutoVis keyword filter (80 of 83 in this rendered view), and that AutoVis-labeled work has appeared in most years since the mid-1990s. Year-by-year bars and the proportion line together show that AutoVis often constitutes a very large share of the sampled Vis papers (many years at or near 100% for this filtered collection), with some years exhibiting mixed program content. The inset bar of top author keywords within the AutoVis subset highlights the field’s vocabulary: generic terms like “visualization” are most frequent, and domain-defining phrases appear prominently — “visual analytics”, “visualization recommendation”, “automatic visualization”, and “mixed-initiative visual analytics” — indicating that recommendation, mixed-initiative interaction and automated design have been sustained themes.</p>
        </p>
      </div>
    </div>
  </div>
</section>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    3. Temporal trends: volume, impact, and lifecycle
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">Time-based analysis reveals a sustained and accelerating interest in automated visualization across Vis venues: AutoVis papers become noticeably more numerous and more central to the corpus from the mid-2010s onward, with an annual count peak around 2021. Volume increases were followed by highly visible systems (Voyager in 2015, Draco in 2018) that attracted large citation and download counts, suggesting that impact often lagged slightly behind methodological innovation and tool releases. At the same time the field’s emphasis shifted from isolated algorithmic prototypes toward mixed-initiative and applied systems — recommendation engines, interactive model-steering, and domain-focused deployments — and recent years show diversification into ML-driven automation, reinforcement learning for layout/selection, and LLM-assisted authoring and evaluation.</p>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-3-1'></div>
            
  <div id="vis_652f5cd7"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "vconcat": [{"data": {"name": "data-f9b573119f25497aa2466c43cd926e1d"}, "mark": {"type": "bar"}, "encoding": {"color": {"field": "Category", "scale": {"domain": ["AutoVis", "Other"], "range": ["#1f77b4", "#bbbbbb"]}, "title": "Category", "type": "nominal"}, "tooltip": [{"field": "Year", "type": "ordinal"}, {"field": "Category", "type": "nominal"}, {"field": "Count", "type": "quantitative"}, {"field": "Share", "format": ".1%", "type": "quantitative"}], "x": {"field": "Year", "title": "Year", "type": "ordinal"}, "y": {"field": "Count", "title": "Number of papers", "type": "quantitative"}}, "height": 320, "title": "Annual volume: AutoVis vs Other (counts)", "width": 700}, {"data": {"name": "data-dea6d13eb107b128877ebd0ab127ba4a"}, "mark": {"type": "line", "color": "#1f77b4", "point": true}, "encoding": {"tooltip": [{"field": "Year", "type": "ordinal"}, {"field": "Share", "format": ".1%", "type": "quantitative"}, {"field": "Count", "type": "quantitative"}], "x": {"field": "Year", "title": "Year", "type": "ordinal"}, "y": {"axis": {"format": "%"}, "field": "Share", "title": "Share of AutoVis", "type": "quantitative"}}, "height": 180, "title": "Annual share of AutoVis papers", "width": 700}], "resolve": {"scale": {"x": "shared"}}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-f9b573119f25497aa2466c43cd926e1d": [{"Year": 1995, "Category": "AutoVis", "Count": 2, "Total": 2, "Share": 1.0}, {"Year": 2004, "Category": "Other", "Count": 2, "Total": 2, "Share": 1.0}, {"Year": 2006, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2007, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2008, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2009, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2010, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2011, "Category": "Other", "Count": 2, "Total": 2, "Share": 1.0}, {"Year": 2012, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2013, "Category": "Other", "Count": 1, "Total": 1, "Share": 1.0}, {"Year": 2014, "Category": "Other", "Count": 4, "Total": 4, "Share": 1.0}, {"Year": 2015, "Category": "Other", "Count": 4, "Total": 4, "Share": 1.0}, {"Year": 2016, "Category": "AutoVis", "Count": 1, "Total": 6, "Share": 0.16666666666666666}, {"Year": 2016, "Category": "Other", "Count": 5, "Total": 6, "Share": 0.8333333333333334}, {"Year": 2017, "Category": "Other", "Count": 2, "Total": 2, "Share": 1.0}, {"Year": 2018, "Category": "Other", "Count": 4, "Total": 4, "Share": 1.0}, {"Year": 2019, "Category": "AutoVis", "Count": 1, "Total": 3, "Share": 0.3333333333333333}, {"Year": 2019, "Category": "Other", "Count": 2, "Total": 3, "Share": 0.6666666666666666}, {"Year": 2020, "Category": "AutoVis", "Count": 2, "Total": 7, "Share": 0.2857142857142857}, {"Year": 2020, "Category": "Other", "Count": 5, "Total": 7, "Share": 0.7142857142857143}, {"Year": 2021, "Category": "AutoVis", "Count": 5, "Total": 16, "Share": 0.3125}, {"Year": 2021, "Category": "Other", "Count": 11, "Total": 16, "Share": 0.6875}, {"Year": 2022, "Category": "AutoVis", "Count": 1, "Total": 5, "Share": 0.2}, {"Year": 2022, "Category": "Other", "Count": 4, "Total": 5, "Share": 0.8}, {"Year": 2023, "Category": "AutoVis", "Count": 1, "Total": 12, "Share": 0.08333333333333333}, {"Year": 2023, "Category": "Other", "Count": 11, "Total": 12, "Share": 0.9166666666666666}, {"Year": 2024, "Category": "AutoVis", "Count": 1, "Total": 7, "Share": 0.14285714285714285}, {"Year": 2024, "Category": "Other", "Count": 6, "Total": 7, "Share": 0.8571428571428571}], "data-dea6d13eb107b128877ebd0ab127ba4a": [{"Year": 1995, "Category": "AutoVis", "Count": 2, "Total": 2, "Share": 1.0}, {"Year": 2016, "Category": "AutoVis", "Count": 1, "Total": 6, "Share": 0.16666666666666666}, {"Year": 2019, "Category": "AutoVis", "Count": 1, "Total": 3, "Share": 0.3333333333333333}, {"Year": 2020, "Category": "AutoVis", "Count": 2, "Total": 7, "Share": 0.2857142857142857}, {"Year": 2021, "Category": "AutoVis", "Count": 5, "Total": 16, "Share": 0.3125}, {"Year": 2022, "Category": "AutoVis", "Count": 1, "Total": 5, "Share": 0.2}, {"Year": 2023, "Category": "AutoVis", "Count": 1, "Total": 12, "Share": 0.08333333333333333}, {"Year": 2024, "Category": "AutoVis", "Count": 1, "Total": 7, "Share": 0.14285714285714285}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_652f5cd7", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The stacked-area and share visualization documents annual production and the AutoVis share of program content. The data shows an upward trend in absolute AutoVis volume with a clear peak in 2021 (the largest number of AutoVis papers in a single year) and numerous earlier years dominated by AutoVis topics in the filtered set. Year-to-year volatility is visible (for example, a large increase in 2023 and a sharp decline in 2022 in this subset), indicating that program composition and keyword labeling can swing by year; nonetheless the medium-term trend is an increased presence of AutoVis work within Vis venues. The share plot confirms that AutoVis was a large fraction of the selected records across many years, although the percent share fluctuates as overall conference programs change.</p>
        </p>
      </div>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-3-2'></div>
            
  <div id="vis_ef73a860"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "vconcat": [{"mark": {"type": "circle", "opacity": 0.8}, "encoding": {"color": {"field": "AutoVis", "title": "Type", "type": "nominal"}, "size": {"field": "CitationCount_CrossRef", "scale": {"range": [20, 300]}, "title": "Total citations", "type": "quantitative"}, "tooltip": [{"field": "Title", "title": "Title", "type": "nominal"}, {"field": "Year", "title": "Year", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "title": "Total citations", "type": "quantitative"}, {"field": "Downloads_Xplore", "title": "Total downloads", "type": "quantitative"}, {"field": "annual_citations", "title": "Annualized citations", "type": "quantitative"}, {"field": "annual_downloads", "title": "Annualized downloads", "type": "quantitative"}], "x": {"field": "age", "title": "Paper age (years)", "type": "quantitative"}, "y": {"field": "annual_citations", "title": "Annualized citations", "type": "quantitative"}}, "height": 260, "title": "Impact: Annualized citations vs Age (AutoVis vs Other)"}, {"mark": {"type": "circle", "opacity": 0.8}, "encoding": {"color": {"field": "AutoVis", "title": "Type", "type": "nominal"}, "size": {"field": "Downloads_Xplore", "scale": {"range": [20, 300]}, "title": "Total downloads", "type": "quantitative"}, "tooltip": [{"field": "Title", "title": "Title", "type": "nominal"}, {"field": "Year", "title": "Year", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "title": "Total citations", "type": "quantitative"}, {"field": "Downloads_Xplore", "title": "Total downloads", "type": "quantitative"}, {"field": "annual_citations", "title": "Annualized citations", "type": "quantitative"}, {"field": "annual_downloads", "title": "Annualized downloads", "type": "quantitative"}], "x": {"field": "age", "title": "Paper age (years)", "type": "quantitative"}, "y": {"field": "annual_downloads", "title": "Annualized downloads", "type": "quantitative"}}, "height": 260, "title": "Lifecycle: Annualized downloads vs Age (AutoVis vs Other)"}], "data": {"name": "data-610efd77bdabd43c999ef66a49de6e41"}, "resolve": {"scale": {"size": "independent"}}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-610efd77bdabd43c999ef66a49de6e41": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 31, "annual_citations": 0.16129032258064516, "annual_downloads": 4.290322580645161, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 1995, "Title": "Subverting structure: data-driven diagram generation", "DOI": "10.1109/visual.1995.480815", "Link": "http://dx.doi.org/10.1109/VISUAL.1995.480815", "FirstPage": 217.0, "LastPage": null, "PaperType": "C", "Abstract": "Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.", "AuthorNames-Deduped": "Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps", "AuthorNames": "G. Golovchinsky;T. Kamps;K. Reichenberger", "AuthorAffiliation": "Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany", "InternalReferences": "10.1109/infvis.1995.528681;10.1109/infvis.1995.528681", "AuthorKeywords": null, "AminerCitationCount": 21.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 66.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 31, "annual_citations": 0.06451612903225806, "annual_downloads": 2.129032258064516, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2004, "Title": "Non-linear model fitting to parameterize diseased blood vessels", "DOI": "10.1109/visual.2004.72", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.72", "FirstPage": 393.0, "LastPage": 400.0, "PaperType": "C", "Abstract": "Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.", "AuthorNames-Deduped": "Alexandra La Cruz;Mat\u00fas Straka;Arnold K\u00f6chl;Milos Sr\u00e1mek;M. Eduard Gr\u00f6ller;Dominik Fleischmann", "AuthorNames": "A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann", "AuthorAffiliation": "University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA", "InternalReferences": "10.1109/visual.2001.964555", "AuthorKeywords": "Visualization, Segmentation, Blood Vessel Detection", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 141.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 22, "annual_citations": 0.22727272727272727, "annual_downloads": 6.409090909090909, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2004, "Title": "Context-Adaptive Mobile Visualization and Information Management", "DOI": "10.1109/visual.2004.19", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.19", "FirstPage": 8.0, "LastPage": 8.0, "PaperType": "M", "Abstract": "This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.", "AuthorNames-Deduped": "Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen", "AuthorNames": "J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen", "AuthorAffiliation": "Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 11.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 4.0, "Downloads_Xplore": 172.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 22, "annual_citations": 0.09090909090909091, "annual_downloads": 7.818181818181818, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 20, "annual_citations": 1.2, "annual_downloads": 23.6, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 19, "annual_citations": 2.3157894736842106, "annual_downloads": 35.05263157894737, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2008, "Title": "Multi-Focused Geospatial Analysis Using Probes", "DOI": "10.1109/tvcg.2008.149", "Link": "http://dx.doi.org/10.1109/TVCG.2008.149", "FirstPage": 1165.0, "LastPage": 1172.0, "PaperType": "J", "Abstract": "Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.", "AuthorNames-Deduped": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorNames": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorAffiliation": "UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center", "InternalReferences": "10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102", "AuthorKeywords": "Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes", "AminerCitationCount": 73.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 20.0, "Downloads_Xplore": 648.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 18, "annual_citations": 1.8888888888888888, "annual_downloads": 36.0, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2009, "Title": "Articulate: a conversational interface for visual analytics", "DOI": "10.1109/vast.2009.5333099", "Link": "http://dx.doi.org/10.1109/VAST.2009.5333099", "FirstPage": 233.0, "LastPage": 234.0, "PaperType": "M", "Abstract": "While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.", "AuthorNames-Deduped": "Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau", "AuthorNames": "Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau", "AuthorAffiliation": "Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA", "InternalReferences": "0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148", "AuthorKeywords": null, "AminerCitationCount": 3.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 9.0, "Downloads_Xplore": 267.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 17, "annual_citations": 0.11764705882352941, "annual_downloads": 15.705882352941176, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2010, "Title": "ALIDA: Using machine learning for intent discernment in visual analytics interfaces", "DOI": "10.1109/vast.2010.5650854", "Link": "http://dx.doi.org/10.1109/VAST.2010.5650854", "FirstPage": 223.0, "LastPage": 224.0, "PaperType": "M", "Abstract": "In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.", "AuthorNames-Deduped": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorNames": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorAffiliation": "School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada", "InternalReferences": null, "AuthorKeywords": "artificial intelligence, cognition, intent discernment, volume rendering", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 3.0, "PubsCited_CrossRef": 6.0, "Downloads_Xplore": 391.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 16, "annual_citations": 0.1875, "annual_downloads": 24.4375, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 15, "annual_citations": 0.6, "annual_downloads": 52.0, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2011, "Title": "Exploring agent-based simulations using temporal graphs", "DOI": "10.1109/vast.2011.6102469", "Link": "http://dx.doi.org/10.1109/VAST.2011.6102469", "FirstPage": 271.0, "LastPage": 272.0, "PaperType": "M", "Abstract": "Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.", "AuthorNames-Deduped": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorNames": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorAffiliation": "Tufts University, USA;Tufts University, USA;Tufts University, USA", "InternalReferences": "0.1109/infvis.2005.1532126", "AuthorKeywords": null, "AminerCitationCount": 0.0, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 8.0, "Downloads_Xplore": 163.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 15, "annual_citations": 0.0, "annual_downloads": 10.866666666666667, "AutoVis": "AutoVis"}, {"Conference": "SciVis", "Year": 2012, "Title": "Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization", "DOI": "10.1109/tvcg.2012.203", "Link": "http://dx.doi.org/10.1109/TVCG.2012.203", "FirstPage": 2345.0, "LastPage": 2354.0, "PaperType": "J", "Abstract": "Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.", "AuthorNames-Deduped": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorNames": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorAffiliation": "Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\u00f6ping University, Sweden", "InternalReferences": "10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123", "AuthorKeywords": "Direct volume rendering, transfer functions, vessel visualization", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 34.0, "Downloads_Xplore": 513.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 14, "annual_citations": 1.0, "annual_downloads": 36.642857142857146, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 13, "annual_citations": 11.076923076923077, "annual_downloads": 375.6923076923077, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2014, "Title": "Finding Waldo: Learning about Users from their Interactions", "DOI": "10.1109/tvcg.2014.2346575", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346575", "FirstPage": 1663.0, "LastPage": 1672.0, "PaperType": "J", "Abstract": "Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.", "AuthorNames-Deduped": "Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorNames": "Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorAffiliation": "Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U", "InternalReferences": "10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204", "AuthorKeywords": "User Interactions, Analytic Provenance, Visualization, Applied Machine Learning", "AminerCitationCount": 145.0, "CitationCount_CrossRef": 95.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 2226.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 12, "annual_citations": 7.916666666666667, "annual_downloads": 185.5, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2014, "Title": "Learning Perceptual Kernels for Visualization Design", "DOI": "10.1109/tvcg.2014.2346978", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346978", "FirstPage": 1933.0, "LastPage": 1942.0, "PaperType": "J", "Abstract": "Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.", "AuthorNames-Deduped": "\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorNames": "\u00c7a\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorAffiliation": "Stanford University;Stanford University;University of Washington", "InternalReferences": "10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186", "AuthorKeywords": "Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding", "AminerCitationCount": 129.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 1247.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 12, "annual_citations": 6.666666666666667, "annual_downloads": 103.91666666666667, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2014, "Title": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "DOI": "10.1109/vast.2014.7042485", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042485", "FirstPage": 93.0, "LastPage": 102.0, "PaperType": "C", "Abstract": "Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.", "AuthorNames-Deduped": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorNames": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorAffiliation": "GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt", "InternalReferences": "10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225", "AuthorKeywords": "Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 331.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 12, "annual_citations": 0.6666666666666666, "annual_downloads": 27.583333333333332, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2014, "Title": "An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging", "DOI": "10.1109/vast.2014.7042481", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042481", "FirstPage": 53.0, "LastPage": 62.0, "PaperType": "C", "Abstract": "For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.", "AuthorNames-Deduped": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorNames": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorAffiliation": "VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria", "InternalReferences": "10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569", "AuthorKeywords": "MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 29.0, "Downloads_Xplore": 300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 12, "annual_citations": 0.4166666666666667, "annual_downloads": 25.0, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 11, "annual_citations": 26.545454545454547, "annual_downloads": 391.54545454545456, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2015, "Title": "Mixed-initiative visual analytics using task-driven recommendations", "DOI": "10.1109/vast.2015.7347625", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347625", "FirstPage": 9.0, "LastPage": 16.0, "PaperType": "C", "Abstract": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.", "AuthorNames-Deduped": "Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorNames": "Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486", "AuthorKeywords": "mixed-initiative visual analytics, task modeling, recommender systems, sensemaking", "AminerCitationCount": 36.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 36.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 11, "annual_citations": 2.272727272727273, "annual_downloads": 74.0909090909091, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2015, "Title": "Collaborative visual analysis with RCloud", "DOI": "10.1109/vast.2015.7347627", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347627", "FirstPage": 25.0, "LastPage": 32.0, "PaperType": "C", "Abstract": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.", "AuthorNames-Deduped": "Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorNames": "Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorAffiliation": "Infovisible;University of Arizona;AT&T Labs;AT&T Labs", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185", "AuthorKeywords": "visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 404.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 11, "annual_citations": 0.6363636363636364, "annual_downloads": 36.72727272727273, "AutoVis": "AutoVis"}, {"Conference": "SciVis", "Year": 2015, "Title": "Automated visualization workflow for simulation experiments", "DOI": "10.1109/scivis.2015.7429509", "Link": "http://dx.doi.org/10.1109/SciVis.2015.7429509", "FirstPage": 153.0, "LastPage": 154.0, "PaperType": "M", "Abstract": "Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.", "AuthorNames-Deduped": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorNames": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorAffiliation": "School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 1.0, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 12.0, "Downloads_Xplore": 137.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 11, "annual_citations": 0.09090909090909091, "annual_downloads": 12.454545454545455, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 9.2, "annual_downloads": 224.5, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 5.7, "annual_downloads": 278.1, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2016, "Title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods", "DOI": "10.1109/tvcg.2016.2598544", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598544", "FirstPage": 271.0, "LastPage": 280.0, "PaperType": "J", "Abstract": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.", "AuthorNames-Deduped": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne", "AuthorNames": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory", "InternalReferences": "10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591", "AuthorKeywords": "trust;transparency;familiarity;uncertainty;biological data analysis", "AminerCitationCount": 41.0, "CitationCount_CrossRef": 41.0, "PubsCited_CrossRef": 41.0, "Downloads_Xplore": 1844.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 4.1, "annual_downloads": 184.4, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2016, "Title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations", "DOI": "10.1109/tvcg.2016.2598543", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598543", "FirstPage": 261.0, "LastPage": 270.0, "PaperType": "J", "Abstract": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.", "AuthorNames-Deduped": "Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorNames": "Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorAffiliation": "Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research", "InternalReferences": "10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878", "AuthorKeywords": "Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization", "AminerCitationCount": 39.0, "CitationCount_CrossRef": 33.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 2188.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 3.3, "annual_downloads": 218.8, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2016, "Title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "DOI": "10.1109/tvcg.2016.2598460", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598460", "FirstPage": 121.0, "LastPage": 130.0, "PaperType": "J", "Abstract": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.", "AuthorNames-Deduped": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook", "AuthorNames": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook", "AuthorAffiliation": "Smith College;Smith College;Smith College;Smith College", "InternalReferences": "10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467", "AuthorKeywords": "Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 16.0, "PubsCited_CrossRef": 87.0, "Downloads_Xplore": 978.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 1.6, "annual_downloads": 97.8, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2016, "Title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "DOI": "10.1109/tvcg.2016.2599378", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2599378", "FirstPage": 231.0, "LastPage": 240.0, "PaperType": "J", "Abstract": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.", "AuthorNames-Deduped": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorNames": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology", "InternalReferences": "10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1", "AuthorKeywords": "Centralized matching;matching visualization;interaction techniques;visual analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 32.0, "Downloads_Xplore": 557.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 10, "annual_citations": 0.8, "annual_downloads": 55.7, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2017, "Title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics", "DOI": "10.1109/vast.2017.8585669", "Link": "http://dx.doi.org/10.1109/VAST.2017.8585669", "FirstPage": 104.0, "LastPage": 115.0, "PaperType": "C", "Abstract": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.", "AuthorNames-Deduped": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorNames": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorAffiliation": "Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486", "AuthorKeywords": "cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General", "AminerCitationCount": 115.0, "CitationCount_CrossRef": 70.0, "PubsCited_CrossRef": 80.0, "Downloads_Xplore": 1801.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 9, "annual_citations": 7.777777777777778, "annual_downloads": 200.11111111111111, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2017, "Title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "DOI": "10.1109/tvcg.2017.2745078", "Link": "http://dx.doi.org/10.1109/TVCG.2017.2745078", "FirstPage": 288.0, "LastPage": 297.0, "PaperType": "J", "Abstract": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "AuthorNames-Deduped": "Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorNames": "Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136", "AuthorKeywords": "Mixed-initiative visual analytics,multi-attribute ranking,user interaction", "AminerCitationCount": 0.0, "CitationCount_CrossRef": 52.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1535.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 9, "annual_citations": 5.777777777777778, "annual_downloads": 170.55555555555554, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 8, "annual_citations": 22.125, "annual_downloads": 404.75, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 8, "annual_citations": 15.125, "annual_downloads": 367.75, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 8, "annual_citations": 11.375, "annual_downloads": 358.875, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2018, "Title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "DOI": "10.1109/tvcg.2018.2864769", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "FirstPage": 374.0, "LastPage": 384.0, "PaperType": "J", "Abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "AuthorNames-Deduped": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001", "AuthorNames": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins", "AuthorAffiliation": "Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493", "AuthorKeywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "AminerCitationCount": 47.0, "CitationCount_CrossRef": 40.0, "PubsCited_CrossRef": 69.0, "Downloads_Xplore": 1217.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 8, "annual_citations": 5.0, "annual_downloads": 152.125, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2019, "Title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "DOI": "10.1109/vast47406.2019.8986948", "Link": "http://dx.doi.org/10.1109/VAST47406.2019.8986948", "FirstPage": 46.0, "LastPage": 56.0, "PaperType": "C", "Abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.", "AuthorNames-Deduped": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorNames": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718", "AuthorKeywords": "Machine learning fairness,visual analytics,intersectional bias,subgroup discovery", "AminerCitationCount": 107.0, "CitationCount_CrossRef": 106.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 2108.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 7, "annual_citations": 15.142857142857142, "annual_downloads": 301.14285714285717, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 7, "annual_citations": 10.142857142857142, "annual_downloads": 380.14285714285717, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2019, "Title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "DOI": "10.1109/tvcg.2019.2934654", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "FirstPage": 1001.0, "LastPage": 1011.0, "PaperType": "J", "Abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "AuthorNames-Deduped": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen", "AuthorNames": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen", "AuthorAffiliation": "University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493", "AuthorKeywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "AminerCitationCount": 30.0, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 59.0, "Downloads_Xplore": 1300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 7, "annual_citations": 2.5714285714285716, "annual_downloads": 185.71428571428572, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 13.333333333333334, "annual_downloads": 620.6666666666666, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2020, "Title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "DOI": "10.1109/tvcg.2020.3030467", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "FirstPage": 294.0, "LastPage": 303.0, "PaperType": "J", "Abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "AuthorNames-Deduped": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorNames": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorAffiliation": "Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University", "InternalReferences": "10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487", "AuthorKeywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 36.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 1931.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 6.0, "annual_downloads": 321.8333333333333, "AutoVis": "AutoVis"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics", "DOI": "10.1109/tvcg.2020.3030448", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030448", "FirstPage": 443.0, "LastPage": 452.0, "PaperType": "J", "Abstract": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.", "AuthorNames-Deduped": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810", "AuthorKeywords": "Infographics,automatic visualization", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1004.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 5.166666666666667, "annual_downloads": 167.33333333333334, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2020, "Title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations", "DOI": "10.1109/tvcg.2020.3030387", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030387", "FirstPage": 495.0, "LastPage": 505.0, "PaperType": "J", "Abstract": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.", "AuthorNames-Deduped": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorNames": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorAffiliation": "Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia", "InternalReferences": "10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757", "AuthorKeywords": "visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 28.0, "PubsCited_CrossRef": 81.0, "Downloads_Xplore": 1243.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 4.666666666666667, "annual_downloads": 207.16666666666666, "AutoVis": "AutoVis"}, {"Conference": "VAST", "Year": 2020, "Title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "DOI": "10.1109/tvcg.2020.3030347", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "FirstPage": 1775.0, "LastPage": 1785.0, "PaperType": "J", "Abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "AuthorNames-Deduped": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorNames": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorAffiliation": "Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France", "InternalReferences": "10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477", "AuthorKeywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 17.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 754.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 2.8333333333333335, "annual_downloads": 125.66666666666667, "AutoVis": "AutoVis"}, {"Conference": "SciVis", "Year": 2020, "Title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization", "DOI": "10.1109/tvcg.2020.3030407", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030407", "FirstPage": 806.0, "LastPage": 816.0, "PaperType": "J", "Abstract": "This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorNames": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorAffiliation": "Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz", "InternalReferences": "10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259", "AuthorKeywords": "Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 10.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 530.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 1.6666666666666667, "annual_downloads": 88.33333333333333, "AutoVis": "AutoVis"}, {"Conference": "SciVis", "Year": 2020, "Title": "IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection", "DOI": "10.1109/tvcg.2020.3030389", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030389", "FirstPage": 775.0, "LastPage": 784.0, "PaperType": "J", "Abstract": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.", "AuthorNames-Deduped": "Juraj P\u00e1lenik;Thomas Spengler;Helwig Hauser", "AuthorNames": "Juraj Palenik;Thomas Spengler;Helwig Hauser", "AuthorAffiliation": "University of Bergen;University of Bergen;University of Bergen", "InternalReferences": "10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190", "AuthorKeywords": "visual parameter space exploration,scientific modelling,atmospheric convection", "AminerCitationCount": 1.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 417.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 6, "annual_citations": 0.3333333333333333, "annual_downloads": 69.5, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 13.8, "annual_downloads": 690.4, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 12.4, "annual_downloads": 518.8, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Augmenting Sports Videos with VisCommentator", "DOI": "10.1109/tvcg.2021.3114806", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114806", "FirstPage": 824.0, "LastPage": 834.0, "PaperType": "J", "Abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.", "AuthorNames-Deduped": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu", "AuthorNames": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458", "AuthorKeywords": "Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling", "AminerCitationCount": 19.0, "CitationCount_CrossRef": 42.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2151.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 8.4, "annual_downloads": 430.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Kori: Interactive Synthesis of Text and Charts in Data Documents", "DOI": "10.1109/tvcg.2021.3114802", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114802", "FirstPage": 184.0, "LastPage": 194.0, "PaperType": "J", "Abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.", "AuthorNames-Deduped": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim", "AuthorNames": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim", "AuthorAffiliation": "University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 1308.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 6.8, "annual_downloads": 261.6, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "VizLinter: A Linter and Fixer Framework for Data Visualization", "DOI": "10.1109/tvcg.2021.3114804", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114804", "FirstPage": 206.0, "LastPage": 216.0, "PaperType": "J", "Abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001", "AuthorNames": "Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China", "InternalReferences": "10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166", "AuthorKeywords": "Visualization Linting,Automated Visualization Design,Visualization Optimization", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 32.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 1919.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 6.4, "annual_downloads": 383.8, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation", "DOI": "10.1109/tvcg.2021.3114826", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114826", "FirstPage": 162.0, "LastPage": 172.0, "PaperType": "J", "Abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.", "AuthorNames-Deduped": "Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001", "AuthorNames": "Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang", "AuthorAffiliation": "Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1788.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 6.2, "annual_downloads": 357.6, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms", "DOI": "10.1109/tvcg.2021.3114814", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114814", "FirstPage": 346.0, "LastPage": 356.0, "PaperType": "J", "Abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.", "AuthorNames-Deduped": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorNames": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorAffiliation": "University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Visualization Tools,Visualization Recommendation Algorithms", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1106.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 5.0, "annual_downloads": 221.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Towards Visual Explainable Active Learning for Zero-Shot Classification", "DOI": "10.1109/tvcg.2021.3114793", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114793", "FirstPage": 791.0, "LastPage": 801.0, "PaperType": "J", "Abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.", "AuthorNames-Deduped": "Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang", "AuthorNames": "Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang", "AuthorAffiliation": "College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China", "InternalReferences": "10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818", "AuthorKeywords": "Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 76.0, "Downloads_Xplore": 1775.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 4.8, "annual_downloads": 355.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "DOI": "10.1109/tvcg.2021.3114810", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114810", "FirstPage": 151.0, "LastPage": 161.0, "PaperType": "J", "Abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA", "InternalReferences": "10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437", "AuthorKeywords": "Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 21.0, "PubsCited_CrossRef": 105.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 4.2, "annual_downloads": 163.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs", "DOI": "10.1109/tvcg.2021.3114877", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114877", "FirstPage": 400.0, "LastPage": 410.0, "PaperType": "J", "Abstract": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.", "AuthorNames-Deduped": "Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185", "AuthorKeywords": "Glyph-based visualization,machine learning,automatic visualization", "AminerCitationCount": 10.0, "CitationCount_CrossRef": 19.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1101.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 3.8, "annual_downloads": 220.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks", "DOI": "10.1109/tvcg.2021.3114858", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114858", "FirstPage": 813.0, "LastPage": 823.0, "PaperType": "J", "Abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.", "AuthorNames-Deduped": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau", "AuthorNames": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau", "AuthorAffiliation": "Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States", "InternalReferences": "10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509", "AuthorKeywords": "Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding", "AminerCitationCount": 8.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 830.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 3.0, "annual_downloads": 166.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics", "DOI": "10.1109/tvcg.2021.3114820", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114820", "FirstPage": 486.0, "LastPage": 496.0, "PaperType": "J", "Abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.", "AuthorNames-Deduped": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorNames": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorAffiliation": "Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493", "AuthorKeywords": "transformers,word embeddings,literature review,web scraper,dataset,visual analytics", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 1087.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 3.0, "annual_downloads": 217.4, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "A Mixed-Initiative Approach to Reusing Infographic Charts", "DOI": "10.1109/tvcg.2021.3114856", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114856", "FirstPage": 173.0, "LastPage": 183.0, "PaperType": "J", "Abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.", "AuthorNames-Deduped": "Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732", "AuthorKeywords": "Infographics,Reusable templates,Graphic design,Automatic visualization", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 13.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1211.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 2.6, "annual_downloads": 242.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization", "DOI": "10.1109/tvcg.2021.3114782", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114782", "FirstPage": 129.0, "LastPage": 139.0, "PaperType": "J", "Abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorNames": "Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136", "AuthorKeywords": "Task-oriented insight preservation,responsive visualization", "AminerCitationCount": 6.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 751.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 1.8, "annual_downloads": 150.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Semantic Snapping for Guided Multi-View Visualization Design", "DOI": "10.1109/tvcg.2021.3114860", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114860", "FirstPage": 43.0, "LastPage": 53.0, "PaperType": "J", "Abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \u201caligned\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.", "AuthorNames-Deduped": "Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner", "AuthorNames": "Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner", "AuthorAffiliation": "Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338", "AuthorKeywords": "Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views", "AminerCitationCount": 5.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 896.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 1.4, "annual_downloads": 179.2, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2021, "Title": "Visualization Equilibrium", "DOI": "10.1109/tvcg.2021.3114842", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114842", "FirstPage": 465.0, "LastPage": 474.0, "PaperType": "J", "Abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.", "AuthorNames-Deduped": "Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907", "AuthorKeywords": "Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 647.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 5, "annual_citations": 0.4, "annual_downloads": 129.4, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2022, "Title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "DOI": "10.1109/tvcg.2022.3209447", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209447", "FirstPage": 331.0, "LastPage": 341.0, "PaperType": "J", "Abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.", "AuthorNames-Deduped": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254", "AuthorKeywords": "Glyph-based visualization,metaphor,machine learning,automatic visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 68.0, "Downloads_Xplore": 1095.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 4, "annual_citations": 4.5, "annual_downloads": 273.75, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2022, "Title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning", "DOI": "10.1109/tvcg.2022.3209468", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209468", "FirstPage": 690.0, "LastPage": 700.0, "PaperType": "J", "Abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.", "AuthorNames-Deduped": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorNames": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234", "AuthorKeywords": "Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 83.0, "Downloads_Xplore": 1671.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 4, "annual_citations": 3.5, "annual_downloads": 417.75, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2022, "Title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning", "DOI": "10.1109/tvcg.2022.3209461", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209461", "FirstPage": 95.0, "LastPage": 105.0, "PaperType": "J", "Abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.", "AuthorNames-Deduped": "Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001", "AuthorNames": "Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen", "AuthorAffiliation": "School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543", "AuthorKeywords": "Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 1276.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 4, "annual_citations": 2.0, "annual_downloads": 319.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 4, "annual_citations": 2.0, "annual_downloads": 384.25, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 4, "annual_citations": 1.75, "annual_downloads": 621.25, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback", "DOI": "10.1109/tvcg.2023.3327363", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327363", "FirstPage": 131.0, "LastPage": 141.0, "PaperType": "J", "Abstract": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.", "AuthorNames-Deduped": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorNames": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorAffiliation": "New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774", "AuthorKeywords": "Narrative visualization,visual storytelling,conversational agent", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 816.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 2.3333333333333335, "annual_downloads": 272.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks", "DOI": "10.1109/tvcg.2023.3327170", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327170", "FirstPage": 944.0, "LastPage": 954.0, "PaperType": "J", "Abstract": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.", "AuthorNames-Deduped": "Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorNames": "Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China", "InternalReferences": "0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 665.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 1.6666666666666667, "annual_downloads": 221.66666666666666, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Mystique: Deconstructing SVG Charts for Layout Reuse", "DOI": "10.1109/tvcg.2023.3327354", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327354", "FirstPage": 447.0, "LastPage": 457.0, "PaperType": "J", "Abstract": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.", "AuthorNames-Deduped": "Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001", "AuthorNames": "Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu", "AuthorAffiliation": "University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States", "InternalReferences": "0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877", "AuthorKeywords": "Chart layout,Reuse,Reverse-engineering,Deconstruction", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 481.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.6666666666666666, "annual_downloads": 160.33333333333334, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning", "DOI": "10.1109/tvcg.2023.3327200", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327200", "FirstPage": 1172.0, "LastPage": 1182.0, "PaperType": "J", "Abstract": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.", "AuthorNames-Deduped": "Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001", "AuthorNames": "Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865", "AuthorKeywords": "Time Series Data,Exploratory Visual Analysis,Reinforcement Learning", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 1050.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.6666666666666666, "annual_downloads": 350.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining", "DOI": "10.1109/tvcg.2023.3326939", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326939", "FirstPage": 1227.0, "LastPage": 1237.0, "PaperType": "J", "Abstract": "Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA", "InternalReferences": "0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475", "AuthorKeywords": "Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 82.0, "Downloads_Xplore": 361.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.6666666666666666, "annual_downloads": 120.33333333333333, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco", "DOI": "10.1109/tvcg.2023.3326527", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326527", "FirstPage": 1063.0, "LastPage": 1073.0, "PaperType": "J", "Abstract": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\u2014a framework to model visualization knowledge\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.", "AuthorNames-Deduped": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorNames": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorAffiliation": "University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA", "InternalReferences": "0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814", "AuthorKeywords": "Graphical Perception Studies,Visualization Recommendation Algorithms", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 371.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.6666666666666666, "annual_downloads": 123.66666666666667, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "The Rational Agent Benchmark for Data Visualization", "DOI": "10.1109/tvcg.2023.3326513", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326513", "FirstPage": 338.0, "LastPage": 347.0, "PaperType": "J", "Abstract": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.", "AuthorNames-Deduped": "Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781", "AuthorKeywords": "Evaluation,decision-making,rational agent,scoring rule", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 33.0, "Downloads_Xplore": 434.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.6666666666666666, "annual_downloads": 144.66666666666666, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams", "DOI": "10.1109/tvcg.2023.3326925", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326925", "FirstPage": 562.0, "LastPage": 572.0, "PaperType": "J", "Abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001", "AuthorNames": "Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428", "AuthorKeywords": "Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 78.0, "Downloads_Xplore": 662.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.3333333333333333, "annual_downloads": 220.66666666666666, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization", "DOI": "10.1109/tvcg.2023.3326583", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326583", "FirstPage": 934.0, "LastPage": 943.0, "PaperType": "J", "Abstract": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell", "AuthorNames": "Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization,responsive visualization,mixed-initiative authoring", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 330.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.3333333333333333, "annual_downloads": 110.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Visual Analytics for Understanding Draco's Knowledge Base", "DOI": "10.1109/tvcg.2023.3326912", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326912", "FirstPage": 392.0, "LastPage": 402.0, "PaperType": "J", "Abstract": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.", "AuthorNames-Deduped": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorNames": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorAffiliation": "VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Visual Analytics,Hypergraph visualization,Rule-based recommendation systems", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 53.0, "Downloads_Xplore": 365.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.0, "annual_downloads": 121.66666666666667, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring", "DOI": "10.1109/tvcg.2023.3326585", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326585", "FirstPage": 1128.0, "LastPage": 1138.0, "PaperType": "J", "Abstract": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.", "AuthorNames-Deduped": "Chenglong Wang;John Thompson 0002;Bongshin Lee", "AuthorNames": "Chenglong Wang;John Thompson;Bongshin Lee", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA", "InternalReferences": "0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369", "AuthorKeywords": "AI,visualization authoring,data transformation,programming by example,natural language,large language model", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 63.0, "Downloads_Xplore": 893.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.0, "annual_downloads": 297.6666666666667, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2023, "Title": "Guided Visual Analytics for Image Selection in Time and Space", "DOI": "10.1109/tvcg.2023.3326572", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326572", "FirstPage": 66.0, "LastPage": 75.0, "PaperType": "J", "Abstract": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.", "AuthorNames-Deduped": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorNames": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorAffiliation": "TU Wien, Austria;TU Wien, Austria;TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 37.0, "Downloads_Xplore": 1208.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 3, "annual_citations": 0.0, "annual_downloads": 402.6666666666667, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts", "DOI": "10.1109/tvcg.2024.3456398", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456398", "FirstPage": 481.0, "LastPage": 491.0, "PaperType": "J", "Abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.", "AuthorNames-Deduped": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorNames": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorAffiliation": "University of California, USA;University of California, USA;University of California, USA;University of California, USA", "InternalReferences": "10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878", "AuthorKeywords": "Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 65.0, "Downloads_Xplore": 386.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.5, "annual_downloads": 193.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration", "DOI": "10.1109/tvcg.2024.3456364", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456364", "FirstPage": 547.0, "LastPage": 557.0, "PaperType": "J", "Abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.", "AuthorNames-Deduped": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorNames": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorAffiliation": "Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA", "InternalReferences": "10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798", "AuthorKeywords": "Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 632.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.5, "annual_downloads": 316.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models", "DOI": "10.1109/tvcg.2024.3456320", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456320", "FirstPage": 1301.0, "LastPage": 1311.0, "PaperType": "J", "Abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.", "AuthorNames-Deduped": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001", "AuthorNames": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Visualization evaluation,automatic visualization,,,large language models,benchmark", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 75.0, "Downloads_Xplore": 625.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.5, "annual_downloads": 312.5, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech", "DOI": "10.1109/tvcg.2024.3456358", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456358", "FirstPage": 864.0, "LastPage": 874.0, "PaperType": "J", "Abstract": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.", "AuthorNames-Deduped": "Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorNames": "Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorAffiliation": "Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia", "InternalReferences": "10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275", "AuthorKeywords": "Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 184.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.0, "annual_downloads": 92.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models", "DOI": "10.1109/tvcg.2024.3456350", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456350", "FirstPage": 710.0, "LastPage": 720.0, "PaperType": "J", "Abstract": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\u2014DracoGPT-Rank and DracoGPT-Recommend\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.", "AuthorNames-Deduped": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorNames": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorAffiliation": "University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527", "AuthorKeywords": "Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 378.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.0, "annual_downloads": 189.0, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent", "DOI": "10.1109/tvcg.2024.3456200", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456200", "FirstPage": 23.0, "LastPage": 33.0, "PaperType": "J", "Abstract": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.", "AuthorNames-Deduped": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu", "AuthorNames": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu", "AuthorAffiliation": "Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806", "AuthorKeywords": "Sports visualization,tactic board,,,tactical analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 813.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.0, "annual_downloads": 406.5, "AutoVis": "AutoVis"}, {"Conference": "Vis", "Year": 2024, "Title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations", "DOI": "10.1109/tvcg.2024.3456182", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456182", "FirstPage": 754.0, "LastPage": 764.0, "PaperType": "J", "Abstract": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6", "AuthorNames-Deduped": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorNames": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorAffiliation": "Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA", "InternalReferences": "10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513", "AuthorKeywords": "Visual inference,statistical rationality,,,human-machine collaboration", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 71.0, "Downloads_Xplore": 138.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "age": 2, "annual_citations": 0.0, "annual_downloads": 69.0, "AutoVis": "AutoVis"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_ef73a860", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The citation/download lifecycle scatterplots use per-paper annualized metrics to compare impact by age. Across the set, median AutoVis papers have modest lifetime citations (median ~14) and median annualized citations ≈2, while downloads are large for top systems (median downloads ≈893). Several influential papers stand out by both lifetime cites and annualized rate — Voyager (2015) and Draco (2018) lead the list of high-impact AutoVis work — indicating that a small number of systems drive much of the field’s visibility. Correlations show that newer papers tend to have higher rates (negative correlation between age and annualized downloads), consistent with recent tooling and benchmark releases receiving fast attention; this also argues for using annualized metrics when comparing impact across publication years.</p>
        </p>
      </div>
    </div>
  </div>
</section>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    4. Topic evolution & research themes
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">To study topic evolution we combine AuthorKeywords with title and abstract text, apply light normalization (lowercasing, punctuation removal, token filtering, and a domain stoplist that excludes obvious words like “visualization” and “data”), then build TF–IDF representations for mixed-membership topic models (LDA or NMF) and co-occurrence networks for community detection. Practical preprocessing includes merging the three text fields, lemmatization or light stemming, and removing very rare terms (appear in <3 papers). We recommend a hybrid approach: use keyword co-occurrence and community detection to surface interpretable candidate theme labels, then fit an LDA/NMF model for per-paper topic mixtures; investigate k in the 5–9 range and select by coherence and stability. Use rolling 3–5-year windows (3-year when sample size permits, otherwise 5-year) to compute transitions and Sankeys. The working hypothesis is that the field moved from rule- and agent-based diagram generators toward recommendation systems and mixed-initiative interfaces and then increasingly to learning-based generation and LLM-assisted tooling; topic measures will test whether specialization increased or themes consolidated around a few modalities (recommendation, mixed-initiative, generation, agents, evaluation).</p>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-4-1'></div>
            
  <div id="vis_bfda3676"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "data": {"name": "data-cc3a260f1932ea9c3d846d9df2672733"}, "mark": {"type": "area"}, "encoding": {"color": {"field": "term", "title": "Theme (top term)", "type": "nominal"}, "tooltip": [{"field": "Year", "type": "ordinal"}, {"field": "term", "type": "nominal"}, {"field": "count", "type": "quantitative"}], "x": {"field": "Year", "title": "Year", "type": "ordinal"}, "y": {"field": "count", "stack": "normalize", "title": "Proportion of papers", "type": "quantitative"}}, "height": 350, "title": "Prevalence over time of top extracted theme terms (multi-label by term presence)", "width": 700, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-cc3a260f1932ea9c3d846d9df2672733": [{"Year": 1995, "count": 2, "term": "design"}, {"Year": 2010, "count": 1, "term": "design"}, {"Year": 2013, "count": 1, "term": "design"}, {"Year": 2014, "count": 1, "term": "design"}, {"Year": 2015, "count": 3, "term": "design"}, {"Year": 2016, "count": 3, "term": "design"}, {"Year": 2018, "count": 2, "term": "design"}, {"Year": 2019, "count": 1, "term": "design"}, {"Year": 2020, "count": 3, "term": "design"}, {"Year": 2021, "count": 10, "term": "design"}, {"Year": 2022, "count": 1, "term": "design"}, {"Year": 2023, "count": 5, "term": "design"}, {"Year": 2024, "count": 5, "term": "design"}, {"Year": 2007, "count": 1, "term": "user"}, {"Year": 2008, "count": 1, "term": "user"}, {"Year": 2009, "count": 1, "term": "user"}, {"Year": 2010, "count": 1, "term": "user"}, {"Year": 2013, "count": 1, "term": "user"}, {"Year": 2014, "count": 2, "term": "user"}, {"Year": 2015, "count": 2, "term": "user"}, {"Year": 2016, "count": 5, "term": "user"}, {"Year": 2017, "count": 2, "term": "user"}, {"Year": 2018, "count": 2, "term": "user"}, {"Year": 2019, "count": 1, "term": "user"}, {"Year": 2020, "count": 3, "term": "user"}, {"Year": 2021, "count": 7, "term": "user"}, {"Year": 2022, "count": 4, "term": "user"}, {"Year": 2023, "count": 7, "term": "user"}, {"Year": 2024, "count": 1, "term": "user"}, {"Year": 2006, "count": 1, "term": "users"}, {"Year": 2008, "count": 1, "term": "users"}, {"Year": 2009, "count": 1, "term": "users"}, {"Year": 2010, "count": 1, "term": "users"}, {"Year": 2013, "count": 1, "term": "users"}, {"Year": 2014, "count": 1, "term": "users"}, {"Year": 2015, "count": 2, "term": "users"}, {"Year": 2016, "count": 2, "term": "users"}, {"Year": 2017, "count": 2, "term": "users"}, {"Year": 2018, "count": 3, "term": "users"}, {"Year": 2019, "count": 3, "term": "users"}, {"Year": 2020, "count": 4, "term": "users"}, {"Year": 2021, "count": 6, "term": "users"}, {"Year": 2022, "count": 4, "term": "users"}, {"Year": 2023, "count": 4, "term": "users"}, {"Year": 2024, "count": 2, "term": "users"}, {"Year": 2004, "count": 1, "term": "analysis"}, {"Year": 2006, "count": 1, "term": "analysis"}, {"Year": 2007, "count": 1, "term": "analysis"}, {"Year": 2008, "count": 1, "term": "analysis"}, {"Year": 2011, "count": 2, "term": "analysis"}, {"Year": 2014, "count": 3, "term": "analysis"}, {"Year": 2015, "count": 3, "term": "analysis"}, {"Year": 2016, "count": 2, "term": "analysis"}, {"Year": 2018, "count": 1, "term": "analysis"}, {"Year": 2020, "count": 4, "term": "analysis"}, {"Year": 2021, "count": 5, "term": "analysis"}, {"Year": 2022, "count": 4, "term": "analysis"}, {"Year": 2023, "count": 5, "term": "analysis"}, {"Year": 2024, "count": 5, "term": "analysis"}, {"Year": 2008, "count": 1, "term": "visualizations"}, {"Year": 2012, "count": 1, "term": "visualizations"}, {"Year": 2015, "count": 3, "term": "visualizations"}, {"Year": 2018, "count": 1, "term": "visualizations"}, {"Year": 2020, "count": 3, "term": "visualizations"}, {"Year": 2021, "count": 8, "term": "visualizations"}, {"Year": 2022, "count": 2, "term": "visualizations"}, {"Year": 2023, "count": 5, "term": "visualizations"}, {"Year": 2024, "count": 5, "term": "visualizations"}, {"Year": 2006, "count": 1, "term": "analytics"}, {"Year": 2009, "count": 1, "term": "analytics"}, {"Year": 2010, "count": 1, "term": "analytics"}, {"Year": 2011, "count": 1, "term": "analytics"}, {"Year": 2014, "count": 2, "term": "analytics"}, {"Year": 2015, "count": 2, "term": "analytics"}, {"Year": 2016, "count": 3, "term": "analytics"}, {"Year": 2017, "count": 2, "term": "analytics"}, {"Year": 2018, "count": 2, "term": "analytics"}, {"Year": 2019, "count": 2, "term": "analytics"}, {"Year": 2020, "count": 2, "term": "analytics"}, {"Year": 2021, "count": 4, "term": "analytics"}, {"Year": 2023, "count": 3, "term": "analytics"}, {"Year": 2024, "count": 2, "term": "analytics"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_bfda3676", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The theme-term prevalence streamgraph shows which surface terms drive the thematic signal over time. Simple inspection of the top terms indicates that design-related vocabulary (“design”) and user-centered words (“user” / “users”) grow in prominence in later years, while terms like “analysis” and “visualizations” are consistently present. This pattern supports the qualitative shift from purely algorithmic or visualization-engine work toward papers that foreground interaction, user intent, and design guidance — consistent with the rise of mixed-initiative recommendation tools and design knowledge systems in the 2014–2021 window.</p>
        </p>
      </div>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-4-2'></div>
            
  <div id="vis_e20e31f6"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "data": {"name": "data-b58fa8f4c9fd6c858fd7362cab72250a"}, "mark": {"type": "area", "interpolate": "monotone", "line": {"color": "#333"}, "opacity": 0.8}, "encoding": {"color": {"field": "theme", "title": "Theme", "type": "nominal"}, "tooltip": [{"field": "Year", "title": "Year", "type": "ordinal"}, {"field": "theme", "title": "Theme", "type": "nominal"}, {"field": "count", "title": "Count", "type": "quantitative"}, {"field": "prop", "format": ".2f", "title": "Proportion", "type": "quantitative"}], "x": {"field": "Year", "sort": [2004, 2006, 2007, 2008, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], "title": "Year", "type": "ordinal"}, "y": {"field": "prop", "stack": "normalize", "title": "Proportion of papers", "type": "quantitative"}}, "height": 360, "title": "Theme prevalence over time (proportion of papers per year)", "width": 700, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-b58fa8f4c9fd6c858fd7362cab72250a": [{"Year": 2004, "theme": "Other", "count": 2, "year_total": 3, "prop": 0.6666666666666666}, {"Year": 2004, "theme": "visualization", "count": 1, "year_total": 3, "prop": 0.3333333333333333}, {"Year": 2006, "theme": "Other", "count": 5, "year_total": 6, "prop": 0.8333333333333334}, {"Year": 2006, "theme": "visual analytics", "count": 1, "year_total": 6, "prop": 0.16666666666666666}, {"Year": 2007, "theme": "Other", "count": 5, "year_total": 5, "prop": 1.0}, {"Year": 2008, "theme": "Other", "count": 5, "year_total": 5, "prop": 1.0}, {"Year": 2010, "theme": "Other", "count": 4, "year_total": 4, "prop": 1.0}, {"Year": 2011, "theme": "Other", "count": 4, "year_total": 5, "prop": 0.8}, {"Year": 2011, "theme": "visual analytics", "count": 1, "year_total": 5, "prop": 0.2}, {"Year": 2012, "theme": "Other", "count": 3, "year_total": 3, "prop": 1.0}, {"Year": 2013, "theme": "Other", "count": 3, "year_total": 4, "prop": 0.75}, {"Year": 2013, "theme": "visualization recommendation", "count": 1, "year_total": 4, "prop": 0.25}, {"Year": 2014, "theme": "Other", "count": 23, "year_total": 26, "prop": 0.8846153846153846}, {"Year": 2014, "theme": "visualization", "count": 3, "year_total": 26, "prop": 0.11538461538461539}, {"Year": 2015, "theme": "Other", "count": 12, "year_total": 14, "prop": 0.8571428571428571}, {"Year": 2015, "theme": "visualization", "count": 1, "year_total": 14, "prop": 0.07142857142857142}, {"Year": 2015, "theme": "visualization recommendation", "count": 1, "year_total": 14, "prop": 0.07142857142857142}, {"Year": 2016, "theme": "Other", "count": 22, "year_total": 25, "prop": 0.88}, {"Year": 2016, "theme": "visual analytics", "count": 2, "year_total": 25, "prop": 0.08}, {"Year": 2016, "theme": "visualization", "count": 1, "year_total": 25, "prop": 0.04}, {"Year": 2017, "theme": "Other", "count": 8, "year_total": 9, "prop": 0.8888888888888888}, {"Year": 2017, "theme": "visual analytics", "count": 1, "year_total": 9, "prop": 0.1111111111111111}, {"Year": 2018, "theme": "Other", "count": 14, "year_total": 17, "prop": 0.8235294117647058}, {"Year": 2018, "theme": "reinforcement learning", "count": 1, "year_total": 17, "prop": 0.058823529411764705}, {"Year": 2018, "theme": "visual analytics", "count": 1, "year_total": 17, "prop": 0.058823529411764705}, {"Year": 2018, "theme": "visualization recommendation", "count": 1, "year_total": 17, "prop": 0.058823529411764705}, {"Year": 2019, "theme": "Other", "count": 12, "year_total": 14, "prop": 0.8571428571428571}, {"Year": 2019, "theme": "automatic visualization", "count": 1, "year_total": 14, "prop": 0.07142857142857142}, {"Year": 2019, "theme": "visual analytics", "count": 1, "year_total": 14, "prop": 0.07142857142857142}, {"Year": 2020, "theme": "Other", "count": 23, "year_total": 26, "prop": 0.8846153846153846}, {"Year": 2020, "theme": "automatic visualization", "count": 1, "year_total": 26, "prop": 0.038461538461538464}, {"Year": 2020, "theme": "reinforcement learning", "count": 1, "year_total": 26, "prop": 0.038461538461538464}, {"Year": 2020, "theme": "visualization recommendation", "count": 1, "year_total": 26, "prop": 0.038461538461538464}, {"Year": 2021, "theme": "Other", "count": 63, "year_total": 72, "prop": 0.875}, {"Year": 2021, "theme": "automatic visualization", "count": 2, "year_total": 72, "prop": 0.027777777777777776}, {"Year": 2021, "theme": "mixed initiative human-machine analysis", "count": 2, "year_total": 72, "prop": 0.027777777777777776}, {"Year": 2021, "theme": "visual analytics", "count": 2, "year_total": 72, "prop": 0.027777777777777776}, {"Year": 2021, "theme": "visualization", "count": 1, "year_total": 72, "prop": 0.013888888888888888}, {"Year": 2021, "theme": "visualization recommendation", "count": 2, "year_total": 72, "prop": 0.027777777777777776}, {"Year": 2022, "theme": "Other", "count": 16, "year_total": 20, "prop": 0.8}, {"Year": 2022, "theme": "automatic visualization", "count": 1, "year_total": 20, "prop": 0.05}, {"Year": 2022, "theme": "reinforcement learning", "count": 1, "year_total": 20, "prop": 0.05}, {"Year": 2022, "theme": "visualization", "count": 1, "year_total": 20, "prop": 0.05}, {"Year": 2022, "theme": "visualization recommendation", "count": 1, "year_total": 20, "prop": 0.05}, {"Year": 2023, "theme": "Other", "count": 42, "year_total": 49, "prop": 0.8571428571428571}, {"Year": 2023, "theme": "automatic visualization", "count": 1, "year_total": 49, "prop": 0.02040816326530612}, {"Year": 2023, "theme": "mixed initiative human-machine analysis", "count": 2, "year_total": 49, "prop": 0.04081632653061224}, {"Year": 2023, "theme": "reinforcement learning", "count": 1, "year_total": 49, "prop": 0.02040816326530612}, {"Year": 2023, "theme": "visual analytics", "count": 1, "year_total": 49, "prop": 0.02040816326530612}, {"Year": 2023, "theme": "visualization", "count": 2, "year_total": 49, "prop": 0.04081632653061224}, {"Year": 2024, "theme": "Other", "count": 26, "year_total": 30, "prop": 0.8666666666666667}, {"Year": 2024, "theme": "automatic visualization", "count": 1, "year_total": 30, "prop": 0.03333333333333333}, {"Year": 2024, "theme": "visual analytics", "count": 1, "year_total": 30, "prop": 0.03333333333333333}, {"Year": 2024, "theme": "visualization", "count": 1, "year_total": 30, "prop": 0.03333333333333333}, {"Year": 2024, "theme": "visualization recommendation", "count": 1, "year_total": 30, "prop": 0.03333333333333333}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_e20e31f6", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">A higher-level topic-prevalence area plot groups papers into interpretable themes (e.g., ML, Interaction, Generation, AutoVis, Agent). Counts show that machine-learning–related and interaction-driven papers are prominent overall, and the recent window (2020–2024) is characterized by significant shares for AutoVis, Generation and ML themes (each taking roughly 15–25% of the window). Trend slopes indicate clear emergence in AutoVis, Interaction and Generation themes across the series and declines in Evaluation or older agent-centric emphases. Transition statistics (author-level theme movement across windows) suggest some continuity within Generation and increasing movement from evaluation-oriented work toward interaction-focused research, aligning with a community emphasis on creating practical, mixed-initiative tools rather than only evaluating conceptual systems.</p>
        </p>
      </div>
    </div>
  </div>
</section>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    5. People, institutions & influence
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">We map social structure and influence through author productivity, co-authorship networks and institution aggregation, and combine those with impact measures (citations and downloads). Key steps were author-name deduplication, institution string grouping (country-level aggregation for overview), and construction of a weighted co-authorship graph (edge weights = coauthorship frequency). Primary measures are weighted degree, betweenness and eigenvector centrality for the network; per-author paper counts and mean citations/downloads for productivity; and institution counts and summed citations for geography and organizational influence. These views reveal whether AutoVis progress is concentrated in a few labs or broadly distributed across many groups, and whether cross-institutional or cross-country collaborations seed influential systems.</p>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-5-1'></div>
            
  <div id="vis_b9edd2a3"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "vconcat": [{"hconcat": [{"data": {"name": "data-1728a491845b32676ad3679dad394475"}, "mark": {"type": "bar"}, "encoding": {"color": {"field": "mean_citations", "scale": {"scheme": "blues"}, "title": "Avg citations", "type": "quantitative"}, "tooltip": [{"field": "Author", "type": "nominal"}, {"field": "papers", "type": "quantitative"}, {"field": "total_citations", "type": "quantitative"}, {"field": "mean_citations", "type": "quantitative"}], "x": {"field": "papers", "title": "Number of papers", "type": "quantitative"}, "y": {"field": "Author", "sort": ["Alex Endert", "Huamin Qu", "Yingcai Wu", "Nan Cao 0001", "Jeffrey Heer", "Jessica Hullman", "Haidong Zhang", "Dominik Moritz", "Dongmei Zhang 0001", "Emily Wall"], "title": null, "type": "nominal"}}, "height": 300, "title": "Top authors by productivity", "width": 400}, {"data": {"name": "data-6501c0de4576bd9b5438c87c10bbf877"}, "mark": {"type": "bar"}, "encoding": {"color": {"field": "mean_citations", "scale": {"scheme": "greens"}, "title": "Avg citations", "type": "quantitative"}, "tooltip": [{"field": "Institution", "type": "nominal"}, {"field": "papers", "type": "quantitative"}, {"field": "total_citations", "type": "quantitative"}, {"field": "mean_citations", "type": "quantitative"}], "x": {"field": "papers", "title": "Number of papers", "type": "quantitative"}, "y": {"field": "Institution", "sort": ["USA", "Zhejiang University", "United States", "Germany", "China", "Vienna", "University of California", "Hong Kong", "University of Bergen", "Pacific Northwest National Laboratory"], "title": null, "type": "nominal"}}, "height": 300, "title": "Top institutions by productivity", "width": 400}]}, {"data": {"name": "data-1b828f7fb7aad4c853b7f32c3ee12c7d"}, "mark": {"type": "circle", "opacity": 0.7}, "encoding": {"color": {"field": "Citation", "legend": null, "scale": {"scheme": "oranges"}, "type": "quantitative"}, "size": {"field": "Downloads", "scale": {"range": [10, 300]}, "title": "Downloads (Xplore)", "type": "quantitative"}, "tooltip": [{"field": "Title", "type": "nominal"}, {"field": "Year", "type": "quantitative"}, {"field": "Citation", "type": "quantitative"}, {"field": "Downloads", "type": "quantitative"}], "x": {"field": "PaperAge", "title": "Paper age (years)", "type": "quantitative"}, "y": {"field": "Citation", "title": "Citations (CrossRef)", "type": "quantitative"}}, "height": 300, "name": "view_1", "title": "Paper age vs citations (size=downloads)", "width": 820}], "params": [{"name": "param_1", "select": {"type": "interval", "encodings": ["x", "y"]}, "bind": "scales", "views": ["view_1"]}], "resolve": {"scale": {"color": "independent"}}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-1728a491845b32676ad3679dad394475": [{"Author": "Alex Endert", "papers": 7, "total_citations": 436.0, "mean_citations": 62.285714285714285, "total_downloads": 13078.0, "mean_downloads": 1868.2857142857142}, {"Author": "Huamin Qu", "papers": 6, "total_citations": 169.0, "mean_citations": 28.166666666666668, "total_downloads": 10284.0, "mean_downloads": 1714.0}, {"Author": "Yingcai Wu", "papers": 6, "total_citations": 129.0, "mean_citations": 21.5, "total_downloads": 8762.0, "mean_downloads": 1460.3333333333333}, {"Author": "Nan Cao 0001", "papers": 5, "total_citations": 123.0, "mean_citations": 24.6, "total_downloads": 8631.0, "mean_downloads": 1726.2}, {"Author": "Jeffrey Heer", "papers": 5, "total_citations": 551.0, "mean_citations": 110.2, "total_downloads": 9541.0, "mean_downloads": 1908.2}, {"Author": "Jessica Hullman", "papers": 4, "total_citations": 14.0, "mean_citations": 3.5, "total_downloads": 2162.0, "mean_downloads": 540.5}, {"Author": "Haidong Zhang", "papers": 4, "total_citations": 146.0, "mean_citations": 36.5, "total_downloads": 6664.0, "mean_downloads": 1666.0}, {"Author": "Dominik Moritz", "papers": 4, "total_citations": 480.0, "mean_citations": 120.0, "total_downloads": 8667.0, "mean_downloads": 2166.75}, {"Author": "Dongmei Zhang 0001", "papers": 4, "total_citations": 146.0, "mean_citations": 36.5, "total_downloads": 6664.0, "mean_downloads": 1666.0}, {"Author": "Emily Wall", "papers": 3, "total_citations": 137.0, "mean_citations": 45.666666666666664, "total_downloads": 4423.0, "mean_downloads": 1474.3333333333333}], "data-6501c0de4576bd9b5438c87c10bbf877": [{"Institution": "USA", "papers": 20, "total_citations": 767.0, "mean_citations": 11.447761194029852}, {"Institution": "Zhejiang University", "papers": 6, "total_citations": 580.0, "mean_citations": 19.333333333333332}, {"Institution": "United States", "papers": 5, "total_citations": 457.0, "mean_citations": 20.772727272727273}, {"Institution": "Germany", "papers": 5, "total_citations": 462.0, "mean_citations": 35.53846153846154}, {"Institution": "China", "papers": 5, "total_citations": 328.0, "mean_citations": 15.619047619047619}, {"Institution": "Vienna", "papers": 4, "total_citations": 136.0, "mean_citations": 13.6}, {"Institution": "University of California", "papers": 3, "total_citations": 153.0, "mean_citations": 25.5}, {"Institution": "Hong Kong", "papers": 3, "total_citations": 280.0, "mean_citations": 56.0}, {"Institution": "University of Bergen", "papers": 3, "total_citations": 71.0, "mean_citations": 10.142857142857142}, {"Institution": "Pacific Northwest National Laboratory", "papers": 3, "total_citations": 527.0, "mean_citations": 40.53846153846154}], "data-1b828f7fb7aad4c853b7f32c3ee12c7d": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 5.0, "Downloads": 133.0, "PaperAge": 30}, {"Conference": "Vis", "Year": 1995, "Title": "Subverting structure: data-driven diagram generation", "DOI": "10.1109/visual.1995.480815", "Link": "http://dx.doi.org/10.1109/VISUAL.1995.480815", "FirstPage": 217.0, "LastPage": null, "PaperType": "C", "Abstract": "Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.", "AuthorNames-Deduped": "Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps", "AuthorNames": "G. Golovchinsky;T. Kamps;K. Reichenberger", "AuthorAffiliation": "Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany", "InternalReferences": "10.1109/infvis.1995.528681;10.1109/infvis.1995.528681", "AuthorKeywords": null, "AminerCitationCount": 21.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 66.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 66.0, "PaperAge": 30}, {"Conference": "Vis", "Year": 2004, "Title": "Non-linear model fitting to parameterize diseased blood vessels", "DOI": "10.1109/visual.2004.72", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.72", "FirstPage": 393.0, "LastPage": 400.0, "PaperType": "C", "Abstract": "Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.", "AuthorNames-Deduped": "Alexandra La Cruz;Mat\u00fas Straka;Arnold K\u00f6chl;Milos Sr\u00e1mek;M. Eduard Gr\u00f6ller;Dominik Fleischmann", "AuthorNames": "A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann", "AuthorAffiliation": "University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA", "InternalReferences": "10.1109/visual.2001.964555", "AuthorKeywords": "Visualization, Segmentation, Blood Vessel Detection", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 141.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 5.0, "Downloads": 141.0, "PaperAge": 21}, {"Conference": "Vis", "Year": 2004, "Title": "Context-Adaptive Mobile Visualization and Information Management", "DOI": "10.1109/visual.2004.19", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.19", "FirstPage": 8.0, "LastPage": 8.0, "PaperType": "M", "Abstract": "This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.", "AuthorNames-Deduped": "Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen", "AuthorNames": "J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen", "AuthorAffiliation": "Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 11.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 4.0, "Downloads_Xplore": 172.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 172.0, "PaperAge": 21}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 24.0, "Downloads": 472.0, "PaperAge": 19}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 44.0, "Downloads": 666.0, "PaperAge": 18}, {"Conference": "InfoVis", "Year": 2008, "Title": "Multi-Focused Geospatial Analysis Using Probes", "DOI": "10.1109/tvcg.2008.149", "Link": "http://dx.doi.org/10.1109/TVCG.2008.149", "FirstPage": 1165.0, "LastPage": 1172.0, "PaperType": "J", "Abstract": "Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.", "AuthorNames-Deduped": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorNames": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorAffiliation": "UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center", "InternalReferences": "10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102", "AuthorKeywords": "Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes", "AminerCitationCount": 73.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 20.0, "Downloads_Xplore": 648.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 34.0, "Downloads": 648.0, "PaperAge": 17}, {"Conference": "VAST", "Year": 2009, "Title": "Articulate: a conversational interface for visual analytics", "DOI": "10.1109/vast.2009.5333099", "Link": "http://dx.doi.org/10.1109/VAST.2009.5333099", "FirstPage": 233.0, "LastPage": 234.0, "PaperType": "M", "Abstract": "While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.", "AuthorNames-Deduped": "Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau", "AuthorNames": "Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau", "AuthorAffiliation": "Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA", "InternalReferences": "0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148", "AuthorKeywords": null, "AminerCitationCount": 3.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 9.0, "Downloads_Xplore": 267.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 267.0, "PaperAge": 16}, {"Conference": "VAST", "Year": 2010, "Title": "ALIDA: Using machine learning for intent discernment in visual analytics interfaces", "DOI": "10.1109/vast.2010.5650854", "Link": "http://dx.doi.org/10.1109/VAST.2010.5650854", "FirstPage": 223.0, "LastPage": 224.0, "PaperType": "M", "Abstract": "In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.", "AuthorNames-Deduped": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorNames": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorAffiliation": "School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada", "InternalReferences": null, "AuthorKeywords": "artificial intelligence, cognition, intent discernment, volume rendering", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 3.0, "PubsCited_CrossRef": 6.0, "Downloads_Xplore": 391.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 3.0, "Downloads": 391.0, "PaperAge": 15}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 9.0, "Downloads": 780.0, "PaperAge": 14}, {"Conference": "VAST", "Year": 2011, "Title": "Exploring agent-based simulations using temporal graphs", "DOI": "10.1109/vast.2011.6102469", "Link": "http://dx.doi.org/10.1109/VAST.2011.6102469", "FirstPage": 271.0, "LastPage": 272.0, "PaperType": "M", "Abstract": "Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.", "AuthorNames-Deduped": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorNames": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorAffiliation": "Tufts University, USA;Tufts University, USA;Tufts University, USA", "InternalReferences": "0.1109/infvis.2005.1532126", "AuthorKeywords": null, "AminerCitationCount": 0.0, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 8.0, "Downloads_Xplore": 163.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 163.0, "PaperAge": 14}, {"Conference": "SciVis", "Year": 2012, "Title": "Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization", "DOI": "10.1109/tvcg.2012.203", "Link": "http://dx.doi.org/10.1109/TVCG.2012.203", "FirstPage": 2345.0, "LastPage": 2354.0, "PaperType": "J", "Abstract": "Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.", "AuthorNames-Deduped": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorNames": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorAffiliation": "Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\u00f6ping University, Sweden", "InternalReferences": "10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123", "AuthorKeywords": "Direct volume rendering, transfer functions, vessel visualization", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 34.0, "Downloads_Xplore": 513.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 14.0, "Downloads": 513.0, "PaperAge": 13}, {"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 144.0, "Downloads": 4884.0, "PaperAge": 12}, {"Conference": "VAST", "Year": 2014, "Title": "Finding Waldo: Learning about Users from their Interactions", "DOI": "10.1109/tvcg.2014.2346575", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346575", "FirstPage": 1663.0, "LastPage": 1672.0, "PaperType": "J", "Abstract": "Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.", "AuthorNames-Deduped": "Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorNames": "Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorAffiliation": "Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U", "InternalReferences": "10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204", "AuthorKeywords": "User Interactions, Analytic Provenance, Visualization, Applied Machine Learning", "AminerCitationCount": 145.0, "CitationCount_CrossRef": 95.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 2226.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 95.0, "Downloads": 2226.0, "PaperAge": 11}, {"Conference": "InfoVis", "Year": 2014, "Title": "Learning Perceptual Kernels for Visualization Design", "DOI": "10.1109/tvcg.2014.2346978", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346978", "FirstPage": 1933.0, "LastPage": 1942.0, "PaperType": "J", "Abstract": "Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.", "AuthorNames-Deduped": "\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorNames": "\u00c7a\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorAffiliation": "Stanford University;Stanford University;University of Washington", "InternalReferences": "10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186", "AuthorKeywords": "Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding", "AminerCitationCount": 129.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 1247.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 80.0, "Downloads": 1247.0, "PaperAge": 11}, {"Conference": "VAST", "Year": 2014, "Title": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "DOI": "10.1109/vast.2014.7042485", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042485", "FirstPage": 93.0, "LastPage": 102.0, "PaperType": "C", "Abstract": "Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.", "AuthorNames-Deduped": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorNames": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorAffiliation": "GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt", "InternalReferences": "10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225", "AuthorKeywords": "Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 331.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 8.0, "Downloads": 331.0, "PaperAge": 11}, {"Conference": "VAST", "Year": 2014, "Title": "An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging", "DOI": "10.1109/vast.2014.7042481", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042481", "FirstPage": 53.0, "LastPage": 62.0, "PaperType": "C", "Abstract": "For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.", "AuthorNames-Deduped": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorNames": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorAffiliation": "VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria", "InternalReferences": "10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569", "AuthorKeywords": "MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 29.0, "Downloads_Xplore": 300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 5.0, "Downloads": 300.0, "PaperAge": 11}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 292.0, "Downloads": 4307.0, "PaperAge": 10}, {"Conference": "VAST", "Year": 2015, "Title": "Mixed-initiative visual analytics using task-driven recommendations", "DOI": "10.1109/vast.2015.7347625", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347625", "FirstPage": 9.0, "LastPage": 16.0, "PaperType": "C", "Abstract": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.", "AuthorNames-Deduped": "Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorNames": "Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486", "AuthorKeywords": "mixed-initiative visual analytics, task modeling, recommender systems, sensemaking", "AminerCitationCount": 36.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 36.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 25.0, "Downloads": 815.0, "PaperAge": 10}, {"Conference": "VAST", "Year": 2015, "Title": "Collaborative visual analysis with RCloud", "DOI": "10.1109/vast.2015.7347627", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347627", "FirstPage": 25.0, "LastPage": 32.0, "PaperType": "C", "Abstract": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.", "AuthorNames-Deduped": "Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorNames": "Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorAffiliation": "Infovisible;University of Arizona;AT&T Labs;AT&T Labs", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185", "AuthorKeywords": "visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 404.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 7.0, "Downloads": 404.0, "PaperAge": 10}, {"Conference": "SciVis", "Year": 2015, "Title": "Automated visualization workflow for simulation experiments", "DOI": "10.1109/scivis.2015.7429509", "Link": "http://dx.doi.org/10.1109/SciVis.2015.7429509", "FirstPage": 153.0, "LastPage": 154.0, "PaperType": "M", "Abstract": "Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.", "AuthorNames-Deduped": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorNames": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorAffiliation": "School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 1.0, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 12.0, "Downloads_Xplore": 137.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 137.0, "PaperAge": 10}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 92.0, "Downloads": 2245.0, "PaperAge": 9}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 57.0, "Downloads": 2781.0, "PaperAge": 9}, {"Conference": "VAST", "Year": 2016, "Title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods", "DOI": "10.1109/tvcg.2016.2598544", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598544", "FirstPage": 271.0, "LastPage": 280.0, "PaperType": "J", "Abstract": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.", "AuthorNames-Deduped": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne", "AuthorNames": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory", "InternalReferences": "10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591", "AuthorKeywords": "trust;transparency;familiarity;uncertainty;biological data analysis", "AminerCitationCount": 41.0, "CitationCount_CrossRef": 41.0, "PubsCited_CrossRef": 41.0, "Downloads_Xplore": 1844.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 41.0, "Downloads": 1844.0, "PaperAge": 9}, {"Conference": "VAST", "Year": 2016, "Title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations", "DOI": "10.1109/tvcg.2016.2598543", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598543", "FirstPage": 261.0, "LastPage": 270.0, "PaperType": "J", "Abstract": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.", "AuthorNames-Deduped": "Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorNames": "Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorAffiliation": "Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research", "InternalReferences": "10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878", "AuthorKeywords": "Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization", "AminerCitationCount": 39.0, "CitationCount_CrossRef": 33.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 2188.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 33.0, "Downloads": 2188.0, "PaperAge": 9}, {"Conference": "VAST", "Year": 2016, "Title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "DOI": "10.1109/tvcg.2016.2598460", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598460", "FirstPage": 121.0, "LastPage": 130.0, "PaperType": "J", "Abstract": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.", "AuthorNames-Deduped": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook", "AuthorNames": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook", "AuthorAffiliation": "Smith College;Smith College;Smith College;Smith College", "InternalReferences": "10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467", "AuthorKeywords": "Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 16.0, "PubsCited_CrossRef": 87.0, "Downloads_Xplore": 978.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 16.0, "Downloads": 978.0, "PaperAge": 9}, {"Conference": "VAST", "Year": 2016, "Title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "DOI": "10.1109/tvcg.2016.2599378", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2599378", "FirstPage": 231.0, "LastPage": 240.0, "PaperType": "J", "Abstract": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.", "AuthorNames-Deduped": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorNames": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology", "InternalReferences": "10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1", "AuthorKeywords": "Centralized matching;matching visualization;interaction techniques;visual analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 32.0, "Downloads_Xplore": 557.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 8.0, "Downloads": 557.0, "PaperAge": 9}, {"Conference": "VAST", "Year": 2017, "Title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics", "DOI": "10.1109/vast.2017.8585669", "Link": "http://dx.doi.org/10.1109/VAST.2017.8585669", "FirstPage": 104.0, "LastPage": 115.0, "PaperType": "C", "Abstract": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.", "AuthorNames-Deduped": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorNames": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorAffiliation": "Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486", "AuthorKeywords": "cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General", "AminerCitationCount": 115.0, "CitationCount_CrossRef": 70.0, "PubsCited_CrossRef": 80.0, "Downloads_Xplore": 1801.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 70.0, "Downloads": 1801.0, "PaperAge": 8}, {"Conference": "VAST", "Year": 2017, "Title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "DOI": "10.1109/tvcg.2017.2745078", "Link": "http://dx.doi.org/10.1109/TVCG.2017.2745078", "FirstPage": 288.0, "LastPage": 297.0, "PaperType": "J", "Abstract": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "AuthorNames-Deduped": "Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorNames": "Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136", "AuthorKeywords": "Mixed-initiative visual analytics,multi-attribute ranking,user interaction", "AminerCitationCount": 0.0, "CitationCount_CrossRef": 52.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1535.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 52.0, "Downloads": 1535.0, "PaperAge": 8}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 177.0, "Downloads": 3238.0, "PaperAge": 7}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 121.0, "Downloads": 2942.0, "PaperAge": 7}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 91.0, "Downloads": 2871.0, "PaperAge": 7}, {"Conference": "VAST", "Year": 2018, "Title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "DOI": "10.1109/tvcg.2018.2864769", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "FirstPage": 374.0, "LastPage": 384.0, "PaperType": "J", "Abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "AuthorNames-Deduped": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001", "AuthorNames": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins", "AuthorAffiliation": "Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493", "AuthorKeywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "AminerCitationCount": 47.0, "CitationCount_CrossRef": 40.0, "PubsCited_CrossRef": 69.0, "Downloads_Xplore": 1217.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 40.0, "Downloads": 1217.0, "PaperAge": 7}, {"Conference": "VAST", "Year": 2019, "Title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "DOI": "10.1109/vast47406.2019.8986948", "Link": "http://dx.doi.org/10.1109/VAST47406.2019.8986948", "FirstPage": 46.0, "LastPage": 56.0, "PaperType": "C", "Abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.", "AuthorNames-Deduped": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorNames": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718", "AuthorKeywords": "Machine learning fairness,visual analytics,intersectional bias,subgroup discovery", "AminerCitationCount": 107.0, "CitationCount_CrossRef": 106.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 2108.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 106.0, "Downloads": 2108.0, "PaperAge": 6}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 71.0, "Downloads": 2661.0, "PaperAge": 6}, {"Conference": "VAST", "Year": 2019, "Title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "DOI": "10.1109/tvcg.2019.2934654", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "FirstPage": 1001.0, "LastPage": 1011.0, "PaperType": "J", "Abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "AuthorNames-Deduped": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen", "AuthorNames": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen", "AuthorAffiliation": "University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493", "AuthorKeywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "AminerCitationCount": 30.0, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 59.0, "Downloads_Xplore": 1300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 18.0, "Downloads": 1300.0, "PaperAge": 6}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 80.0, "Downloads": 3724.0, "PaperAge": 5}, {"Conference": "InfoVis", "Year": 2020, "Title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "DOI": "10.1109/tvcg.2020.3030467", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "FirstPage": 294.0, "LastPage": 303.0, "PaperType": "J", "Abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "AuthorNames-Deduped": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorNames": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorAffiliation": "Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University", "InternalReferences": "10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487", "AuthorKeywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 36.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 1931.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 36.0, "Downloads": 1931.0, "PaperAge": 5}, {"Conference": "InfoVis", "Year": 2020, "Title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics", "DOI": "10.1109/tvcg.2020.3030448", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030448", "FirstPage": 443.0, "LastPage": 452.0, "PaperType": "J", "Abstract": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.", "AuthorNames-Deduped": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810", "AuthorKeywords": "Infographics,automatic visualization", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1004.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 31.0, "Downloads": 1004.0, "PaperAge": 5}, {"Conference": "VAST", "Year": 2020, "Title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations", "DOI": "10.1109/tvcg.2020.3030387", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030387", "FirstPage": 495.0, "LastPage": 505.0, "PaperType": "J", "Abstract": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.", "AuthorNames-Deduped": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorNames": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorAffiliation": "Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia", "InternalReferences": "10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757", "AuthorKeywords": "visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 28.0, "PubsCited_CrossRef": 81.0, "Downloads_Xplore": 1243.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 28.0, "Downloads": 1243.0, "PaperAge": 5}, {"Conference": "VAST", "Year": 2020, "Title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "DOI": "10.1109/tvcg.2020.3030347", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "FirstPage": 1775.0, "LastPage": 1785.0, "PaperType": "J", "Abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "AuthorNames-Deduped": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorNames": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorAffiliation": "Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France", "InternalReferences": "10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477", "AuthorKeywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 17.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 754.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 17.0, "Downloads": 754.0, "PaperAge": 5}, {"Conference": "SciVis", "Year": 2020, "Title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization", "DOI": "10.1109/tvcg.2020.3030407", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030407", "FirstPage": 806.0, "LastPage": 816.0, "PaperType": "J", "Abstract": "This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorNames": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorAffiliation": "Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz", "InternalReferences": "10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259", "AuthorKeywords": "Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 10.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 530.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 10.0, "Downloads": 530.0, "PaperAge": 5}, {"Conference": "SciVis", "Year": 2020, "Title": "IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection", "DOI": "10.1109/tvcg.2020.3030389", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030389", "FirstPage": 775.0, "LastPage": 784.0, "PaperType": "J", "Abstract": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.", "AuthorNames-Deduped": "Juraj P\u00e1lenik;Thomas Spengler;Helwig Hauser", "AuthorNames": "Juraj Palenik;Thomas Spengler;Helwig Hauser", "AuthorAffiliation": "University of Bergen;University of Bergen;University of Bergen", "InternalReferences": "10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190", "AuthorKeywords": "visual parameter space exploration,scientific modelling,atmospheric convection", "AminerCitationCount": 1.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 417.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 417.0, "PaperAge": 5}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 69.0, "Downloads": 3452.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 62.0, "Downloads": 2594.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Augmenting Sports Videos with VisCommentator", "DOI": "10.1109/tvcg.2021.3114806", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114806", "FirstPage": 824.0, "LastPage": 834.0, "PaperType": "J", "Abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.", "AuthorNames-Deduped": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu", "AuthorNames": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458", "AuthorKeywords": "Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling", "AminerCitationCount": 19.0, "CitationCount_CrossRef": 42.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2151.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 42.0, "Downloads": 2151.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Kori: Interactive Synthesis of Text and Charts in Data Documents", "DOI": "10.1109/tvcg.2021.3114802", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114802", "FirstPage": 184.0, "LastPage": 194.0, "PaperType": "J", "Abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.", "AuthorNames-Deduped": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim", "AuthorNames": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim", "AuthorAffiliation": "University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 1308.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 34.0, "Downloads": 1308.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "VizLinter: A Linter and Fixer Framework for Data Visualization", "DOI": "10.1109/tvcg.2021.3114804", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114804", "FirstPage": 206.0, "LastPage": 216.0, "PaperType": "J", "Abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001", "AuthorNames": "Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China", "InternalReferences": "10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166", "AuthorKeywords": "Visualization Linting,Automated Visualization Design,Visualization Optimization", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 32.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 1919.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 32.0, "Downloads": 1919.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation", "DOI": "10.1109/tvcg.2021.3114826", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114826", "FirstPage": 162.0, "LastPage": 172.0, "PaperType": "J", "Abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.", "AuthorNames-Deduped": "Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001", "AuthorNames": "Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang", "AuthorAffiliation": "Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1788.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 31.0, "Downloads": 1788.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms", "DOI": "10.1109/tvcg.2021.3114814", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114814", "FirstPage": 346.0, "LastPage": 356.0, "PaperType": "J", "Abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.", "AuthorNames-Deduped": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorNames": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorAffiliation": "University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Visualization Tools,Visualization Recommendation Algorithms", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1106.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 25.0, "Downloads": 1106.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Towards Visual Explainable Active Learning for Zero-Shot Classification", "DOI": "10.1109/tvcg.2021.3114793", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114793", "FirstPage": 791.0, "LastPage": 801.0, "PaperType": "J", "Abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.", "AuthorNames-Deduped": "Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang", "AuthorNames": "Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang", "AuthorAffiliation": "College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China", "InternalReferences": "10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818", "AuthorKeywords": "Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 76.0, "Downloads_Xplore": 1775.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 24.0, "Downloads": 1775.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "DOI": "10.1109/tvcg.2021.3114810", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114810", "FirstPage": 151.0, "LastPage": 161.0, "PaperType": "J", "Abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA", "InternalReferences": "10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437", "AuthorKeywords": "Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 21.0, "PubsCited_CrossRef": 105.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 21.0, "Downloads": 815.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs", "DOI": "10.1109/tvcg.2021.3114877", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114877", "FirstPage": 400.0, "LastPage": 410.0, "PaperType": "J", "Abstract": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.", "AuthorNames-Deduped": "Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185", "AuthorKeywords": "Glyph-based visualization,machine learning,automatic visualization", "AminerCitationCount": 10.0, "CitationCount_CrossRef": 19.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1101.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 19.0, "Downloads": 1101.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks", "DOI": "10.1109/tvcg.2021.3114858", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114858", "FirstPage": 813.0, "LastPage": 823.0, "PaperType": "J", "Abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.", "AuthorNames-Deduped": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau", "AuthorNames": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau", "AuthorAffiliation": "Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States", "InternalReferences": "10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509", "AuthorKeywords": "Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding", "AminerCitationCount": 8.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 830.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 15.0, "Downloads": 830.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics", "DOI": "10.1109/tvcg.2021.3114820", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114820", "FirstPage": 486.0, "LastPage": 496.0, "PaperType": "J", "Abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.", "AuthorNames-Deduped": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorNames": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorAffiliation": "Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493", "AuthorKeywords": "transformers,word embeddings,literature review,web scraper,dataset,visual analytics", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 1087.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 15.0, "Downloads": 1087.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "A Mixed-Initiative Approach to Reusing Infographic Charts", "DOI": "10.1109/tvcg.2021.3114856", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114856", "FirstPage": 173.0, "LastPage": 183.0, "PaperType": "J", "Abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.", "AuthorNames-Deduped": "Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732", "AuthorKeywords": "Infographics,Reusable templates,Graphic design,Automatic visualization", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 13.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1211.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 13.0, "Downloads": 1211.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization", "DOI": "10.1109/tvcg.2021.3114782", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114782", "FirstPage": 129.0, "LastPage": 139.0, "PaperType": "J", "Abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorNames": "Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136", "AuthorKeywords": "Task-oriented insight preservation,responsive visualization", "AminerCitationCount": 6.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 751.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 9.0, "Downloads": 751.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Semantic Snapping for Guided Multi-View Visualization Design", "DOI": "10.1109/tvcg.2021.3114860", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114860", "FirstPage": 43.0, "LastPage": 53.0, "PaperType": "J", "Abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \u201caligned\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.", "AuthorNames-Deduped": "Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner", "AuthorNames": "Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner", "AuthorAffiliation": "Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338", "AuthorKeywords": "Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views", "AminerCitationCount": 5.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 896.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 7.0, "Downloads": 896.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2021, "Title": "Visualization Equilibrium", "DOI": "10.1109/tvcg.2021.3114842", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114842", "FirstPage": 465.0, "LastPage": 474.0, "PaperType": "J", "Abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.", "AuthorNames-Deduped": "Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907", "AuthorKeywords": "Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 647.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 647.0, "PaperAge": 4}, {"Conference": "Vis", "Year": 2022, "Title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "DOI": "10.1109/tvcg.2022.3209447", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209447", "FirstPage": 331.0, "LastPage": 341.0, "PaperType": "J", "Abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.", "AuthorNames-Deduped": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254", "AuthorKeywords": "Glyph-based visualization,metaphor,machine learning,automatic visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 68.0, "Downloads_Xplore": 1095.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 18.0, "Downloads": 1095.0, "PaperAge": 3}, {"Conference": "Vis", "Year": 2022, "Title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning", "DOI": "10.1109/tvcg.2022.3209468", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209468", "FirstPage": 690.0, "LastPage": 700.0, "PaperType": "J", "Abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.", "AuthorNames-Deduped": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorNames": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234", "AuthorKeywords": "Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 83.0, "Downloads_Xplore": 1671.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 14.0, "Downloads": 1671.0, "PaperAge": 3}, {"Conference": "Vis", "Year": 2022, "Title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning", "DOI": "10.1109/tvcg.2022.3209461", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209461", "FirstPage": 95.0, "LastPage": 105.0, "PaperType": "J", "Abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.", "AuthorNames-Deduped": "Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001", "AuthorNames": "Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen", "AuthorAffiliation": "School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543", "AuthorKeywords": "Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 1276.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 8.0, "Downloads": 1276.0, "PaperAge": 3}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 8.0, "Downloads": 1537.0, "PaperAge": 3}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 7.0, "Downloads": 2485.0, "PaperAge": 3}, {"Conference": "Vis", "Year": 2023, "Title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback", "DOI": "10.1109/tvcg.2023.3327363", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327363", "FirstPage": 131.0, "LastPage": 141.0, "PaperType": "J", "Abstract": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.", "AuthorNames-Deduped": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorNames": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorAffiliation": "New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774", "AuthorKeywords": "Narrative visualization,visual storytelling,conversational agent", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 816.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 7.0, "Downloads": 816.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks", "DOI": "10.1109/tvcg.2023.3327170", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327170", "FirstPage": 944.0, "LastPage": 954.0, "PaperType": "J", "Abstract": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.", "AuthorNames-Deduped": "Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorNames": "Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China", "InternalReferences": "0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 665.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 5.0, "Downloads": 665.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Mystique: Deconstructing SVG Charts for Layout Reuse", "DOI": "10.1109/tvcg.2023.3327354", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327354", "FirstPage": 447.0, "LastPage": 457.0, "PaperType": "J", "Abstract": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.", "AuthorNames-Deduped": "Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001", "AuthorNames": "Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu", "AuthorAffiliation": "University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States", "InternalReferences": "0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877", "AuthorKeywords": "Chart layout,Reuse,Reverse-engineering,Deconstruction", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 481.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 481.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning", "DOI": "10.1109/tvcg.2023.3327200", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327200", "FirstPage": 1172.0, "LastPage": 1182.0, "PaperType": "J", "Abstract": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.", "AuthorNames-Deduped": "Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001", "AuthorNames": "Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865", "AuthorKeywords": "Time Series Data,Exploratory Visual Analysis,Reinforcement Learning", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 1050.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 1050.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining", "DOI": "10.1109/tvcg.2023.3326939", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326939", "FirstPage": 1227.0, "LastPage": 1237.0, "PaperType": "J", "Abstract": "Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA", "InternalReferences": "0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475", "AuthorKeywords": "Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 82.0, "Downloads_Xplore": 361.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 361.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco", "DOI": "10.1109/tvcg.2023.3326527", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326527", "FirstPage": 1063.0, "LastPage": 1073.0, "PaperType": "J", "Abstract": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\u2014a framework to model visualization knowledge\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.", "AuthorNames-Deduped": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorNames": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorAffiliation": "University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA", "InternalReferences": "0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814", "AuthorKeywords": "Graphical Perception Studies,Visualization Recommendation Algorithms", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 371.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 371.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "The Rational Agent Benchmark for Data Visualization", "DOI": "10.1109/tvcg.2023.3326513", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326513", "FirstPage": 338.0, "LastPage": 347.0, "PaperType": "J", "Abstract": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.", "AuthorNames-Deduped": "Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781", "AuthorKeywords": "Evaluation,decision-making,rational agent,scoring rule", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 33.0, "Downloads_Xplore": 434.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 2.0, "Downloads": 434.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams", "DOI": "10.1109/tvcg.2023.3326925", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326925", "FirstPage": 562.0, "LastPage": 572.0, "PaperType": "J", "Abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001", "AuthorNames": "Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428", "AuthorKeywords": "Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 78.0, "Downloads_Xplore": 662.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 662.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization", "DOI": "10.1109/tvcg.2023.3326583", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326583", "FirstPage": 934.0, "LastPage": 943.0, "PaperType": "J", "Abstract": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell", "AuthorNames": "Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization,responsive visualization,mixed-initiative authoring", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 330.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 330.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Visual Analytics for Understanding Draco's Knowledge Base", "DOI": "10.1109/tvcg.2023.3326912", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326912", "FirstPage": 392.0, "LastPage": 402.0, "PaperType": "J", "Abstract": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.", "AuthorNames-Deduped": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorNames": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorAffiliation": "VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Visual Analytics,Hypergraph visualization,Rule-based recommendation systems", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 53.0, "Downloads_Xplore": 365.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 365.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring", "DOI": "10.1109/tvcg.2023.3326585", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326585", "FirstPage": 1128.0, "LastPage": 1138.0, "PaperType": "J", "Abstract": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.", "AuthorNames-Deduped": "Chenglong Wang;John Thompson 0002;Bongshin Lee", "AuthorNames": "Chenglong Wang;John Thompson;Bongshin Lee", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA", "InternalReferences": "0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369", "AuthorKeywords": "AI,visualization authoring,data transformation,programming by example,natural language,large language model", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 63.0, "Downloads_Xplore": 893.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 893.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2023, "Title": "Guided Visual Analytics for Image Selection in Time and Space", "DOI": "10.1109/tvcg.2023.3326572", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326572", "FirstPage": 66.0, "LastPage": 75.0, "PaperType": "J", "Abstract": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.", "AuthorNames-Deduped": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorNames": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorAffiliation": "TU Wien, Austria;TU Wien, Austria;TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 37.0, "Downloads_Xplore": 1208.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 1208.0, "PaperAge": 2}, {"Conference": "Vis", "Year": 2024, "Title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts", "DOI": "10.1109/tvcg.2024.3456398", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456398", "FirstPage": 481.0, "LastPage": 491.0, "PaperType": "J", "Abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.", "AuthorNames-Deduped": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorNames": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorAffiliation": "University of California, USA;University of California, USA;University of California, USA;University of California, USA", "InternalReferences": "10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878", "AuthorKeywords": "Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 65.0, "Downloads_Xplore": 386.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 386.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration", "DOI": "10.1109/tvcg.2024.3456364", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456364", "FirstPage": 547.0, "LastPage": 557.0, "PaperType": "J", "Abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.", "AuthorNames-Deduped": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorNames": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorAffiliation": "Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA", "InternalReferences": "10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798", "AuthorKeywords": "Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 632.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 632.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models", "DOI": "10.1109/tvcg.2024.3456320", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456320", "FirstPage": 1301.0, "LastPage": 1311.0, "PaperType": "J", "Abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.", "AuthorNames-Deduped": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001", "AuthorNames": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Visualization evaluation,automatic visualization,,,large language models,benchmark", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 75.0, "Downloads_Xplore": 625.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 1.0, "Downloads": 625.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech", "DOI": "10.1109/tvcg.2024.3456358", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456358", "FirstPage": 864.0, "LastPage": 874.0, "PaperType": "J", "Abstract": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.", "AuthorNames-Deduped": "Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorNames": "Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorAffiliation": "Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia", "InternalReferences": "10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275", "AuthorKeywords": "Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 184.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 184.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models", "DOI": "10.1109/tvcg.2024.3456350", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456350", "FirstPage": 710.0, "LastPage": 720.0, "PaperType": "J", "Abstract": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\u2014DracoGPT-Rank and DracoGPT-Recommend\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.", "AuthorNames-Deduped": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorNames": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorAffiliation": "University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527", "AuthorKeywords": "Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 378.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 378.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent", "DOI": "10.1109/tvcg.2024.3456200", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456200", "FirstPage": 23.0, "LastPage": 33.0, "PaperType": "J", "Abstract": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.", "AuthorNames-Deduped": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu", "AuthorNames": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu", "AuthorAffiliation": "Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806", "AuthorKeywords": "Sports visualization,tactic board,,,tactical analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 813.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 813.0, "PaperAge": 1}, {"Conference": "Vis", "Year": 2024, "Title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations", "DOI": "10.1109/tvcg.2024.3456182", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456182", "FirstPage": 754.0, "LastPage": 764.0, "PaperType": "J", "Abstract": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6", "AuthorNames-Deduped": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorNames": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorAffiliation": "Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA", "InternalReferences": "10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513", "AuthorKeywords": "Visual inference,statistical rationality,,,human-machine collaboration", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 71.0, "Downloads_Xplore": 138.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Citation": 0.0, "Downloads": 138.0, "PaperAge": 1}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_b9edd2a3", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">Author- and institution-level summaries show a small set of highly productive and highly-cited contributors. Top productive authors include Alex Endert, Huamin Qu, Yingcai Wu and Jeffrey Heer; some authors (e.g., Heer, Dominik Moritz) have especially high citations per paper, indicating outsized influence from a few high-impact works. Institutional aggregation highlights strong contributions from University of Washington, Georgia Tech, Microsoft Research Asia, Zhejiang University and Tongji University, with the USA and China prominent in country-level counts. Paper-level leaders by citations and downloads again point to a handful of milestone systems (Voyager, Draco, Design Space) responsible for disproportionate attention. Correlations between paper age and citations are weakly positive, and age vs downloads slightly negative, reinforcing the need to consider annualized rates when comparing recent and older work.</p>
        </p>
      </div>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-5-2'></div>
          </div>
        </div>
        
        <!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
        </head>
        <body>
            
    <div id="network_82835d33-b39f-4e0d-a051-3275aacc5080" style="width: 100%px; height: 100%px;"></div>
    <script src="https://unpkg.com/@antv/g6@5/dist/g6.min.js"></script>
    <script>
const data = {"nodes": [{"id": "Eli T. Brown"}, {"id": "Zhicheng Liu 0001"}, {"id": "Nuo Chen"}, {"id": "Philipp Muigg"}, {"id": "Laura A. Garrison"}, {"id": "Stefan Lindholm"}, {"id": "Yingcai Wu"}, {"id": "Robert Kincaid"}, {"id": "Ratanond Koonchanok"}, {"id": "Yuzhe Luo"}, {"id": "Alexis Pister"}, {"id": "Hans-J\u00f6rg Schulz"}, {"id": "Guadalupe Canahuate"}, {"id": "Ryan Wesslen"}, {"id": "Hyeok Kim"}, {"id": "Jane Hoffswell"}, {"id": "Duen Horng Chau"}, {"id": "Weiwei Cui"}, {"id": "Arvind Satyanarayan"}, {"id": "Liqi Cheng"}, {"id": "He Huang"}, {"id": "Steve DiPaola"}, {"id": "Dongyu Liu"}, {"id": "Yuqing Yang 0001"}, {"id": "Yunjeong Chang"}, {"id": "Shunan Guo"}, {"id": "Lars Schuchardt"}, {"id": "Alireza Karduni"}, {"id": "Yang Shi 0007"}, {"id": "Thomas Spengler"}, {"id": "Rui Zhang"}, {"id": "Nan Cao 0001"}, {"id": "Helwig Hauser"}, {"id": "William Ribarsky"}, {"id": "Dominik Fleischmann"}, {"id": "Fabian Sperrle"}, {"id": "Dazhen Deng"}, {"id": "Ross Maciejewski"}, {"id": "Jason D. Hartline"}, {"id": "Emily Wall"}, {"id": "Jiawan Zhang"}, {"id": "Paola Valdivia"}, {"id": "Ingrid Zukerman"}, {"id": "Jiahang Xu"}, {"id": "Carla Floricel"}, {"id": "Junran Yang"}, {"id": "Alan Lundgard"}, {"id": "Ignacio P\u00e9rez-Messina"}, {"id": "Kanit Wongsuphasawat"}, {"id": "Chen Chen 0080"}, {"id": "\u00c1ngel Alexander Cabrera"}, {"id": "Anushka Anand"}, {"id": "Jeremy G. Freeman"}, {"id": "Ryan A. Rossi"}, {"id": "Nan Chen"}, {"id": "Jeffrey Heer"}, {"id": "Danqing Shi"}, {"id": "Bharath Kalidindi"}, {"id": "Jean-Daniel Fekete"}, {"id": "Bahador Saket"}, {"id": "Songheng Zhang"}, {"id": "Qing Chen 0001"}, {"id": "Aryaman Bahukhandi"}, {"id": "Tian Gao"}, {"id": "Khairi Reda"}, {"id": "\u00c7agatay Demiralp"}, {"id": "Will Epperson"}, {"id": "Tamara Munzner"}, {"id": "Xiao Xie"}, {"id": "Yongkang Xiao"}, {"id": "Silvia Miksch"}, {"id": "Vidya Setlur"}, {"id": "Ziao Liu"}, {"id": "Helen Zhao 0001"}, {"id": "Han-Wei Shen"}, {"id": "Fan Du"}, {"id": "Mitchell Gordon"}, {"id": "Chenglong Wang"}, {"id": "Matthew Butler 0002"}, {"id": "Arnold K\u00f6chl"}, {"id": "Sana Malik"}, {"id": "Arjun Srinivasan"}, {"id": "Nafiul Nipu"}, {"id": "Hannah Kim 0001"}, {"id": "Xinyi He"}, {"id": "Michael Glueck"}, {"id": "Alex Endert"}, {"id": "Liang Gou"}, {"id": "Thomas Ertl"}, {"id": "Tan Tang"}, {"id": "Zeyu Li 0003"}, {"id": "Remco Chang"}, {"id": "Hao Yang 0007"}, {"id": "Glenn Sun"}, {"id": "Klaus Reichenberger"}, {"id": "Sebastian Bremm"}, {"id": "Lyndsey Franklin"}, {"id": "Jonathan P. Leidig"}, {"id": "Yuchen Yang"}, {"id": "Ke Xu"}, {"id": "R. Jordan Crouser"}, {"id": "Xinhuan Shu"}, {"id": "Qianwen Wang"}, {"id": "Zehua Zeng"}, {"id": "Paolo Buono"}, {"id": "Shuhan Liu"}, {"id": "Nick Cramer"}, {"id": "Gunnar L\u00e4th\u00e9n"}, {"id": "Gene Golovchinsky"}, {"id": "Shichao Jia"}, {"id": "Hui Zhang 0051"}, {"id": "Quan Lin"}, {"id": "Zhe Xu 0007"}, {"id": "Alexandra La Cruz"}, {"id": "Bingchang Chen"}, {"id": "Huichen Will Wang"}, {"id": "Yu Hou"}, {"id": "Nilaksh Das"}, {"id": "Eston Schweickart"}, {"id": "Michael Oppermann"}, {"id": "Xiaohan Jiao"}, {"id": "G. Elisabeta Marai"}, {"id": "Simon Urbanek"}, {"id": "Joe Bruce"}, {"id": "Nam Wook Kim"}, {"id": "Renzhong Li"}, {"id": "Heidrun Schumann"}, {"id": "Mira Dontcheva"}, {"id": "Peiran Ren"}, {"id": "Stefan Bruckner"}, {"id": "Sehi L'Yi"}, {"id": "Oskar Elek"}, {"id": "Michelle A. Borkin"}, {"id": "Leslie M. Blaha"}, {"id": "Oliver Deussen"}, {"id": "M. Eduard Gr\u00f6ller"}, {"id": "Halden Lin"}, {"id": "Wenwen Dou"}, {"id": "Davide Ceneda"}, {"id": "Thomas Kamps"}, {"id": "Andrew Wentzel"}, {"id": "Bernhard Pointner"}, {"id": "Joon-Yong Lee"}, {"id": "Yihong Wu 0003"}, {"id": "Eunyee Koh"}, {"id": "Yanna Lin"}, {"id": "Siming Chen 0001"}, {"id": "David J. Israel"}, {"id": "Anders Persson"}, {"id": "Bill Howe"}, {"id": "Lizhen Qu"}, {"id": "Chin-Yew Lin"}, {"id": "Mikayla Biggs"}, {"id": "Achim Ebert"}, {"id": "Mengyu Zhou"}, {"id": "Kay Hamacher"}, {"id": "Bei Chen"}, {"id": "Thomas Butkiewicz"}, {"id": "Yuge Zhang"}, {"id": "Yusheng Qi"}, {"id": "Ziyang Guo"}, {"id": "Junpeng Wang 0001"}, {"id": "Katja B\u00fchler"}, {"id": "Haijun Xia"}, {"id": "Greg L. Nelson"}, {"id": "Zachary Wartell"}, {"id": "Andrew E. Johnson 0001"}, {"id": "Wei Shuai"}, {"id": "Steven Mark Drucker"}, {"id": "Yiwen Sun"}, {"id": "Shuainan Ye"}, {"id": "Wenshuo Zhao"}, {"id": "Huamin Qu"}, {"id": "Johanna Schmidt"}, {"id": "Reiner Lenz"}, {"id": "Yixian Zheng"}, {"id": "Subhajit Das 0002"}, {"id": "Zheng Zhou"}, {"id": "Johannes Knittel"}, {"id": "Abdallah Sherif Radwan Mohamed"}, {"id": "Paula Kayongo"}, {"id": "Shahid Latif"}, {"id": "Yun Wang 0012"}, {"id": "Yixuan Li"}, {"id": "Aritra Dasgupta"}, {"id": "Michael S. Bernstein"}, {"id": "Minsuk Kahng"}, {"id": "Lei Fang 0004"}, {"id": "Hans Hagen"}, {"id": "Shizhao Sun"}, {"id": "Ryan Wilson"}, {"id": "Phoebe Moh"}, {"id": "Rahul Duggal"}, {"id": "Mat\u00fas Straka"}, {"id": "Abhraneel Sarma"}, {"id": "Christopher Collins 0001"}, {"id": "Wilmot Li"}, {"id": "Kresimir Matkovic"}, {"id": "Zhutian Chen"}, {"id": "Lisanne van Dijk"}, {"id": "Frank Keul"}, {"id": "Stephen C. North"}, {"id": "Yangqiu Song"}, {"id": "Olav Lenz"}, {"id": "Michalis Mamakos"}, {"id": "Milos Sr\u00e1mek"}, {"id": "Zhuochen Jin"}, {"id": "Anne Laprie"}, {"id": "Richard Souvenir"}, {"id": "Helmut Doleisch"}, {"id": "Wenchao Wu"}, {"id": "Reyk Hillert"}, {"id": "Haidong Zhang"}, {"id": "Sol\u00e9akh\u00e9na Ken"}, {"id": "Zui Chen"}, {"id": "Mennatallah El-Assady"}, {"id": "Steffen Oeltze"}, {"id": "Magnus Heitzler"}, {"id": "Bongshin Lee"}, {"id": "J. Xavier Prochaska"}, {"id": "Tatiana von Landesberger"}, {"id": "Yifan Wu 0005"}, {"id": "Omar Shaikh"}, {"id": "Michael E. Papka"}, {"id": "Duen Horng (Polo) Chau"}, {"id": "Yunhai Wang"}, {"id": "Leilani Battle"}, {"id": "Xiangtong Chu"}, {"id": "Lvkeshen Shen"}, {"id": "Lu Ying"}, {"id": "Moqi He"}, {"id": "Chunyao Qian"}, {"id": "Alvitta Ottley"}, {"id": "Jock D. Mackinlay"}, {"id": "Haekyu Park"}, {"id": "Haotian Li 0001"}, {"id": "Joseph N. Burchett"}, {"id": "Fanny Chevalier"}, {"id": "Jinpeng Wang 0001"}, {"id": "Austin P. Wright"}, {"id": "Gordon Woodhull"}, {"id": "Dongmei Zhang 0001"}, {"id": "Magnus Borga"}, {"id": "Yoon Kim"}, {"id": "Matthias Schlachter"}, {"id": "Miguel Nunes"}, {"id": "Kim Marriott"}, {"id": "Robert A. Lafrance"}, {"id": "Xinke Wu"}, {"id": "Ying Chen"}, {"id": "Tera Marie Green"}, {"id": "Hanghang Tong"}, {"id": "Leni Yang"}, {"id": "Lingyun Yu 0001"}, {"id": "Samuel Reinders"}, {"id": "Heidrun Steinmetz"}, {"id": "Yngve Sekse Kristiansen"}, {"id": "Catherine Plaisant"}, {"id": "Azam Khan"}, {"id": "Fred Hohman"}, {"id": "Thomas Nocke"}, {"id": "Nils Gehlenborg"}, {"id": "Po-Ming Law"}, {"id": "Aditeya Pandey"}, {"id": "Sam Yu-Te Lee"}, {"id": "Jessica Hullman"}, {"id": "Carlos Eduardo Scheidegger"}, {"id": "Rebecca Kehlbeck"}, {"id": "Arpit Narechania"}, {"id": "Fuling Sun"}, {"id": "Kwan-Liu Ma"}, {"id": "Adam M. Smith 0001"}, {"id": "Russ Burtner"}, {"id": "Jamie Morgenstern"}, {"id": "Wolfgang Freiler"}, {"id": "Ravish Chawla"}, {"id": "Benjamin Rowland"}, {"id": "Jochen Ehret"}, {"id": "Kristin A. Cook"}, {"id": "Daniel A. Keim"}, {"id": "Xinyue Xu"}, {"id": "Dominik Moritz"}, {"id": "Aoyu Wu"}, {"id": "Kan Ren"}, {"id": "Guande Wu"}, {"id": "Simon Breslav"}, {"id": "Steffen Koch 0001"}, {"id": "Jason Leigh"}, {"id": "Santhosh Dharmapuri"}, {"id": "Yong Wang 0021"}, {"id": "Bernhard Preim"}, {"id": "Michael Wolverton"}, {"id": "John T. Stasko"}, {"id": "Clifton David Fuller"}, {"id": "Jian-Guang Lou"}, {"id": "Dennis Chau"}, {"id": "Walter Schubert"}, {"id": "Angus G. Forbes"}, {"id": "Gromit Yeuk-Yin Chan"}, {"id": "Fabian Beck 0001"}, {"id": "Xiaoyu Zhang 0014"}, {"id": "Tak Yeon Lee"}, {"id": "Jovan Popovic"}, {"id": "Jiazhe Wang"}, {"id": "Hanspeter Pfister"}, {"id": "Samuel H. Payne"}, {"id": "Jian Zhao 0010"}, {"id": "Youfu Yan"}, {"id": "John Thompson 0002"}, {"id": "Juraj P\u00e1lenik"}], "edges": [{"source": "Eli T. Brown", "target": "Remco Chang", "value": 3, "filtered": true}, {"source": "Eli T. Brown", "target": "Alvitta Ottley", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Helen Zhao 0001", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Quan Lin", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Richard Souvenir", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Alex Endert", "value": 3, "filtered": true}, {"source": "Eli T. Brown", "target": "Bahador Saket", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Hannah Kim 0001", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Emily Wall", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Subhajit Das 0002", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Ravish Chawla", "value": 1, "filtered": true}, {"source": "Eli T. Brown", "target": "Bharath Kalidindi", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Jeffrey Heer", "value": 2, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "John T. Stasko", "value": 9, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Nam Wook Kim", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Eston Schweickart", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Mira Dontcheva", "value": 2, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Wilmot Li", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Jovan Popovic", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Hanspeter Pfister", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Arvind Satyanarayan", "value": 1, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Bongshin Lee", "value": 3, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "John Thompson 0002", "value": 1, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Po-Ming Law", "value": 1, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Sana Malik", "value": 1, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Leilani Battle", "value": 2, "filtered": false}, {"source": "Zhicheng Liu 0001", "target": "Chen Chen 0080", "value": 2, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Yunhai Wang", "value": 1, "filtered": true}, {"source": "Zhicheng Liu 0001", "target": "Yunjeong Chang", "value": 1, "filtered": true}, {"source": "Nuo Chen", "target": "Shichao Jia", "value": 1, "filtered": true}, {"source": "Nuo Chen", "target": "Zeyu Li 0003", "value": 1, "filtered": true}, {"source": "Nuo Chen", "target": "Jiawan Zhang", "value": 1, "filtered": true}, {"source": "Philipp Muigg", "target": "Helmut Doleisch", "value": 5, "filtered": true}, {"source": "Philipp Muigg", "target": "Helwig Hauser", "value": 3, "filtered": true}, {"source": "Philipp Muigg", "target": "Steffen Oeltze", "value": 1, "filtered": true}, {"source": "Philipp Muigg", "target": "Bernhard Preim", "value": 1, "filtered": true}, {"source": "Philipp Muigg", "target": "M. Eduard Gr\u00f6ller", "value": 1, "filtered": false}, {"source": "Laura A. Garrison", "target": "Yngve Sekse Kristiansen", "value": 1, "filtered": true}, {"source": "Laura A. Garrison", "target": "Stefan Bruckner", "value": 2, "filtered": true}, {"source": "Stefan Lindholm", "target": "Anders Persson", "value": 2, "filtered": true}, {"source": "Stefan Lindholm", "target": "Gunnar L\u00e4th\u00e9n", "value": 1, "filtered": true}, {"source": "Stefan Lindholm", "target": "Reiner Lenz", "value": 1, "filtered": true}, {"source": "Stefan Lindholm", "target": "Magnus Borga", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Dongyu Liu", "value": 3, "filtered": false}, {"source": "Yingcai Wu", "target": "Huamin Qu", "value": 13, "filtered": true}, {"source": "Yingcai Wu", "target": "Weiwei Cui", "value": 4, "filtered": false}, {"source": "Yingcai Wu", "target": "Xiao Xie", "value": 15, "filtered": true}, {"source": "Yingcai Wu", "target": "Dazhen Deng", "value": 9, "filtered": true}, {"source": "Yingcai Wu", "target": "Hui Zhang 0051", "value": 12, "filtered": true}, {"source": "Yingcai Wu", "target": "Xinhuan Shu", "value": 5, "filtered": true}, {"source": "Yingcai Wu", "target": "Shuainan Ye", "value": 5, "filtered": true}, {"source": "Yingcai Wu", "target": "Zhutian Chen", "value": 5, "filtered": true}, {"source": "Yingcai Wu", "target": "Xiangtong Chu", "value": 4, "filtered": true}, {"source": "Yingcai Wu", "target": "Zheng Zhou", "value": 2, "filtered": false}, {"source": "Yingcai Wu", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Yingcai Wu", "target": "Haijun Xia", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Tan Tang", "value": 9, "filtered": true}, {"source": "Yingcai Wu", "target": "Renzhong Li", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Shuhan Liu", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Johannes Knittel", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Steffen Koch 0001", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Lingyun Yu 0001", "value": 6, "filtered": true}, {"source": "Yingcai Wu", "target": "Peiran Ren", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Thomas Ertl", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Ziyang Guo", "value": 3, "filtered": false}, {"source": "Yingcai Wu", "target": "Aoyu Wu", "value": 3, "filtered": true}, {"source": "Yingcai Wu", "target": "Fan Du", "value": 1, "filtered": false}, {"source": "Yingcai Wu", "target": "Lu Ying", "value": 3, "filtered": true}, {"source": "Yingcai Wu", "target": "Yuzhe Luo", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Lvkeshen Shen", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Yuchen Yang", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Hanspeter Pfister", "value": 1, "filtered": false}, {"source": "Yingcai Wu", "target": "Yihong Wu 0003", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Moqi He", "value": 2, "filtered": true}, {"source": "Yingcai Wu", "target": "Yunhai Wang", "value": 1, "filtered": false}, {"source": "Yingcai Wu", "target": "Ziao Liu", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Liqi Cheng", "value": 1, "filtered": true}, {"source": "Yingcai Wu", "target": "Yun Wang 0012", "value": 1, "filtered": false}, {"source": "Yingcai Wu", "target": "Haidong Zhang", "value": 1, "filtered": false}, {"source": "Robert Kincaid", "target": "Tamara Munzner", "value": 3, "filtered": true}, {"source": "Robert Kincaid", "target": "Michael Oppermann", "value": 1, "filtered": true}, {"source": "Ratanond Koonchanok", "target": "Michael E. Papka", "value": 1, "filtered": true}, {"source": "Ratanond Koonchanok", "target": "Khairi Reda", "value": 1, "filtered": true}, {"source": "Yuzhe Luo", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Yuzhe Luo", "target": "Tan Tang", "value": 1, "filtered": true}, {"source": "Yuzhe Luo", "target": "Lvkeshen Shen", "value": 1, "filtered": true}, {"source": "Yuzhe Luo", "target": "Xiao Xie", "value": 1, "filtered": true}, {"source": "Yuzhe Luo", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Alexis Pister", "target": "Paolo Buono", "value": 1, "filtered": true}, {"source": "Alexis Pister", "target": "Jean-Daniel Fekete", "value": 1, "filtered": true}, {"source": "Alexis Pister", "target": "Catherine Plaisant", "value": 1, "filtered": true}, {"source": "Alexis Pister", "target": "Paola Valdivia", "value": 1, "filtered": true}, {"source": "Alexis Pister", "target": "Xinhuan Shu", "value": 1, "filtered": false}, {"source": "Alexis Pister", "target": "Fanny Chevalier", "value": 1, "filtered": false}, {"source": "Hans-J\u00f6rg Schulz", "target": "Davide Ceneda", "value": 1, "filtered": false}, {"source": "Hans-J\u00f6rg Schulz", "target": "Silvia Miksch", "value": 1, "filtered": false}, {"source": "Hans-J\u00f6rg Schulz", "target": "Thomas Nocke", "value": 1, "filtered": true}, {"source": "Hans-J\u00f6rg Schulz", "target": "Magnus Heitzler", "value": 1, "filtered": true}, {"source": "Hans-J\u00f6rg Schulz", "target": "Heidrun Schumann", "value": 2, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Carla Floricel", "value": 2, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Nafiul Nipu", "value": 1, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Mikayla Biggs", "value": 1, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Andrew Wentzel", "value": 4, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Abdallah Sherif Radwan Mohamed", "value": 2, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "Clifton David Fuller", "value": 3, "filtered": true}, {"source": "Guadalupe Canahuate", "target": "G. Elisabeta Marai", "value": 4, "filtered": true}, {"source": "Ryan Wesslen", "target": "Alireza Karduni", "value": 4, "filtered": true}, {"source": "Ryan Wesslen", "target": "Wenwen Dou", "value": 4, "filtered": false}, {"source": "Ryan Wesslen", "target": "Arpit Narechania", "value": 1, "filtered": true}, {"source": "Ryan Wesslen", "target": "Emily Wall", "value": 1, "filtered": true}, {"source": "Ryan Wesslen", "target": "William Ribarsky", "value": 1, "filtered": false}, {"source": "Hyeok Kim", "target": "Ryan A. Rossi", "value": 2, "filtered": true}, {"source": "Hyeok Kim", "target": "Abhraneel Sarma", "value": 1, "filtered": true}, {"source": "Hyeok Kim", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Hyeok Kim", "target": "Jessica Hullman", "value": 2, "filtered": true}, {"source": "Hyeok Kim", "target": "Jane Hoffswell", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Arvind Satyanarayan", "value": 1, "filtered": false}, {"source": "Jane Hoffswell", "target": "Jeffrey Heer", "value": 1, "filtered": false}, {"source": "Jane Hoffswell", "target": "Zehua Zeng", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Phoebe Moh", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Fan Du", "value": 2, "filtered": true}, {"source": "Jane Hoffswell", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Sana Malik", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Eunyee Koh", "value": 3, "filtered": true}, {"source": "Jane Hoffswell", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Vidya Setlur", "value": 1, "filtered": false}, {"source": "Jane Hoffswell", "target": "Arjun Srinivasan", "value": 1, "filtered": false}, {"source": "Jane Hoffswell", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Shunan Guo", "value": 2, "filtered": true}, {"source": "Jane Hoffswell", "target": "Gromit Yeuk-Yin Chan", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Ryan A. Rossi", "value": 3, "filtered": true}, {"source": "Jane Hoffswell", "target": "Abhraneel Sarma", "value": 1, "filtered": false}, {"source": "Jane Hoffswell", "target": "Jessica Hullman", "value": 1, "filtered": true}, {"source": "Jane Hoffswell", "target": "Huichen Will Wang", "value": 1, "filtered": false}, {"source": "Duen Horng Chau", "target": "\u00c1ngel Alexander Cabrera", "value": 1, "filtered": true}, {"source": "Duen Horng Chau", "target": "Will Epperson", "value": 1, "filtered": true}, {"source": "Duen Horng Chau", "target": "Fred Hohman", "value": 2, "filtered": true}, {"source": "Duen Horng Chau", "target": "Minsuk Kahng", "value": 3, "filtered": true}, {"source": "Duen Horng Chau", "target": "Jamie Morgenstern", "value": 1, "filtered": true}, {"source": "Duen Horng Chau", "target": "Alex Endert", "value": 1, "filtered": false}, {"source": "Duen Horng Chau", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Yangqiu Song", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Huamin Qu", "value": 12, "filtered": false}, {"source": "Weiwei Cui", "target": "Yun Wang 0012", "value": 5, "filtered": true}, {"source": "Weiwei Cui", "target": "Haidong Zhang", "value": 7, "filtered": true}, {"source": "Weiwei Cui", "target": "Ke Xu", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Dongmei Zhang 0001", "value": 5, "filtered": true}, {"source": "Weiwei Cui", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "He Huang", "value": 3, "filtered": true}, {"source": "Weiwei Cui", "target": "Bei Chen", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Lei Fang 0004", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Jian-Guang Lou", "value": 2, "filtered": true}, {"source": "Weiwei Cui", "target": "Jian Zhao 0010", "value": 2, "filtered": false}, {"source": "Weiwei Cui", "target": "Nan Cao 0001", "value": 2, "filtered": false}, {"source": "Weiwei Cui", "target": "Yong Wang 0021", "value": 2, "filtered": false}, {"source": "Weiwei Cui", "target": "Tan Tang", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Lingyun Yu 0001", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Chunyao Qian", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Shizhao Sun", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Qianwen Wang", "value": 2, "filtered": false}, {"source": "Weiwei Cui", "target": "Jinpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Chin-Yew Lin", "value": 1, "filtered": true}, {"source": "Weiwei Cui", "target": "Yang Shi 0007", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Renzhong Li", "value": 1, "filtered": false}, {"source": "Weiwei Cui", "target": "Xiao Xie", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Dominik Moritz", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Kanit Wongsuphasawat", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Jeffrey Heer", "value": 3, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Alan Lundgard", "value": 1, "filtered": true}, {"source": "Arvind Satyanarayan", "target": "Bongshin Lee", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "John Thompson 0002", "value": 1, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Vidya Setlur", "value": 2, "filtered": false}, {"source": "Arvind Satyanarayan", "target": "Remco Chang", "value": 1, "filtered": false}, {"source": "Liqi Cheng", "target": "Ziao Liu", "value": 1, "filtered": true}, {"source": "Liqi Cheng", "target": "Xiao Xie", "value": 1, "filtered": true}, {"source": "Liqi Cheng", "target": "Moqi He", "value": 1, "filtered": true}, {"source": "Liqi Cheng", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Liqi Cheng", "target": "Yihong Wu 0003", "value": 1, "filtered": true}, {"source": "Liqi Cheng", "target": "Hui Zhang 0051", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Yun Wang 0012", "value": 4, "filtered": true}, {"source": "He Huang", "target": "Bei Chen", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Lei Fang 0004", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Haidong Zhang", "value": 4, "filtered": true}, {"source": "He Huang", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Dongmei Zhang 0001", "value": 4, "filtered": true}, {"source": "He Huang", "target": "Jinpeng Wang 0001", "value": 1, "filtered": true}, {"source": "He Huang", "target": "Chin-Yew Lin", "value": 1, "filtered": true}, {"source": "Steve DiPaola", "target": "Tera Marie Green", "value": 1, "filtered": true}, {"source": "Steve DiPaola", "target": "Ross Maciejewski", "value": 1, "filtered": true}, {"source": "Dongyu Liu", "target": "Huamin Qu", "value": 2, "filtered": false}, {"source": "Dongyu Liu", "target": "Ziyang Guo", "value": 2, "filtered": false}, {"source": "Dongyu Liu", "target": "Fan Du", "value": 1, "filtered": false}, {"source": "Dongyu Liu", "target": "Yanna Lin", "value": 1, "filtered": false}, {"source": "Dongyu Liu", "target": "Sam Yu-Te Lee", "value": 1, "filtered": true}, {"source": "Dongyu Liu", "target": "Aryaman Bahukhandi", "value": 1, "filtered": true}, {"source": "Dongyu Liu", "target": "Kwan-Liu Ma", "value": 2, "filtered": true}, {"source": "Yuqing Yang 0001", "target": "Nan Chen", "value": 1, "filtered": true}, {"source": "Yuqing Yang 0001", "target": "Yuge Zhang", "value": 1, "filtered": true}, {"source": "Yuqing Yang 0001", "target": "Jiahang Xu", "value": 1, "filtered": true}, {"source": "Yuqing Yang 0001", "target": "Kan Ren", "value": 1, "filtered": true}, {"source": "Yunjeong Chang", "target": "Chen Chen 0080", "value": 1, "filtered": true}, {"source": "Yunjeong Chang", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Yunjeong Chang", "target": "Yunhai Wang", "value": 1, "filtered": true}, {"source": "Shunan Guo", "target": "Ke Xu", "value": 1, "filtered": false}, {"source": "Shunan Guo", "target": "Nan Cao 0001", "value": 4, "filtered": false}, {"source": "Shunan Guo", "target": "Zhuochen Jin", "value": 2, "filtered": false}, {"source": "Shunan Guo", "target": "Fan Du", "value": 2, "filtered": false}, {"source": "Shunan Guo", "target": "Nan Chen", "value": 1, "filtered": false}, {"source": "Shunan Guo", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Shunan Guo", "target": "Yang Shi 0007", "value": 1, "filtered": false}, {"source": "Shunan Guo", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Shunan Guo", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Shunan Guo", "target": "Gromit Yeuk-Yin Chan", "value": 1, "filtered": true}, {"source": "Shunan Guo", "target": "Ryan A. Rossi", "value": 2, "filtered": true}, {"source": "Shunan Guo", "target": "Eunyee Koh", "value": 2, "filtered": true}, {"source": "Shunan Guo", "target": "Abhraneel Sarma", "value": 1, "filtered": false}, {"source": "Lars Schuchardt", "target": "Jochen Ehret", "value": 1, "filtered": true}, {"source": "Lars Schuchardt", "target": "Achim Ebert", "value": 1, "filtered": true}, {"source": "Lars Schuchardt", "target": "Heidrun Steinmetz", "value": 1, "filtered": true}, {"source": "Lars Schuchardt", "target": "Hans Hagen", "value": 1, "filtered": true}, {"source": "Alireza Karduni", "target": "Wenwen Dou", "value": 3, "filtered": false}, {"source": "Alireza Karduni", "target": "Arpit Narechania", "value": 1, "filtered": true}, {"source": "Alireza Karduni", "target": "Emily Wall", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Danqing Shi", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Xinyue Xu", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Fuling Sun", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Nan Cao 0001", "value": 8, "filtered": true}, {"source": "Yang Shi 0007", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Yang Shi 0007", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Yang Shi 0007", "target": "Xiaohan Jiao", "value": 3, "filtered": true}, {"source": "Yang Shi 0007", "target": "Yixuan Li", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Yusheng Qi", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Qing Chen 0001", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Siming Chen 0001", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Tian Gao", "value": 2, "filtered": true}, {"source": "Yang Shi 0007", "target": "Bingchang Chen", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Zhuochen Jin", "value": 1, "filtered": true}, {"source": "Yang Shi 0007", "target": "Ke Xu", "value": 1, "filtered": true}, {"source": "Thomas Spengler", "target": "Juraj P\u00e1lenik", "value": 1, "filtered": true}, {"source": "Thomas Spengler", "target": "Helwig Hauser", "value": 1, "filtered": true}, {"source": "Rui Zhang", "target": "Youfu Yan", "value": 1, "filtered": true}, {"source": "Rui Zhang", "target": "Yu Hou", "value": 1, "filtered": true}, {"source": "Rui Zhang", "target": "Yongkang Xiao", "value": 1, "filtered": true}, {"source": "Rui Zhang", "target": "Qianwen Wang", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Jian Zhao 0010", "value": 1, "filtered": false}, {"source": "Nan Cao 0001", "target": "Christopher Collins 0001", "value": 1, "filtered": false}, {"source": "Nan Cao 0001", "target": "Huamin Qu", "value": 5, "filtered": false}, {"source": "Nan Cao 0001", "target": "Danqing Shi", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Xinyue Xu", "value": 2, "filtered": true}, {"source": "Nan Cao 0001", "target": "Fuling Sun", "value": 2, "filtered": true}, {"source": "Nan Cao 0001", "target": "Ke Xu", "value": 3, "filtered": true}, {"source": "Nan Cao 0001", "target": "Zhuochen Jin", "value": 3, "filtered": true}, {"source": "Nan Cao 0001", "target": "Fan Du", "value": 1, "filtered": false}, {"source": "Nan Cao 0001", "target": "Siming Chen 0001", "value": 2, "filtered": true}, {"source": "Nan Cao 0001", "target": "Qing Chen 0001", "value": 3, "filtered": true}, {"source": "Nan Cao 0001", "target": "Zui Chen", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Jiazhe Wang", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Nan Chen", "value": 2, "filtered": true}, {"source": "Nan Cao 0001", "target": "Yun Wang 0012", "value": 1, "filtered": false}, {"source": "Nan Cao 0001", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Nan Cao 0001", "target": "Xiaohan Jiao", "value": 3, "filtered": true}, {"source": "Nan Cao 0001", "target": "Yixuan Li", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Yusheng Qi", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Tian Gao", "value": 2, "filtered": true}, {"source": "Nan Cao 0001", "target": "Bingchang Chen", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Wei Shuai", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Zhe Xu 0007", "value": 1, "filtered": true}, {"source": "Nan Cao 0001", "target": "Hanghang Tong", "value": 1, "filtered": true}, {"source": "Helwig Hauser", "target": "Helmut Doleisch", "value": 5, "filtered": true}, {"source": "Helwig Hauser", "target": "Silvia Miksch", "value": 3, "filtered": false}, {"source": "Helwig Hauser", "target": "Steffen Oeltze", "value": 1, "filtered": true}, {"source": "Helwig Hauser", "target": "Bernhard Preim", "value": 1, "filtered": true}, {"source": "Helwig Hauser", "target": "Kresimir Matkovic", "value": 10, "filtered": false}, {"source": "Helwig Hauser", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Helwig Hauser", "target": "Hans Hagen", "value": 1, "filtered": false}, {"source": "Helwig Hauser", "target": "Wolfgang Freiler", "value": 1, "filtered": false}, {"source": "Helwig Hauser", "target": "M. Eduard Gr\u00f6ller", "value": 2, "filtered": false}, {"source": "Helwig Hauser", "target": "Stefan Bruckner", "value": 2, "filtered": false}, {"source": "Helwig Hauser", "target": "Juraj P\u00e1lenik", "value": 2, "filtered": true}, {"source": "William Ribarsky", "target": "Wenwen Dou", "value": 12, "filtered": true}, {"source": "William Ribarsky", "target": "Remco Chang", "value": 12, "filtered": true}, {"source": "William Ribarsky", "target": "Tera Marie Green", "value": 2, "filtered": false}, {"source": "William Ribarsky", "target": "R. Jordan Crouser", "value": 1, "filtered": false}, {"source": "William Ribarsky", "target": "Thomas Butkiewicz", "value": 3, "filtered": true}, {"source": "William Ribarsky", "target": "Zachary Wartell", "value": 1, "filtered": true}, {"source": "Dominik Fleischmann", "target": "M. Eduard Gr\u00f6ller", "value": 5, "filtered": true}, {"source": "Dominik Fleischmann", "target": "Mat\u00fas Straka", "value": 2, "filtered": true}, {"source": "Dominik Fleischmann", "target": "Alexandra La Cruz", "value": 2, "filtered": true}, {"source": "Dominik Fleischmann", "target": "Arnold K\u00f6chl", "value": 2, "filtered": true}, {"source": "Dominik Fleischmann", "target": "Milos Sr\u00e1mek", "value": 3, "filtered": true}, {"source": "Fabian Sperrle", "target": "Mennatallah El-Assady", "value": 4, "filtered": true}, {"source": "Fabian Sperrle", "target": "Daniel A. Keim", "value": 2, "filtered": true}, {"source": "Fabian Sperrle", "target": "Christopher Collins 0001", "value": 2, "filtered": true}, {"source": "Fabian Sperrle", "target": "Oliver Deussen", "value": 1, "filtered": true}, {"source": "Fabian Sperrle", "target": "Rebecca Kehlbeck", "value": 1, "filtered": false}, {"source": "Fabian Sperrle", "target": "Davide Ceneda", "value": 1, "filtered": false}, {"source": "Dazhen Deng", "target": "Xiao Xie", "value": 4, "filtered": false}, {"source": "Dazhen Deng", "target": "Hui Zhang 0051", "value": 4, "filtered": false}, {"source": "Dazhen Deng", "target": "Zheng Zhou", "value": 1, "filtered": false}, {"source": "Dazhen Deng", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Dazhen Deng", "target": "Xinhuan Shu", "value": 1, "filtered": true}, {"source": "Dazhen Deng", "target": "Yuchen Yang", "value": 1, "filtered": true}, {"source": "Dazhen Deng", "target": "Tan Tang", "value": 2, "filtered": true}, {"source": "Dazhen Deng", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Dazhen Deng", "target": "Aoyu Wu", "value": 2, "filtered": true}, {"source": "Dazhen Deng", "target": "Huamin Qu", "value": 2, "filtered": true}, {"source": "Dazhen Deng", "target": "Yihong Wu 0003", "value": 1, "filtered": false}, {"source": "Dazhen Deng", "target": "Moqi He", "value": 1, "filtered": false}, {"source": "Ross Maciejewski", "target": "Thomas Ertl", "value": 2, "filtered": false}, {"source": "Ross Maciejewski", "target": "Steffen Koch 0001", "value": 1, "filtered": false}, {"source": "Ross Maciejewski", "target": "Hanghang Tong", "value": 2, "filtered": false}, {"source": "Ross Maciejewski", "target": "Tera Marie Green", "value": 1, "filtered": true}, {"source": "Jason D. Hartline", "target": "Yifan Wu 0005", "value": 1, "filtered": true}, {"source": "Jason D. Hartline", "target": "Ziyang Guo", "value": 1, "filtered": true}, {"source": "Jason D. Hartline", "target": "Michalis Mamakos", "value": 1, "filtered": true}, {"source": "Jason D. Hartline", "target": "Jessica Hullman", "value": 2, "filtered": true}, {"source": "Jason D. Hartline", "target": "Paula Kayongo", "value": 1, "filtered": true}, {"source": "Jason D. Hartline", "target": "Glenn Sun", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Leslie M. Blaha", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Lyndsey Franklin", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Alex Endert", "value": 6, "filtered": true}, {"source": "Emily Wall", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Emily Wall", "target": "Subhajit Das 0002", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Ravish Chawla", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Bharath Kalidindi", "value": 1, "filtered": true}, {"source": "Emily Wall", "target": "Hannah Kim 0001", "value": 1, "filtered": false}, {"source": "Emily Wall", "target": "Arpit Narechania", "value": 3, "filtered": true}, {"source": "Jiawan Zhang", "target": "Siming Chen 0001", "value": 1, "filtered": false}, {"source": "Jiawan Zhang", "target": "Shichao Jia", "value": 3, "filtered": true}, {"source": "Jiawan Zhang", "target": "Zeyu Li 0003", "value": 3, "filtered": true}, {"source": "Paola Valdivia", "target": "Paolo Buono", "value": 1, "filtered": true}, {"source": "Paola Valdivia", "target": "Jean-Daniel Fekete", "value": 1, "filtered": true}, {"source": "Paola Valdivia", "target": "Catherine Plaisant", "value": 1, "filtered": true}, {"source": "Ingrid Zukerman", "target": "Samuel Reinders", "value": 1, "filtered": true}, {"source": "Ingrid Zukerman", "target": "Matthew Butler 0002", "value": 1, "filtered": true}, {"source": "Ingrid Zukerman", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Ingrid Zukerman", "target": "Lizhen Qu", "value": 1, "filtered": true}, {"source": "Ingrid Zukerman", "target": "Kim Marriott", "value": 1, "filtered": true}, {"source": "Jiahang Xu", "target": "Nan Chen", "value": 1, "filtered": true}, {"source": "Jiahang Xu", "target": "Yuge Zhang", "value": 1, "filtered": true}, {"source": "Jiahang Xu", "target": "Kan Ren", "value": 1, "filtered": true}, {"source": "Carla Floricel", "target": "Nafiul Nipu", "value": 2, "filtered": true}, {"source": "Carla Floricel", "target": "Mikayla Biggs", "value": 1, "filtered": true}, {"source": "Carla Floricel", "target": "Andrew Wentzel", "value": 2, "filtered": true}, {"source": "Carla Floricel", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Carla Floricel", "target": "Abdallah Sherif Radwan Mohamed", "value": 2, "filtered": true}, {"source": "Carla Floricel", "target": "Clifton David Fuller", "value": 2, "filtered": true}, {"source": "Carla Floricel", "target": "G. Elisabeta Marai", "value": 3, "filtered": true}, {"source": "Junran Yang", "target": "Zehua Zeng", "value": 1, "filtered": true}, {"source": "Junran Yang", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Junran Yang", "target": "Jeffrey Heer", "value": 1, "filtered": true}, {"source": "Junran Yang", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Ignacio P\u00e9rez-Messina", "target": "Davide Ceneda", "value": 1, "filtered": true}, {"source": "Ignacio P\u00e9rez-Messina", "target": "Silvia Miksch", "value": 1, "filtered": true}, {"source": "Kanit Wongsuphasawat", "target": "Dominik Moritz", "value": 2, "filtered": true}, {"source": "Kanit Wongsuphasawat", "target": "Jeffrey Heer", "value": 2, "filtered": true}, {"source": "Kanit Wongsuphasawat", "target": "Anushka Anand", "value": 1, "filtered": true}, {"source": "Kanit Wongsuphasawat", "target": "Jock D. Mackinlay", "value": 1, "filtered": true}, {"source": "Kanit Wongsuphasawat", "target": "Bill Howe", "value": 1, "filtered": true}, {"source": "Chen Chen 0080", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Chen Chen 0080", "target": "Yunhai Wang", "value": 1, "filtered": true}, {"source": "\u00c1ngel Alexander Cabrera", "target": "Will Epperson", "value": 1, "filtered": true}, {"source": "\u00c1ngel Alexander Cabrera", "target": "Fred Hohman", "value": 1, "filtered": true}, {"source": "\u00c1ngel Alexander Cabrera", "target": "Minsuk Kahng", "value": 1, "filtered": true}, {"source": "\u00c1ngel Alexander Cabrera", "target": "Jamie Morgenstern", "value": 1, "filtered": true}, {"source": "Anushka Anand", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Anushka Anand", "target": "Jock D. Mackinlay", "value": 1, "filtered": true}, {"source": "Anushka Anand", "target": "Bill Howe", "value": 1, "filtered": true}, {"source": "Anushka Anand", "target": "Jeffrey Heer", "value": 1, "filtered": true}, {"source": "Anushka Anand", "target": "Vidya Setlur", "value": 1, "filtered": false}, {"source": "Jeremy G. Freeman", "target": "R. Jordan Crouser", "value": 1, "filtered": true}, {"source": "Jeremy G. Freeman", "target": "Remco Chang", "value": 1, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Abhraneel Sarma", "value": 2, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Jessica Hullman", "value": 2, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Gromit Yeuk-Yin Chan", "value": 1, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Eunyee Koh", "value": 2, "filtered": true}, {"source": "Ryan A. Rossi", "target": "Fan Du", "value": 1, "filtered": false}, {"source": "Nan Chen", "target": "Zhuochen Jin", "value": 1, "filtered": false}, {"source": "Nan Chen", "target": "Yuge Zhang", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Kan Ren", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Qing Chen 0001", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Wei Shuai", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Zhe Xu 0007", "value": 1, "filtered": true}, {"source": "Nan Chen", "target": "Hanghang Tong", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Dominik Moritz", "value": 5, "filtered": true}, {"source": "Jeffrey Heer", "target": "Jock D. Mackinlay", "value": 2, "filtered": true}, {"source": "Jeffrey Heer", "target": "Bill Howe", "value": 2, "filtered": true}, {"source": "Jeffrey Heer", "target": "Chenglong Wang", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Greg L. Nelson", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Halden Lin", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Adam M. Smith 0001", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "\u00c7agatay Demiralp", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Michael S. Bernstein", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Bongshin Lee", "value": 1, "filtered": false}, {"source": "Jeffrey Heer", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Jeffrey Heer", "target": "John Thompson 0002", "value": 1, "filtered": false}, {"source": "Jeffrey Heer", "target": "Ziyang Guo", "value": 1, "filtered": false}, {"source": "Jeffrey Heer", "target": "Jessica Hullman", "value": 1, "filtered": false}, {"source": "Jeffrey Heer", "target": "Zehua Zeng", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Leilani Battle", "value": 2, "filtered": true}, {"source": "Jeffrey Heer", "target": "Huichen Will Wang", "value": 1, "filtered": true}, {"source": "Jeffrey Heer", "target": "Mitchell Gordon", "value": 1, "filtered": true}, {"source": "Danqing Shi", "target": "Xinyue Xu", "value": 1, "filtered": true}, {"source": "Danqing Shi", "target": "Fuling Sun", "value": 1, "filtered": true}, {"source": "Bharath Kalidindi", "target": "Subhajit Das 0002", "value": 1, "filtered": true}, {"source": "Bharath Kalidindi", "target": "Ravish Chawla", "value": 1, "filtered": true}, {"source": "Bharath Kalidindi", "target": "Alex Endert", "value": 1, "filtered": true}, {"source": "Jean-Daniel Fekete", "target": "Catherine Plaisant", "value": 3, "filtered": true}, {"source": "Jean-Daniel Fekete", "target": "Paolo Buono", "value": 1, "filtered": true}, {"source": "Jean-Daniel Fekete", "target": "Fanny Chevalier", "value": 1, "filtered": false}, {"source": "Jean-Daniel Fekete", "target": "Dominik Moritz", "value": 1, "filtered": false}, {"source": "Jean-Daniel Fekete", "target": "Yunhai Wang", "value": 1, "filtered": false}, {"source": "Bahador Saket", "target": "Hannah Kim 0001", "value": 1, "filtered": true}, {"source": "Bahador Saket", "target": "Alex Endert", "value": 3, "filtered": true}, {"source": "Songheng Zhang", "target": "Haotian Li 0001", "value": 1, "filtered": true}, {"source": "Songheng Zhang", "target": "Yong Wang 0021", "value": 1, "filtered": true}, {"source": "Songheng Zhang", "target": "Yangqiu Song", "value": 1, "filtered": true}, {"source": "Songheng Zhang", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Qing Chen 0001", "target": "Fuling Sun", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Xinyue Xu", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Zui Chen", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Jiazhe Wang", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Yixuan Li", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Yusheng Qi", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Siming Chen 0001", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Wei Shuai", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Zhe Xu 0007", "value": 1, "filtered": true}, {"source": "Qing Chen 0001", "target": "Hanghang Tong", "value": 1, "filtered": true}, {"source": "Aryaman Bahukhandi", "target": "Sam Yu-Te Lee", "value": 1, "filtered": true}, {"source": "Aryaman Bahukhandi", "target": "Kwan-Liu Ma", "value": 1, "filtered": true}, {"source": "Tian Gao", "target": "Xiaohan Jiao", "value": 2, "filtered": true}, {"source": "Tian Gao", "target": "Bingchang Chen", "value": 1, "filtered": true}, {"source": "Tian Gao", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Tian Gao", "target": "Zhuochen Jin", "value": 1, "filtered": true}, {"source": "Tian Gao", "target": "Ke Xu", "value": 1, "filtered": true}, {"source": "Khairi Reda", "target": "Michael E. Papka", "value": 3, "filtered": true}, {"source": "Khairi Reda", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Khairi Reda", "target": "Jason Leigh", "value": 1, "filtered": false}, {"source": "\u00c7agatay Demiralp", "target": "Michael S. Bernstein", "value": 1, "filtered": true}, {"source": "\u00c7agatay Demiralp", "target": "G. Elisabeta Marai", "value": 1, "filtered": false}, {"source": "Will Epperson", "target": "Fred Hohman", "value": 1, "filtered": true}, {"source": "Will Epperson", "target": "Minsuk Kahng", "value": 1, "filtered": true}, {"source": "Will Epperson", "target": "Jamie Morgenstern", "value": 1, "filtered": true}, {"source": "Will Epperson", "target": "Dominik Moritz", "value": 1, "filtered": false}, {"source": "Tamara Munzner", "target": "Hanspeter Pfister", "value": 2, "filtered": false}, {"source": "Tamara Munzner", "target": "Michael Oppermann", "value": 2, "filtered": true}, {"source": "Xiao Xie", "target": "Hui Zhang 0051", "value": 9, "filtered": true}, {"source": "Xiao Xie", "target": "Zheng Zhou", "value": 2, "filtered": false}, {"source": "Xiao Xie", "target": "Xiangtong Chu", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Shuainan Ye", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Zhutian Chen", "value": 2, "filtered": false}, {"source": "Xiao Xie", "target": "Fan Du", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Tan Tang", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Lvkeshen Shen", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Haijun Xia", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Hanspeter Pfister", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Yihong Wu 0003", "value": 2, "filtered": true}, {"source": "Xiao Xie", "target": "Moqi He", "value": 2, "filtered": true}, {"source": "Xiao Xie", "target": "Ziao Liu", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Xiao Xie", "target": "Renzhong Li", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Yun Wang 0012", "value": 1, "filtered": false}, {"source": "Xiao Xie", "target": "Haidong Zhang", "value": 1, "filtered": false}, {"source": "Yongkang Xiao", "target": "Youfu Yan", "value": 1, "filtered": true}, {"source": "Yongkang Xiao", "target": "Yu Hou", "value": 1, "filtered": true}, {"source": "Yongkang Xiao", "target": "Qianwen Wang", "value": 1, "filtered": true}, {"source": "Silvia Miksch", "target": "Davide Ceneda", "value": 3, "filtered": true}, {"source": "Silvia Miksch", "target": "M. Eduard Gr\u00f6ller", "value": 1, "filtered": false}, {"source": "Silvia Miksch", "target": "Heidrun Schumann", "value": 1, "filtered": false}, {"source": "Silvia Miksch", "target": "Christopher Collins 0001", "value": 1, "filtered": false}, {"source": "Silvia Miksch", "target": "Mennatallah El-Assady", "value": 1, "filtered": false}, {"source": "Silvia Miksch", "target": "Johanna Schmidt", "value": 2, "filtered": true}, {"source": "Silvia Miksch", "target": "Bernhard Pointner", "value": 1, "filtered": true}, {"source": "Vidya Setlur", "target": "Eunyee Koh", "value": 1, "filtered": false}, {"source": "Vidya Setlur", "target": "Arjun Srinivasan", "value": 2, "filtered": true}, {"source": "Vidya Setlur", "target": "Aditeya Pandey", "value": 1, "filtered": true}, {"source": "Ziao Liu", "target": "Moqi He", "value": 1, "filtered": true}, {"source": "Ziao Liu", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Ziao Liu", "target": "Yihong Wu 0003", "value": 1, "filtered": true}, {"source": "Ziao Liu", "target": "Hui Zhang 0051", "value": 1, "filtered": true}, {"source": "Helen Zhao 0001", "target": "Alvitta Ottley", "value": 1, "filtered": true}, {"source": "Helen Zhao 0001", "target": "Quan Lin", "value": 1, "filtered": true}, {"source": "Helen Zhao 0001", "target": "Richard Souvenir", "value": 1, "filtered": true}, {"source": "Helen Zhao 0001", "target": "Alex Endert", "value": 1, "filtered": true}, {"source": "Helen Zhao 0001", "target": "Remco Chang", "value": 1, "filtered": true}, {"source": "Han-Wei Shen", "target": "Junpeng Wang 0001", "value": 4, "filtered": true}, {"source": "Han-Wei Shen", "target": "Liang Gou", "value": 2, "filtered": true}, {"source": "Han-Wei Shen", "target": "Hao Yang 0007", "value": 1, "filtered": true}, {"source": "Han-Wei Shen", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Fan Du", "target": "Catherine Plaisant", "value": 1, "filtered": false}, {"source": "Fan Du", "target": "Zhuochen Jin", "value": 1, "filtered": false}, {"source": "Fan Du", "target": "Yanna Lin", "value": 1, "filtered": false}, {"source": "Fan Du", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Fan Du", "target": "Zehua Zeng", "value": 1, "filtered": true}, {"source": "Fan Du", "target": "Phoebe Moh", "value": 1, "filtered": true}, {"source": "Fan Du", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Fan Du", "target": "Sana Malik", "value": 1, "filtered": true}, {"source": "Fan Du", "target": "Eunyee Koh", "value": 2, "filtered": true}, {"source": "Fan Du", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Fan Du", "target": "Abhraneel Sarma", "value": 1, "filtered": false}, {"source": "Mitchell Gordon", "target": "Huichen Will Wang", "value": 1, "filtered": true}, {"source": "Mitchell Gordon", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Greg L. Nelson", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Halden Lin", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Adam M. Smith 0001", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Bill Howe", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "John Thompson 0002", "value": 1, "filtered": true}, {"source": "Chenglong Wang", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Matthew Butler 0002", "target": "Samuel Reinders", "value": 1, "filtered": true}, {"source": "Matthew Butler 0002", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Matthew Butler 0002", "target": "Lizhen Qu", "value": 1, "filtered": true}, {"source": "Matthew Butler 0002", "target": "Kim Marriott", "value": 1, "filtered": true}, {"source": "Arnold K\u00f6chl", "target": "Mat\u00fas Straka", "value": 2, "filtered": true}, {"source": "Arnold K\u00f6chl", "target": "Alexandra La Cruz", "value": 2, "filtered": true}, {"source": "Arnold K\u00f6chl", "target": "Milos Sr\u00e1mek", "value": 3, "filtered": true}, {"source": "Arnold K\u00f6chl", "target": "M. Eduard Gr\u00f6ller", "value": 4, "filtered": true}, {"source": "Arnold K\u00f6chl", "target": "Stefan Bruckner", "value": 2, "filtered": false}, {"source": "Sana Malik", "target": "Zehua Zeng", "value": 1, "filtered": true}, {"source": "Sana Malik", "target": "Phoebe Moh", "value": 1, "filtered": true}, {"source": "Sana Malik", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Sana Malik", "target": "Eunyee Koh", "value": 1, "filtered": true}, {"source": "Sana Malik", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Sana Malik", "target": "Po-Ming Law", "value": 1, "filtered": false}, {"source": "Arjun Srinivasan", "target": "Steven Mark Drucker", "value": 1, "filtered": true}, {"source": "Arjun Srinivasan", "target": "Alex Endert", "value": 2, "filtered": true}, {"source": "Arjun Srinivasan", "target": "John T. Stasko", "value": 3, "filtered": true}, {"source": "Arjun Srinivasan", "target": "Arpit Narechania", "value": 1, "filtered": false}, {"source": "Arjun Srinivasan", "target": "Aditeya Pandey", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "Mikayla Biggs", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "Andrew Wentzel", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "Abdallah Sherif Radwan Mohamed", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "Clifton David Fuller", "value": 1, "filtered": true}, {"source": "Nafiul Nipu", "target": "G. Elisabeta Marai", "value": 2, "filtered": true}, {"source": "Hannah Kim 0001", "target": "Alex Endert", "value": 4, "filtered": true}, {"source": "Xinyi He", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Xinyi He", "target": "Yun Wang 0012", "value": 1, "filtered": true}, {"source": "Xinyi He", "target": "Mengyu Zhou", "value": 1, "filtered": true}, {"source": "Xinyi He", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Xinyi He", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Xinyi He", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Michael Glueck", "target": "Jian Zhao 0010", "value": 2, "filtered": true}, {"source": "Michael Glueck", "target": "Fanny Chevalier", "value": 5, "filtered": true}, {"source": "Michael Glueck", "target": "Azam Khan", "value": 5, "filtered": true}, {"source": "Michael Glueck", "target": "Simon Breslav", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Steven Mark Drucker", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "John T. Stasko", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Alvitta Ottley", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Quan Lin", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Richard Souvenir", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Remco Chang", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Leslie M. Blaha", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Lyndsey Franklin", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Subhajit Das 0002", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Ravish Chawla", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Fred Hohman", "value": 1, "filtered": false}, {"source": "Alex Endert", "target": "Kristin A. Cook", "value": 2, "filtered": true}, {"source": "Alex Endert", "target": "Nick Cramer", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "David J. Israel", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Joe Bruce", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Russ Burtner", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Aoyu Wu", "value": 1, "filtered": false}, {"source": "Alex Endert", "target": "Yong Wang 0021", "value": 1, "filtered": false}, {"source": "Alex Endert", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Alex Endert", "target": "Arpit Narechania", "value": 4, "filtered": false}, {"source": "Alex Endert", "target": "R. Jordan Crouser", "value": 1, "filtered": true}, {"source": "Alex Endert", "target": "Mennatallah El-Assady", "value": 1, "filtered": false}, {"source": "Liang Gou", "target": "Junpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Liang Gou", "target": "Hao Yang 0007", "value": 1, "filtered": true}, {"source": "Liang Gou", "target": "Jian Zhao 0010", "value": 1, "filtered": false}, {"source": "Liang Gou", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": false}, {"source": "Liang Gou", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Thomas Ertl", "target": "Steffen Koch 0001", "value": 13, "filtered": true}, {"source": "Thomas Ertl", "target": "Tan Tang", "value": 2, "filtered": true}, {"source": "Thomas Ertl", "target": "Renzhong Li", "value": 1, "filtered": true}, {"source": "Thomas Ertl", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Thomas Ertl", "target": "Shuhan Liu", "value": 1, "filtered": true}, {"source": "Thomas Ertl", "target": "Johannes Knittel", "value": 3, "filtered": true}, {"source": "Thomas Ertl", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Thomas Ertl", "target": "Peiran Ren", "value": 1, "filtered": true}, {"source": "Thomas Ertl", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Thomas Ertl", "target": "Fabian Beck 0001", "value": 1, "filtered": false}, {"source": "Tan Tang", "target": "Renzhong Li", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Shuhan Liu", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Johannes Knittel", "value": 2, "filtered": true}, {"source": "Tan Tang", "target": "Steffen Koch 0001", "value": 2, "filtered": true}, {"source": "Tan Tang", "target": "Lingyun Yu 0001", "value": 6, "filtered": true}, {"source": "Tan Tang", "target": "Peiran Ren", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Lu Ying", "value": 3, "filtered": true}, {"source": "Tan Tang", "target": "Lvkeshen Shen", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Xinhuan Shu", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Yuchen Yang", "value": 1, "filtered": true}, {"source": "Tan Tang", "target": "Shuainan Ye", "value": 1, "filtered": false}, {"source": "Zeyu Li 0003", "target": "Shichao Jia", "value": 3, "filtered": true}, {"source": "Remco Chang", "target": "Alvitta Ottley", "value": 4, "filtered": true}, {"source": "Remco Chang", "target": "Quan Lin", "value": 1, "filtered": true}, {"source": "Remco Chang", "target": "Richard Souvenir", "value": 1, "filtered": true}, {"source": "Remco Chang", "target": "Wenwen Dou", "value": 5, "filtered": true}, {"source": "Remco Chang", "target": "Yanna Lin", "value": 1, "filtered": false}, {"source": "Remco Chang", "target": "R. Jordan Crouser", "value": 5, "filtered": true}, {"source": "Remco Chang", "target": "Thomas Butkiewicz", "value": 2, "filtered": true}, {"source": "Remco Chang", "target": "Zachary Wartell", "value": 1, "filtered": true}, {"source": "Remco Chang", "target": "Subhajit Das 0002", "value": 1, "filtered": false}, {"source": "Remco Chang", "target": "Leilani Battle", "value": 2, "filtered": false}, {"source": "Remco Chang", "target": "Tera Marie Green", "value": 1, "filtered": false}, {"source": "Remco Chang", "target": "Tatiana von Landesberger", "value": 1, "filtered": false}, {"source": "Hao Yang 0007", "target": "Junpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Glenn Sun", "target": "Paula Kayongo", "value": 1, "filtered": true}, {"source": "Glenn Sun", "target": "Jessica Hullman", "value": 1, "filtered": true}, {"source": "Klaus Reichenberger", "target": "Thomas Kamps", "value": 2, "filtered": true}, {"source": "Klaus Reichenberger", "target": "Gene Golovchinsky", "value": 2, "filtered": true}, {"source": "Sebastian Bremm", "target": "Tatiana von Landesberger", "value": 4, "filtered": true}, {"source": "Sebastian Bremm", "target": "Kay Hamacher", "value": 2, "filtered": true}, {"source": "Sebastian Bremm", "target": "Olav Lenz", "value": 1, "filtered": true}, {"source": "Sebastian Bremm", "target": "Frank Keul", "value": 1, "filtered": true}, {"source": "Lyndsey Franklin", "target": "Leslie M. Blaha", "value": 1, "filtered": true}, {"source": "Lyndsey Franklin", "target": "R. Jordan Crouser", "value": 1, "filtered": true}, {"source": "Lyndsey Franklin", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Lyndsey Franklin", "target": "Joe Bruce", "value": 1, "filtered": false}, {"source": "Jonathan P. Leidig", "target": "Santhosh Dharmapuri", "value": 1, "filtered": true}, {"source": "Yuchen Yang", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Yuchen Yang", "target": "Xinhuan Shu", "value": 1, "filtered": true}, {"source": "Yuchen Yang", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Ke Xu", "target": "Yun Wang 0012", "value": 3, "filtered": false}, {"source": "Ke Xu", "target": "Haidong Zhang", "value": 2, "filtered": false}, {"source": "Ke Xu", "target": "Dongmei Zhang 0001", "value": 1, "filtered": false}, {"source": "Ke Xu", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Ke Xu", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Ke Xu", "target": "Bingchang Chen", "value": 1, "filtered": true}, {"source": "Ke Xu", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Ke Xu", "target": "Zhuochen Jin", "value": 1, "filtered": true}, {"source": "Ke Xu", "target": "Xiaohan Jiao", "value": 1, "filtered": true}, {"source": "R. Jordan Crouser", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "R. Jordan Crouser", "target": "Leilani Battle", "value": 1, "filtered": false}, {"source": "R. Jordan Crouser", "target": "Alvitta Ottley", "value": 1, "filtered": false}, {"source": "Xinhuan Shu", "target": "Hui Zhang 0051", "value": 1, "filtered": false}, {"source": "Xinhuan Shu", "target": "Aoyu Wu", "value": 1, "filtered": false}, {"source": "Xinhuan Shu", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Xinhuan Shu", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Xinhuan Shu", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Xinhuan Shu", "target": "Fanny Chevalier", "value": 1, "filtered": false}, {"source": "Qianwen Wang", "target": "Zhutian Chen", "value": 2, "filtered": false}, {"source": "Qianwen Wang", "target": "Yun Wang 0012", "value": 1, "filtered": false}, {"source": "Qianwen Wang", "target": "Yong Wang 0021", "value": 3, "filtered": false}, {"source": "Qianwen Wang", "target": "Huamin Qu", "value": 6, "filtered": false}, {"source": "Qianwen Wang", "target": "Nils Gehlenborg", "value": 5, "filtered": true}, {"source": "Qianwen Wang", "target": "Sehi L'Yi", "value": 2, "filtered": true}, {"source": "Qianwen Wang", "target": "Oliver Deussen", "value": 1, "filtered": false}, {"source": "Qianwen Wang", "target": "Yunhai Wang", "value": 1, "filtered": false}, {"source": "Qianwen Wang", "target": "Aditeya Pandey", "value": 1, "filtered": true}, {"source": "Qianwen Wang", "target": "Michelle A. Borkin", "value": 1, "filtered": true}, {"source": "Qianwen Wang", "target": "Youfu Yan", "value": 1, "filtered": true}, {"source": "Qianwen Wang", "target": "Yu Hou", "value": 1, "filtered": true}, {"source": "Zehua Zeng", "target": "Phoebe Moh", "value": 1, "filtered": true}, {"source": "Zehua Zeng", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Zehua Zeng", "target": "Eunyee Koh", "value": 1, "filtered": true}, {"source": "Zehua Zeng", "target": "Leilani Battle", "value": 2, "filtered": true}, {"source": "Zehua Zeng", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Paolo Buono", "target": "Catherine Plaisant", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Renzhong Li", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Johannes Knittel", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Steffen Koch 0001", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Shuhan Liu", "target": "Peiran Ren", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Aritra Dasgupta", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Joon-Yong Lee", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Ryan Wilson", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Robert A. Lafrance", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Kristin A. Cook", "value": 2, "filtered": true}, {"source": "Nick Cramer", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "David J. Israel", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Joe Bruce", "value": 1, "filtered": true}, {"source": "Nick Cramer", "target": "Russ Burtner", "value": 1, "filtered": true}, {"source": "Gunnar L\u00e4th\u00e9n", "target": "Reiner Lenz", "value": 1, "filtered": true}, {"source": "Gunnar L\u00e4th\u00e9n", "target": "Anders Persson", "value": 1, "filtered": true}, {"source": "Gunnar L\u00e4th\u00e9n", "target": "Magnus Borga", "value": 1, "filtered": true}, {"source": "Gene Golovchinsky", "target": "Thomas Kamps", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Zheng Zhou", "value": 2, "filtered": false}, {"source": "Hui Zhang 0051", "target": "Xiangtong Chu", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Shuainan Ye", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Zhutian Chen", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Haijun Xia", "value": 1, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Yihong Wu 0003", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Moqi He", "value": 2, "filtered": true}, {"source": "Hui Zhang 0051", "target": "Yunhai Wang", "value": 1, "filtered": false}, {"source": "Hui Zhang 0051", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Quan Lin", "target": "Alvitta Ottley", "value": 1, "filtered": true}, {"source": "Quan Lin", "target": "Richard Souvenir", "value": 1, "filtered": true}, {"source": "Zhe Xu 0007", "target": "Wei Shuai", "value": 1, "filtered": true}, {"source": "Zhe Xu 0007", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Zhe Xu 0007", "target": "Hanghang Tong", "value": 1, "filtered": true}, {"source": "Alexandra La Cruz", "target": "Mat\u00fas Straka", "value": 2, "filtered": true}, {"source": "Alexandra La Cruz", "target": "Milos Sr\u00e1mek", "value": 2, "filtered": true}, {"source": "Alexandra La Cruz", "target": "M. Eduard Gr\u00f6ller", "value": 2, "filtered": true}, {"source": "Bingchang Chen", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Bingchang Chen", "target": "Zhuochen Jin", "value": 1, "filtered": true}, {"source": "Bingchang Chen", "target": "Xiaohan Jiao", "value": 1, "filtered": true}, {"source": "Huichen Will Wang", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Yu Hou", "target": "Youfu Yan", "value": 1, "filtered": true}, {"source": "Nilaksh Das", "target": "Omar Shaikh", "value": 2, "filtered": true}, {"source": "Nilaksh Das", "target": "Haekyu Park", "value": 2, "filtered": true}, {"source": "Nilaksh Das", "target": "Fred Hohman", "value": 2, "filtered": true}, {"source": "Nilaksh Das", "target": "Minsuk Kahng", "value": 1, "filtered": false}, {"source": "Nilaksh Das", "target": "Duen Horng (Polo) Chau", "value": 2, "filtered": true}, {"source": "Nilaksh Das", "target": "Rahul Duggal", "value": 1, "filtered": true}, {"source": "Nilaksh Das", "target": "Austin P. Wright", "value": 1, "filtered": true}, {"source": "Eston Schweickart", "target": "Nam Wook Kim", "value": 1, "filtered": true}, {"source": "Eston Schweickart", "target": "Mira Dontcheva", "value": 1, "filtered": true}, {"source": "Eston Schweickart", "target": "Wilmot Li", "value": 1, "filtered": true}, {"source": "Eston Schweickart", "target": "Jovan Popovic", "value": 1, "filtered": true}, {"source": "Eston Schweickart", "target": "Hanspeter Pfister", "value": 1, "filtered": true}, {"source": "Xiaohan Jiao", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Xiaohan Jiao", "target": "Zhuochen Jin", "value": 1, "filtered": true}, {"source": "G. Elisabeta Marai", "target": "Mikayla Biggs", "value": 1, "filtered": true}, {"source": "G. Elisabeta Marai", "target": "Andrew Wentzel", "value": 4, "filtered": true}, {"source": "G. Elisabeta Marai", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "G. Elisabeta Marai", "target": "Abdallah Sherif Radwan Mohamed", "value": 2, "filtered": true}, {"source": "G. Elisabeta Marai", "target": "Clifton David Fuller", "value": 3, "filtered": true}, {"source": "Simon Urbanek", "target": "Stephen C. North", "value": 1, "filtered": true}, {"source": "Simon Urbanek", "target": "Carlos Eduardo Scheidegger", "value": 1, "filtered": true}, {"source": "Simon Urbanek", "target": "Gordon Woodhull", "value": 1, "filtered": true}, {"source": "Joe Bruce", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Joe Bruce", "target": "David J. Israel", "value": 1, "filtered": true}, {"source": "Joe Bruce", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "Joe Bruce", "target": "Russ Burtner", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Michelle A. Borkin", "value": 1, "filtered": false}, {"source": "Nam Wook Kim", "target": "Hanspeter Pfister", "value": 4, "filtered": true}, {"source": "Nam Wook Kim", "target": "Mira Dontcheva", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Wilmot Li", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Jovan Popovic", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Shahid Latif", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Zheng Zhou", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Yoon Kim", "value": 1, "filtered": true}, {"source": "Nam Wook Kim", "target": "Fabian Beck 0001", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Johannes Knittel", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Steffen Koch 0001", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Peiran Ren", "value": 1, "filtered": true}, {"source": "Renzhong Li", "target": "Yun Wang 0012", "value": 1, "filtered": false}, {"source": "Renzhong Li", "target": "Haidong Zhang", "value": 1, "filtered": false}, {"source": "Heidrun Schumann", "target": "Thomas Nocke", "value": 2, "filtered": true}, {"source": "Heidrun Schumann", "target": "Magnus Heitzler", "value": 1, "filtered": true}, {"source": "Mira Dontcheva", "target": "Wilmot Li", "value": 1, "filtered": true}, {"source": "Mira Dontcheva", "target": "Jovan Popovic", "value": 1, "filtered": true}, {"source": "Mira Dontcheva", "target": "Hanspeter Pfister", "value": 1, "filtered": true}, {"source": "Peiran Ren", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Peiran Ren", "target": "Johannes Knittel", "value": 1, "filtered": true}, {"source": "Peiran Ren", "target": "Steffen Koch 0001", "value": 1, "filtered": true}, {"source": "Peiran Ren", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Stefan Bruckner", "target": "M. Eduard Gr\u00f6ller", "value": 13, "filtered": false}, {"source": "Stefan Bruckner", "target": "Katja B\u00fchler", "value": 1, "filtered": false}, {"source": "Stefan Bruckner", "target": "Johanna Schmidt", "value": 2, "filtered": false}, {"source": "Stefan Bruckner", "target": "Yngve Sekse Kristiansen", "value": 1, "filtered": true}, {"source": "Stefan Bruckner", "target": "Juraj P\u00e1lenik", "value": 1, "filtered": false}, {"source": "Stefan Bruckner", "target": "Milos Sr\u00e1mek", "value": 1, "filtered": false}, {"source": "Sehi L'Yi", "target": "Nils Gehlenborg", "value": 5, "filtered": true}, {"source": "Sehi L'Yi", "target": "Aditeya Pandey", "value": 1, "filtered": true}, {"source": "Sehi L'Yi", "target": "Michelle A. Borkin", "value": 1, "filtered": true}, {"source": "Oskar Elek", "target": "Joseph N. Burchett", "value": 1, "filtered": true}, {"source": "Oskar Elek", "target": "J. Xavier Prochaska", "value": 1, "filtered": true}, {"source": "Oskar Elek", "target": "Angus G. Forbes", "value": 1, "filtered": true}, {"source": "Michelle A. Borkin", "target": "Hanspeter Pfister", "value": 2, "filtered": false}, {"source": "Michelle A. Borkin", "target": "Aditeya Pandey", "value": 2, "filtered": true}, {"source": "Michelle A. Borkin", "target": "Nils Gehlenborg", "value": 1, "filtered": true}, {"source": "Oliver Deussen", "target": "Daniel A. Keim", "value": 4, "filtered": true}, {"source": "Oliver Deussen", "target": "Yunhai Wang", "value": 15, "filtered": false}, {"source": "Oliver Deussen", "target": "Mennatallah El-Assady", "value": 2, "filtered": true}, {"source": "Oliver Deussen", "target": "Christopher Collins 0001", "value": 2, "filtered": true}, {"source": "Oliver Deussen", "target": "Rebecca Kehlbeck", "value": 3, "filtered": true}, {"source": "Oliver Deussen", "target": "Bongshin Lee", "value": 2, "filtered": false}, {"source": "Oliver Deussen", "target": "Yong Wang 0021", "value": 1, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Katja B\u00fchler", "value": 1, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Johanna Schmidt", "value": 2, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Mat\u00fas Straka", "value": 2, "filtered": true}, {"source": "M. Eduard Gr\u00f6ller", "target": "Milos Sr\u00e1mek", "value": 4, "filtered": true}, {"source": "M. Eduard Gr\u00f6ller", "target": "Helmut Doleisch", "value": 1, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Kresimir Matkovic", "value": 4, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Wenchao Wu", "value": 1, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Yixian Zheng", "value": 1, "filtered": false}, {"source": "M. Eduard Gr\u00f6ller", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Halden Lin", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Halden Lin", "target": "Greg L. Nelson", "value": 1, "filtered": true}, {"source": "Halden Lin", "target": "Adam M. Smith 0001", "value": 1, "filtered": true}, {"source": "Halden Lin", "target": "Bill Howe", "value": 1, "filtered": true}, {"source": "Wenwen Dou", "target": "Thomas Butkiewicz", "value": 2, "filtered": true}, {"source": "Wenwen Dou", "target": "Zachary Wartell", "value": 1, "filtered": true}, {"source": "Davide Ceneda", "target": "Mennatallah El-Assady", "value": 2, "filtered": false}, {"source": "Davide Ceneda", "target": "Christopher Collins 0001", "value": 1, "filtered": false}, {"source": "Andrew Wentzel", "target": "Mikayla Biggs", "value": 1, "filtered": true}, {"source": "Andrew Wentzel", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Andrew Wentzel", "target": "Abdallah Sherif Radwan Mohamed", "value": 2, "filtered": true}, {"source": "Andrew Wentzel", "target": "Clifton David Fuller", "value": 3, "filtered": true}, {"source": "Bernhard Pointner", "target": "Johanna Schmidt", "value": 1, "filtered": true}, {"source": "Joon-Yong Lee", "target": "Aritra Dasgupta", "value": 1, "filtered": true}, {"source": "Joon-Yong Lee", "target": "Ryan Wilson", "value": 1, "filtered": true}, {"source": "Joon-Yong Lee", "target": "Robert A. Lafrance", "value": 1, "filtered": true}, {"source": "Joon-Yong Lee", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Joon-Yong Lee", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Yihong Wu 0003", "target": "Moqi He", "value": 2, "filtered": true}, {"source": "Yihong Wu 0003", "target": "Wenshuo Zhao", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Phoebe Moh", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Gromit Yeuk-Yin Chan", "value": 1, "filtered": true}, {"source": "Eunyee Koh", "target": "Abhraneel Sarma", "value": 1, "filtered": false}, {"source": "Yanna Lin", "target": "Huamin Qu", "value": 2, "filtered": true}, {"source": "Yanna Lin", "target": "Haotian Li 0001", "value": 1, "filtered": true}, {"source": "Yanna Lin", "target": "Leni Yang", "value": 1, "filtered": true}, {"source": "Yanna Lin", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Siming Chen 0001", "target": "Huamin Qu", "value": 1, "filtered": false}, {"source": "Siming Chen 0001", "target": "Yixuan Li", "value": 1, "filtered": true}, {"source": "Siming Chen 0001", "target": "Yusheng Qi", "value": 1, "filtered": true}, {"source": "David J. Israel", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "David J. Israel", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "David J. Israel", "target": "Russ Burtner", "value": 1, "filtered": true}, {"source": "Anders Persson", "target": "Reiner Lenz", "value": 1, "filtered": true}, {"source": "Anders Persson", "target": "Magnus Borga", "value": 1, "filtered": true}, {"source": "Bill Howe", "target": "Dominik Moritz", "value": 2, "filtered": true}, {"source": "Bill Howe", "target": "Jock D. Mackinlay", "value": 1, "filtered": true}, {"source": "Bill Howe", "target": "Greg L. Nelson", "value": 1, "filtered": true}, {"source": "Bill Howe", "target": "Adam M. Smith 0001", "value": 1, "filtered": true}, {"source": "Lizhen Qu", "target": "Samuel Reinders", "value": 1, "filtered": true}, {"source": "Lizhen Qu", "target": "Bongshin Lee", "value": 1, "filtered": true}, {"source": "Lizhen Qu", "target": "Kim Marriott", "value": 1, "filtered": true}, {"source": "Chin-Yew Lin", "target": "Jinpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Chin-Yew Lin", "target": "Yun Wang 0012", "value": 1, "filtered": true}, {"source": "Chin-Yew Lin", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Chin-Yew Lin", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Mikayla Biggs", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Mikayla Biggs", "target": "Abdallah Sherif Radwan Mohamed", "value": 1, "filtered": true}, {"source": "Mikayla Biggs", "target": "Clifton David Fuller", "value": 1, "filtered": true}, {"source": "Achim Ebert", "target": "Jochen Ehret", "value": 1, "filtered": true}, {"source": "Achim Ebert", "target": "Heidrun Steinmetz", "value": 1, "filtered": true}, {"source": "Achim Ebert", "target": "Hans Hagen", "value": 1, "filtered": true}, {"source": "Mengyu Zhou", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Mengyu Zhou", "target": "Yun Wang 0012", "value": 1, "filtered": true}, {"source": "Mengyu Zhou", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Mengyu Zhou", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Mengyu Zhou", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Kay Hamacher", "target": "Tatiana von Landesberger", "value": 2, "filtered": true}, {"source": "Kay Hamacher", "target": "Olav Lenz", "value": 1, "filtered": true}, {"source": "Kay Hamacher", "target": "Frank Keul", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Yun Wang 0012", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Lei Fang 0004", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "Bei Chen", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Thomas Butkiewicz", "target": "Zachary Wartell", "value": 1, "filtered": true}, {"source": "Yuge Zhang", "target": "Kan Ren", "value": 1, "filtered": true}, {"source": "Yusheng Qi", "target": "Yixuan Li", "value": 1, "filtered": true}, {"source": "Ziyang Guo", "target": "Jessica Hullman", "value": 3, "filtered": true}, {"source": "Ziyang Guo", "target": "Yifan Wu 0005", "value": 1, "filtered": true}, {"source": "Ziyang Guo", "target": "Michalis Mamakos", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Miguel Nunes", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Matthias Schlachter", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Sol\u00e9akh\u00e9na Ken", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Kresimir Matkovic", "value": 1, "filtered": true}, {"source": "Katja B\u00fchler", "target": "Anne Laprie", "value": 1, "filtered": true}, {"source": "Haijun Xia", "target": "Zhutian Chen", "value": 2, "filtered": true}, {"source": "Haijun Xia", "target": "Shuainan Ye", "value": 1, "filtered": true}, {"source": "Haijun Xia", "target": "Xiangtong Chu", "value": 1, "filtered": true}, {"source": "Haijun Xia", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Haijun Xia", "target": "Hanspeter Pfister", "value": 1, "filtered": false}, {"source": "Greg L. Nelson", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Greg L. Nelson", "target": "Adam M. Smith 0001", "value": 1, "filtered": true}, {"source": "Andrew E. Johnson 0001", "target": "Jason Leigh", "value": 2, "filtered": true}, {"source": "Andrew E. Johnson 0001", "target": "Yiwen Sun", "value": 1, "filtered": true}, {"source": "Andrew E. Johnson 0001", "target": "Dennis Chau", "value": 1, "filtered": true}, {"source": "Wei Shuai", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Wei Shuai", "target": "Hanghang Tong", "value": 1, "filtered": true}, {"source": "Steven Mark Drucker", "target": "Jessica Hullman", "value": 1, "filtered": false}, {"source": "Steven Mark Drucker", "target": "Bongshin Lee", "value": 3, "filtered": false}, {"source": "Steven Mark Drucker", "target": "John T. Stasko", "value": 1, "filtered": true}, {"source": "Yiwen Sun", "target": "Jason Leigh", "value": 1, "filtered": true}, {"source": "Yiwen Sun", "target": "Dennis Chau", "value": 1, "filtered": true}, {"source": "Shuainan Ye", "target": "Zhutian Chen", "value": 4, "filtered": true}, {"source": "Shuainan Ye", "target": "Xiangtong Chu", "value": 4, "filtered": true}, {"source": "Shuainan Ye", "target": "Huamin Qu", "value": 1, "filtered": true}, {"source": "Shuainan Ye", "target": "Lu Ying", "value": 1, "filtered": false}, {"source": "Wenshuo Zhao", "target": "Moqi He", "value": 1, "filtered": true}, {"source": "Huamin Qu", "target": "Yangqiu Song", "value": 3, "filtered": true}, {"source": "Huamin Qu", "target": "Jian Zhao 0010", "value": 3, "filtered": false}, {"source": "Huamin Qu", "target": "Haotian Li 0001", "value": 3, "filtered": true}, {"source": "Huamin Qu", "target": "Yong Wang 0021", "value": 11, "filtered": true}, {"source": "Huamin Qu", "target": "Wenchao Wu", "value": 3, "filtered": true}, {"source": "Huamin Qu", "target": "Yixian Zheng", "value": 3, "filtered": true}, {"source": "Huamin Qu", "target": "Zhutian Chen", "value": 5, "filtered": true}, {"source": "Huamin Qu", "target": "Yun Wang 0012", "value": 4, "filtered": true}, {"source": "Huamin Qu", "target": "Xiangtong Chu", "value": 1, "filtered": true}, {"source": "Huamin Qu", "target": "Aoyu Wu", "value": 7, "filtered": true}, {"source": "Huamin Qu", "target": "Haidong Zhang", "value": 2, "filtered": true}, {"source": "Huamin Qu", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Huamin Qu", "target": "Leni Yang", "value": 4, "filtered": true}, {"source": "Huamin Qu", "target": "Bongshin Lee", "value": 1, "filtered": false}, {"source": "Huamin Qu", "target": "Po-Ming Law", "value": 1, "filtered": true}, {"source": "Huamin Qu", "target": "Nils Gehlenborg", "value": 1, "filtered": false}, {"source": "Reiner Lenz", "target": "Magnus Borga", "value": 1, "filtered": true}, {"source": "Yixian Zheng", "target": "Wenchao Wu", "value": 3, "filtered": true}, {"source": "Yixian Zheng", "target": "Po-Ming Law", "value": 1, "filtered": true}, {"source": "Subhajit Das 0002", "target": "Ravish Chawla", "value": 1, "filtered": true}, {"source": "Zheng Zhou", "target": "Shahid Latif", "value": 1, "filtered": true}, {"source": "Zheng Zhou", "target": "Yoon Kim", "value": 1, "filtered": true}, {"source": "Zheng Zhou", "target": "Fabian Beck 0001", "value": 1, "filtered": true}, {"source": "Johannes Knittel", "target": "Xinke Wu", "value": 1, "filtered": true}, {"source": "Johannes Knittel", "target": "Steffen Koch 0001", "value": 3, "filtered": true}, {"source": "Johannes Knittel", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Abdallah Sherif Radwan Mohamed", "target": "Lisanne van Dijk", "value": 1, "filtered": true}, {"source": "Abdallah Sherif Radwan Mohamed", "target": "Clifton David Fuller", "value": 2, "filtered": true}, {"source": "Paula Kayongo", "target": "Jessica Hullman", "value": 2, "filtered": true}, {"source": "Shahid Latif", "target": "Yoon Kim", "value": 1, "filtered": true}, {"source": "Shahid Latif", "target": "Fabian Beck 0001", "value": 3, "filtered": true}, {"source": "Yun Wang 0012", "target": "Haidong Zhang", "value": 9, "filtered": true}, {"source": "Yun Wang 0012", "target": "Dongmei Zhang 0001", "value": 6, "filtered": true}, {"source": "Yun Wang 0012", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Yun Wang 0012", "target": "Lei Fang 0004", "value": 1, "filtered": true}, {"source": "Yun Wang 0012", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "Yun Wang 0012", "target": "Zhutian Chen", "value": 1, "filtered": false}, {"source": "Yun Wang 0012", "target": "Yong Wang 0021", "value": 1, "filtered": false}, {"source": "Yun Wang 0012", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Yun Wang 0012", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Yun Wang 0012", "target": "Jinpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Yun Wang 0012", "target": "Jian Zhao 0010", "value": 1, "filtered": false}, {"source": "Aritra Dasgupta", "target": "Ryan Wilson", "value": 1, "filtered": true}, {"source": "Aritra Dasgupta", "target": "Robert A. Lafrance", "value": 1, "filtered": true}, {"source": "Aritra Dasgupta", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Aritra Dasgupta", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Minsuk Kahng", "target": "Duen Horng (Polo) Chau", "value": 3, "filtered": false}, {"source": "Minsuk Kahng", "target": "Omar Shaikh", "value": 1, "filtered": false}, {"source": "Minsuk Kahng", "target": "Haekyu Park", "value": 1, "filtered": false}, {"source": "Minsuk Kahng", "target": "Fred Hohman", "value": 2, "filtered": true}, {"source": "Minsuk Kahng", "target": "Jamie Morgenstern", "value": 1, "filtered": true}, {"source": "Minsuk Kahng", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Lei Fang 0004", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Lei Fang 0004", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Lei Fang 0004", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "Lei Fang 0004", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Hans Hagen", "target": "Helmut Doleisch", "value": 1, "filtered": false}, {"source": "Hans Hagen", "target": "Jochen Ehret", "value": 2, "filtered": true}, {"source": "Hans Hagen", "target": "Heidrun Steinmetz", "value": 1, "filtered": true}, {"source": "Shizhao Sun", "target": "Chunyao Qian", "value": 1, "filtered": true}, {"source": "Shizhao Sun", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "Shizhao Sun", "target": "Haidong Zhang", "value": 1, "filtered": true}, {"source": "Shizhao Sun", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Ryan Wilson", "target": "Robert A. Lafrance", "value": 1, "filtered": true}, {"source": "Ryan Wilson", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Ryan Wilson", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Phoebe Moh", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Phoebe Moh", "target": "Leilani Battle", "value": 1, "filtered": true}, {"source": "Rahul Duggal", "target": "Haekyu Park", "value": 1, "filtered": true}, {"source": "Rahul Duggal", "target": "Austin P. Wright", "value": 1, "filtered": true}, {"source": "Rahul Duggal", "target": "Omar Shaikh", "value": 1, "filtered": true}, {"source": "Rahul Duggal", "target": "Fred Hohman", "value": 1, "filtered": true}, {"source": "Rahul Duggal", "target": "Duen Horng (Polo) Chau", "value": 1, "filtered": true}, {"source": "Mat\u00fas Straka", "target": "Milos Sr\u00e1mek", "value": 2, "filtered": true}, {"source": "Abhraneel Sarma", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Abhraneel Sarma", "target": "Jessica Hullman", "value": 1, "filtered": true}, {"source": "Christopher Collins 0001", "target": "Jian Zhao 0010", "value": 3, "filtered": false}, {"source": "Christopher Collins 0001", "target": "Fanny Chevalier", "value": 2, "filtered": false}, {"source": "Christopher Collins 0001", "target": "Mennatallah El-Assady", "value": 4, "filtered": true}, {"source": "Christopher Collins 0001", "target": "Daniel A. Keim", "value": 3, "filtered": true}, {"source": "Christopher Collins 0001", "target": "Rebecca Kehlbeck", "value": 1, "filtered": true}, {"source": "Wilmot Li", "target": "Jovan Popovic", "value": 1, "filtered": true}, {"source": "Wilmot Li", "target": "Hanspeter Pfister", "value": 1, "filtered": true}, {"source": "Kresimir Matkovic", "target": "Wolfgang Freiler", "value": 2, "filtered": false}, {"source": "Kresimir Matkovic", "target": "Miguel Nunes", "value": 1, "filtered": true}, {"source": "Kresimir Matkovic", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Kresimir Matkovic", "target": "Matthias Schlachter", "value": 1, "filtered": true}, {"source": "Kresimir Matkovic", "target": "Sol\u00e9akh\u00e9na Ken", "value": 1, "filtered": true}, {"source": "Kresimir Matkovic", "target": "Anne Laprie", "value": 1, "filtered": true}, {"source": "Zhutian Chen", "target": "Xiangtong Chu", "value": 4, "filtered": true}, {"source": "Zhutian Chen", "target": "Yong Wang 0021", "value": 2, "filtered": false}, {"source": "Zhutian Chen", "target": "Hanspeter Pfister", "value": 4, "filtered": false}, {"source": "Lisanne van Dijk", "target": "Clifton David Fuller", "value": 1, "filtered": true}, {"source": "Frank Keul", "target": "Olav Lenz", "value": 1, "filtered": true}, {"source": "Frank Keul", "target": "Tatiana von Landesberger", "value": 1, "filtered": true}, {"source": "Stephen C. North", "target": "Daniel A. Keim", "value": 5, "filtered": false}, {"source": "Stephen C. North", "target": "Carlos Eduardo Scheidegger", "value": 1, "filtered": true}, {"source": "Stephen C. North", "target": "Gordon Woodhull", "value": 1, "filtered": true}, {"source": "Yangqiu Song", "target": "Haotian Li 0001", "value": 1, "filtered": true}, {"source": "Yangqiu Song", "target": "Yong Wang 0021", "value": 1, "filtered": true}, {"source": "Olav Lenz", "target": "Tatiana von Landesberger", "value": 1, "filtered": true}, {"source": "Michalis Mamakos", "target": "Yifan Wu 0005", "value": 1, "filtered": true}, {"source": "Michalis Mamakos", "target": "Jessica Hullman", "value": 1, "filtered": true}, {"source": "Zhuochen Jin", "target": "Ying Chen", "value": 1, "filtered": true}, {"source": "Anne Laprie", "target": "Miguel Nunes", "value": 1, "filtered": true}, {"source": "Anne Laprie", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Anne Laprie", "target": "Matthias Schlachter", "value": 1, "filtered": true}, {"source": "Anne Laprie", "target": "Sol\u00e9akh\u00e9na Ken", "value": 1, "filtered": true}, {"source": "Richard Souvenir", "target": "Alvitta Ottley", "value": 1, "filtered": true}, {"source": "Helmut Doleisch", "target": "Steffen Oeltze", "value": 2, "filtered": true}, {"source": "Helmut Doleisch", "target": "Bernhard Preim", "value": 2, "filtered": true}, {"source": "Helmut Doleisch", "target": "Wolfgang Freiler", "value": 2, "filtered": true}, {"source": "Helmut Doleisch", "target": "Reyk Hillert", "value": 1, "filtered": true}, {"source": "Helmut Doleisch", "target": "Walter Schubert", "value": 1, "filtered": true}, {"source": "Wenchao Wu", "target": "Po-Ming Law", "value": 1, "filtered": true}, {"source": "Reyk Hillert", "target": "Steffen Oeltze", "value": 1, "filtered": true}, {"source": "Reyk Hillert", "target": "Wolfgang Freiler", "value": 1, "filtered": true}, {"source": "Reyk Hillert", "target": "Bernhard Preim", "value": 1, "filtered": true}, {"source": "Reyk Hillert", "target": "Walter Schubert", "value": 1, "filtered": true}, {"source": "Haidong Zhang", "target": "Dongmei Zhang 0001", "value": 7, "filtered": true}, {"source": "Haidong Zhang", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Haidong Zhang", "target": "Jian-Guang Lou", "value": 2, "filtered": true}, {"source": "Haidong Zhang", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Haidong Zhang", "target": "Chunyao Qian", "value": 1, "filtered": true}, {"source": "Haidong Zhang", "target": "Leni Yang", "value": 1, "filtered": false}, {"source": "Haidong Zhang", "target": "Jinpeng Wang 0001", "value": 1, "filtered": true}, {"source": "Sol\u00e9akh\u00e9na Ken", "target": "Miguel Nunes", "value": 1, "filtered": true}, {"source": "Sol\u00e9akh\u00e9na Ken", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Sol\u00e9akh\u00e9na Ken", "target": "Matthias Schlachter", "value": 1, "filtered": true}, {"source": "Zui Chen", "target": "Fuling Sun", "value": 1, "filtered": true}, {"source": "Zui Chen", "target": "Xinyue Xu", "value": 1, "filtered": true}, {"source": "Zui Chen", "target": "Jiazhe Wang", "value": 1, "filtered": true}, {"source": "Mennatallah El-Assady", "target": "Daniel A. Keim", "value": 3, "filtered": true}, {"source": "Mennatallah El-Assady", "target": "Rebecca Kehlbeck", "value": 2, "filtered": true}, {"source": "Mennatallah El-Assady", "target": "Arpit Narechania", "value": 1, "filtered": false}, {"source": "Steffen Oeltze", "target": "Bernhard Preim", "value": 2, "filtered": true}, {"source": "Steffen Oeltze", "target": "Wolfgang Freiler", "value": 1, "filtered": true}, {"source": "Steffen Oeltze", "target": "Walter Schubert", "value": 1, "filtered": true}, {"source": "Magnus Heitzler", "target": "Thomas Nocke", "value": 1, "filtered": true}, {"source": "Bongshin Lee", "target": "John T. Stasko", "value": 2, "filtered": false}, {"source": "Bongshin Lee", "target": "Jessica Hullman", "value": 1, "filtered": false}, {"source": "Bongshin Lee", "target": "John Thompson 0002", "value": 2, "filtered": true}, {"source": "Bongshin Lee", "target": "Aoyu Wu", "value": 1, "filtered": false}, {"source": "Bongshin Lee", "target": "Catherine Plaisant", "value": 1, "filtered": false}, {"source": "Bongshin Lee", "target": "Yunhai Wang", "value": 3, "filtered": true}, {"source": "Bongshin Lee", "target": "Samuel Reinders", "value": 1, "filtered": true}, {"source": "Bongshin Lee", "target": "Kim Marriott", "value": 1, "filtered": true}, {"source": "J. Xavier Prochaska", "target": "Joseph N. Burchett", "value": 1, "filtered": true}, {"source": "J. Xavier Prochaska", "target": "Angus G. Forbes", "value": 1, "filtered": true}, {"source": "Yifan Wu 0005", "target": "Jessica Hullman", "value": 2, "filtered": true}, {"source": "Omar Shaikh", "target": "Haekyu Park", "value": 2, "filtered": true}, {"source": "Omar Shaikh", "target": "Fred Hohman", "value": 2, "filtered": true}, {"source": "Omar Shaikh", "target": "Duen Horng (Polo) Chau", "value": 2, "filtered": true}, {"source": "Omar Shaikh", "target": "Austin P. Wright", "value": 1, "filtered": true}, {"source": "Michael E. Papka", "target": "Kwan-Liu Ma", "value": 1, "filtered": false}, {"source": "Michael E. Papka", "target": "Jason Leigh", "value": 1, "filtered": false}, {"source": "Duen Horng (Polo) Chau", "target": "Haekyu Park", "value": 3, "filtered": true}, {"source": "Duen Horng (Polo) Chau", "target": "Fred Hohman", "value": 3, "filtered": true}, {"source": "Duen Horng (Polo) Chau", "target": "Austin P. Wright", "value": 1, "filtered": true}, {"source": "Yunhai Wang", "target": "Dominik Moritz", "value": 1, "filtered": false}, {"source": "Yunhai Wang", "target": "Rebecca Kehlbeck", "value": 2, "filtered": false}, {"source": "Yunhai Wang", "target": "Yong Wang 0021", "value": 1, "filtered": false}, {"source": "Leilani Battle", "target": "Tak Yeon Lee", "value": 1, "filtered": true}, {"source": "Leilani Battle", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Lvkeshen Shen", "target": "Lu Ying", "value": 1, "filtered": true}, {"source": "Lvkeshen Shen", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Lu Ying", "target": "Lingyun Yu 0001", "value": 2, "filtered": true}, {"source": "Chunyao Qian", "target": "Jian-Guang Lou", "value": 1, "filtered": true}, {"source": "Chunyao Qian", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Jock D. Mackinlay", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Haekyu Park", "target": "Fred Hohman", "value": 3, "filtered": true}, {"source": "Haekyu Park", "target": "Austin P. Wright", "value": 1, "filtered": true}, {"source": "Haotian Li 0001", "target": "Yong Wang 0021", "value": 1, "filtered": true}, {"source": "Haotian Li 0001", "target": "Leni Yang", "value": 1, "filtered": true}, {"source": "Haotian Li 0001", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Joseph N. Burchett", "target": "Angus G. Forbes", "value": 1, "filtered": true}, {"source": "Fanny Chevalier", "target": "Jian Zhao 0010", "value": 5, "filtered": true}, {"source": "Fanny Chevalier", "target": "Azam Khan", "value": 5, "filtered": true}, {"source": "Fanny Chevalier", "target": "Simon Breslav", "value": 2, "filtered": true}, {"source": "Jinpeng Wang 0001", "target": "Dongmei Zhang 0001", "value": 1, "filtered": true}, {"source": "Austin P. Wright", "target": "Fred Hohman", "value": 1, "filtered": true}, {"source": "Gordon Woodhull", "target": "Carlos Eduardo Scheidegger", "value": 1, "filtered": true}, {"source": "Dongmei Zhang 0001", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Dongmei Zhang 0001", "target": "Jian-Guang Lou", "value": 2, "filtered": true}, {"source": "Dongmei Zhang 0001", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Yoon Kim", "target": "Fabian Beck 0001", "value": 1, "filtered": true}, {"source": "Matthias Schlachter", "target": "Miguel Nunes", "value": 1, "filtered": true}, {"source": "Matthias Schlachter", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Miguel Nunes", "target": "Benjamin Rowland", "value": 1, "filtered": true}, {"source": "Kim Marriott", "target": "Hanspeter Pfister", "value": 1, "filtered": false}, {"source": "Kim Marriott", "target": "Samuel Reinders", "value": 1, "filtered": true}, {"source": "Robert A. Lafrance", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Robert A. Lafrance", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Xinke Wu", "target": "Steffen Koch 0001", "value": 1, "filtered": true}, {"source": "Xinke Wu", "target": "Lingyun Yu 0001", "value": 1, "filtered": true}, {"source": "Hanghang Tong", "target": "Guande Wu", "value": 1, "filtered": true}, {"source": "Leni Yang", "target": "Aoyu Wu", "value": 1, "filtered": true}, {"source": "Lingyun Yu 0001", "target": "Steffen Koch 0001", "value": 1, "filtered": true}, {"source": "Heidrun Steinmetz", "target": "Jochen Ehret", "value": 1, "filtered": true}, {"source": "Catherine Plaisant", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Azam Khan", "target": "Jian Zhao 0010", "value": 2, "filtered": true}, {"source": "Azam Khan", "target": "Simon Breslav", "value": 2, "filtered": true}, {"source": "Fred Hohman", "target": "Jamie Morgenstern", "value": 1, "filtered": true}, {"source": "Fred Hohman", "target": "Dominik Moritz", "value": 1, "filtered": false}, {"source": "Nils Gehlenborg", "target": "Hanspeter Pfister", "value": 7, "filtered": false}, {"source": "Nils Gehlenborg", "target": "Aditeya Pandey", "value": 1, "filtered": true}, {"source": "Sam Yu-Te Lee", "target": "Kwan-Liu Ma", "value": 1, "filtered": true}, {"source": "Jessica Hullman", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Rebecca Kehlbeck", "target": "Daniel A. Keim", "value": 1, "filtered": true}, {"source": "Arpit Narechania", "target": "John T. Stasko", "value": 1, "filtered": false}, {"source": "Fuling Sun", "target": "Xinyue Xu", "value": 2, "filtered": true}, {"source": "Fuling Sun", "target": "Jiazhe Wang", "value": 1, "filtered": true}, {"source": "Kwan-Liu Ma", "target": "Jian Zhao 0010", "value": 2, "filtered": false}, {"source": "Kwan-Liu Ma", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": false}, {"source": "Adam M. Smith 0001", "target": "Dominik Moritz", "value": 1, "filtered": true}, {"source": "Russ Burtner", "target": "Kristin A. Cook", "value": 1, "filtered": true}, {"source": "Russ Burtner", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "Wolfgang Freiler", "target": "Bernhard Preim", "value": 1, "filtered": true}, {"source": "Wolfgang Freiler", "target": "Walter Schubert", "value": 1, "filtered": true}, {"source": "Kristin A. Cook", "target": "Samuel H. Payne", "value": 1, "filtered": true}, {"source": "Kristin A. Cook", "target": "Michael Wolverton", "value": 1, "filtered": true}, {"source": "Xinyue Xu", "target": "Jiazhe Wang", "value": 1, "filtered": true}, {"source": "Aoyu Wu", "target": "Yong Wang 0021", "value": 1, "filtered": false}, {"source": "Guande Wu", "target": "Gromit Yeuk-Yin Chan", "value": 1, "filtered": true}, {"source": "Simon Breslav", "target": "Jian Zhao 0010", "value": 1, "filtered": true}, {"source": "Jason Leigh", "target": "Dennis Chau", "value": 1, "filtered": true}, {"source": "Bernhard Preim", "target": "Walter Schubert", "value": 1, "filtered": true}, {"source": "John T. Stasko", "target": "John Thompson 0002", "value": 1, "filtered": false}, {"source": "Jian-Guang Lou", "target": "Xiaoyu Zhang 0014", "value": 1, "filtered": true}, {"source": "Jovan Popovic", "target": "Hanspeter Pfister", "value": 1, "filtered": true}]}
const { Graph } = G6
const graph = new Graph({
container: 'network_82835d33-b39f-4e0d-a051-3275aacc5080',
autoFit: 'view',
data,
layout: {
type: 'force-atlas2',
preventOverlap: true,
kr: 20,
center: [250, 250],
},
behaviors: ['drag-canvas', 'zoom-canvas', 'drag-element'],
edge: {
style: {
lineWidth: 2,
opacity: d => d.filtered ? 1 : 0.4},
},
node: {
style: {
labelText: d => d.id,
},
},
});
graph.render();
</script>
    
        </body>
        </html>
        
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The co-authorship network visualizes dense local clusters and several bridging hubs. After simple disambiguation we found ~310 canonical authors and recommended pruning low-weight edges (coauthorship weight < 2) for clarity. Hub measures identify central connectors such as Yingcai Wu, Alex Endert, Nan Cao, Huamin Qu, Jeffrey Heer and Dominik Moritz — these individuals act as collaboration anchors that link research clusters (e.g., Zhejiang University clusters, Microsoft Research Asia / Tongji groups, and North American mixed-initiative teams). The network is international and interdisciplinary: strong intra-country activity exists (USA and China), but many influential collaborations cross institutions and regions, showing that AutoVis is driven both by focused labs and by cross-cutting partnerships rather than a single closed cadre.</p>
        </p>
      </div>
    </div>
  </div>
</section>
<section class='mb-16'>
  <h2 class='text-3xl font-bold mb-6 border-b border-gray-200 pb-2'>
    6. Representative papers, case studies & directions
  </h2>
  <div class='space-y-10'>
    <div class='prose prose-lg prose-gray max-w-none'>
      <p class="text-gray-700 leading-relaxed mb-4">Representative milestone papers combine methodological novelty with usable systems and evaluation artifacts; examples include Voyager (2015) for visualization recommendation and exploration, Draco (2018) for formalizing design knowledge as constraints, and Design Space of Visualization Tasks (2013) for task-oriented framing. These works helped shift the field toward recommendation, mixed-initiative interfaces and model-guided design. Case studies such as Voder, Calliope and KG4Vis illustrate applied AutoVis research across NLG, storytelling and knowledge-graph approaches. Open gaps are clear: many papers lack rigorous comparative evaluation (nearly half of the records have no explicit evaluation), reproducibility signals are sparse, and longitudinal lifecycle analyses are constrained by static metrics. Recommended directions are: establish standard evaluation benchmarks and protocols for AutoVis (task-based and dataset-scale), improve reproducibility (artifact and code sharing, replicability stamps), deepen human-in-the-loop studies that quantify when automation helps vs harms, scale algorithms to real-world data sizes and latency constraints, and study deployment and fairness (e.g., auditing ML-based recommenders). Researchers should prioritize evaluation and reproducibility alongside novel systems.</p>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-6-1'></div>
            
  <div id="vis_9ef604c3"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "layer": [{"data": {"name": "data-9ee8ca1ceb908e3c3b19548c6274874a"}, "mark": {"type": "circle", "opacity": 0.6}, "encoding": {"color": {"field": "Award_bonus", "legend": {"format": "d", "values": [0, 1]}, "scale": {"domain": [0, 1], "range": ["steelblue", "gold"]}, "title": "Award", "type": "nominal"}, "size": {"field": "Downloads_Xplore", "scale": {"range": [20, 200]}, "title": "Downloads (size)", "type": "quantitative"}, "tooltip": [{"field": "Year", "type": "quantitative"}, {"field": "Title", "type": "nominal"}, {"field": "composite", "format": ".3f", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "type": "quantitative"}, {"field": "Downloads_Xplore", "type": "quantitative"}, {"field": "Award", "type": "nominal"}], "x": {"field": "Year", "title": "Year", "type": "quantitative"}, "y": {"field": "composite", "title": "Composite importance score", "type": "quantitative"}}}, {"data": {"name": "data-b9820031a9596b0071123fb510e3c2da"}, "mark": {"type": "point", "color": "red", "filled": true, "shape": "diamond", "size": 180}, "encoding": {"tooltip": [{"field": "Year", "type": "quantitative"}, {"field": "Title", "type": "nominal"}, {"field": "composite", "format": ".3f", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "type": "quantitative"}, {"field": "Downloads_Xplore", "type": "quantitative"}, {"field": "Award", "type": "nominal"}], "x": {"field": "Year", "type": "quantitative"}, "y": {"field": "composite", "type": "quantitative"}}}, {"data": {"name": "data-b9820031a9596b0071123fb510e3c2da"}, "mark": {"type": "text", "dx": 5, "dy": -10, "fontSize": 11, "fontWeight": "bold"}, "encoding": {"text": {"field": "label", "type": "nominal"}, "x": {"field": "Year", "type": "quantitative"}, "y": {"field": "composite", "type": "quantitative"}}}], "height": 320, "resolve": {"scale": {"size": "independent"}}, "title": "Annotated timeline: AutoVis milestone candidates (by composite importance)", "width": 800, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-9ee8ca1ceb908e3c3b19548c6274874a": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.013906185139061852, "composite": 0.012733499377334994, "window_start": 1995, "window_label": "1995-1997"}, {"Conference": "Vis", "Year": 1995, "Title": "Subverting structure: data-driven diagram generation", "DOI": "10.1109/visual.1995.480815", "Link": "http://dx.doi.org/10.1109/VISUAL.1995.480815", "FirstPage": 217.0, "LastPage": null, "PaperType": "C", "Abstract": "Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.", "AuthorNames-Deduped": "Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps", "AuthorNames": "G. Golovchinsky;T. Kamps;K. Reichenberger", "AuthorAffiliation": "Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany", "InternalReferences": "10.1109/infvis.1995.528681;10.1109/infvis.1995.528681", "AuthorKeywords": null, "AminerCitationCount": 21.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 66.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.0, "composite": 0.003424657534246575, "window_start": 1995, "window_label": "1995-1997"}, {"Conference": "Vis", "Year": 2004, "Title": "Non-linear model fitting to parameterize diseased blood vessels", "DOI": "10.1109/visual.2004.72", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.72", "FirstPage": 393.0, "LastPage": 400.0, "PaperType": "C", "Abstract": "Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.", "AuthorNames-Deduped": "Alexandra La Cruz;Mat\u00fas Straka;Arnold K\u00f6chl;Milos Sr\u00e1mek;M. Eduard Gr\u00f6ller;Dominik Fleischmann", "AuthorNames": "A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann", "AuthorAffiliation": "University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA", "InternalReferences": "10.1109/visual.2001.964555", "AuthorKeywords": "Visualization, Segmentation, Blood Vessel Detection", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 141.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.015566625155666251, "composite": 0.013231631382316313, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "Vis", "Year": 2004, "Title": "Context-Adaptive Mobile Visualization and Information Management", "DOI": "10.1109/visual.2004.19", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.19", "FirstPage": 8.0, "LastPage": 8.0, "PaperType": "M", "Abstract": "This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.", "AuthorNames-Deduped": "Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen", "AuthorNames": "J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen", "AuthorAffiliation": "Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 11.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 4.0, "Downloads_Xplore": 172.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.022000830220008302, "composite": 0.010024906600249066, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.08426733084267331, "composite": 0.06637608966376089, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1506849315068493, "dl_norm": 0.12453300124533001, "composite": 0.11270236612702365, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "InfoVis", "Year": 2008, "Title": "Multi-Focused Geospatial Analysis Using Probes", "DOI": "10.1109/tvcg.2008.149", "Link": "http://dx.doi.org/10.1109/TVCG.2008.149", "FirstPage": 1165.0, "LastPage": 1172.0, "PaperType": "J", "Abstract": "Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.", "AuthorNames-Deduped": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorNames": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorAffiliation": "UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center", "InternalReferences": "10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102", "AuthorKeywords": "Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes", "AminerCitationCount": 73.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 20.0, "Downloads_Xplore": 648.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11643835616438356, "dl_norm": 0.12079701120797011, "composite": 0.0944582814445828, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "VAST", "Year": 2009, "Title": "Articulate: a conversational interface for visual analytics", "DOI": "10.1109/vast.2009.5333099", "Link": "http://dx.doi.org/10.1109/VAST.2009.5333099", "FirstPage": 233.0, "LastPage": 234.0, "PaperType": "M", "Abstract": "While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.", "AuthorNames-Deduped": "Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau", "AuthorNames": "Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau", "AuthorAffiliation": "Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA", "InternalReferences": "0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148", "AuthorKeywords": null, "AminerCitationCount": 3.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 9.0, "Downloads_Xplore": 267.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.041718555417185554, "composite": 0.01594022415940224, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "VAST", "Year": 2010, "Title": "ALIDA: Using machine learning for intent discernment in visual analytics interfaces", "DOI": "10.1109/vast.2010.5650854", "Link": "http://dx.doi.org/10.1109/VAST.2010.5650854", "FirstPage": 223.0, "LastPage": 224.0, "PaperType": "M", "Abstract": "In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.", "AuthorNames-Deduped": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorNames": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorAffiliation": "School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada", "InternalReferences": null, "AuthorKeywords": "artificial intelligence, cognition, intent discernment, volume rendering", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 3.0, "PubsCited_CrossRef": 6.0, "Downloads_Xplore": 391.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.010273972602739725, "dl_norm": 0.06745537567455376, "composite": 0.02537359900373599, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14819427148194272, "composite": 0.059869240348692405, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "VAST", "Year": 2011, "Title": "Exploring agent-based simulations using temporal graphs", "DOI": "10.1109/vast.2011.6102469", "Link": "http://dx.doi.org/10.1109/VAST.2011.6102469", "FirstPage": 271.0, "LastPage": 272.0, "PaperType": "M", "Abstract": "Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.", "AuthorNames-Deduped": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorNames": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorAffiliation": "Tufts University, USA;Tufts University, USA;Tufts University, USA", "InternalReferences": "0.1109/infvis.2005.1532126", "AuthorKeywords": null, "AminerCitationCount": 0.0, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 8.0, "Downloads_Xplore": 163.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.02013283520132835, "composite": 0.006039850560398506, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "SciVis", "Year": 2012, "Title": "Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization", "DOI": "10.1109/tvcg.2012.203", "Link": "http://dx.doi.org/10.1109/TVCG.2012.203", "FirstPage": 2345.0, "LastPage": 2354.0, "PaperType": "J", "Abstract": "Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.", "AuthorNames-Deduped": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorNames": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorAffiliation": "Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\u00f6ping University, Sweden", "InternalReferences": "10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123", "AuthorKeywords": "Direct volume rendering, transfer functions, vessel visualization", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 34.0, "Downloads_Xplore": 513.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04794520547945205, "dl_norm": 0.09277708592777086, "composite": 0.05180572851805729, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.4931506849315068, "dl_norm": 1.0, "composite": 0.5465753424657533, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "Finding Waldo: Learning about Users from their Interactions", "DOI": "10.1109/tvcg.2014.2346575", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346575", "FirstPage": 1663.0, "LastPage": 1672.0, "PaperType": "J", "Abstract": "Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.", "AuthorNames-Deduped": "Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorNames": "Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorAffiliation": "Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U", "InternalReferences": "10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204", "AuthorKeywords": "User Interactions, Analytic Provenance, Visualization, Applied Machine Learning", "AminerCitationCount": 145.0, "CitationCount_CrossRef": 95.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 2226.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.3253424657534247, "dl_norm": 0.44831880448318806, "composite": 0.29716687422166876, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2014, "Title": "Learning Perceptual Kernels for Visualization Design", "DOI": "10.1109/tvcg.2014.2346978", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346978", "FirstPage": 1933.0, "LastPage": 1942.0, "PaperType": "J", "Abstract": "Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.", "AuthorNames-Deduped": "\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorNames": "\u00c7a\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorAffiliation": "Stanford University;Stanford University;University of Washington", "InternalReferences": "10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186", "AuthorKeywords": "Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding", "AminerCitationCount": 129.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 1247.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.273972602739726, "dl_norm": 0.24512245745122457, "composite": 0.21052303860523036, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "DOI": "10.1109/vast.2014.7042485", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042485", "FirstPage": 93.0, "LastPage": 102.0, "PaperType": "C", "Abstract": "Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.", "AuthorNames-Deduped": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorNames": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorAffiliation": "GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt", "InternalReferences": "10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225", "AuthorKeywords": "Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 331.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.055002075550020756, "composite": 0.030199252801992527, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging", "DOI": "10.1109/vast.2014.7042481", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042481", "FirstPage": 53.0, "LastPage": 62.0, "PaperType": "C", "Abstract": "For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.", "AuthorNames-Deduped": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorNames": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorAffiliation": "VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria", "InternalReferences": "10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569", "AuthorKeywords": "MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 29.0, "Downloads_Xplore": 300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.048567870485678705, "composite": 0.02313200498132005, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 1.0, "dl_norm": 0.8802407638024077, "composite": 0.7640722291407223, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2015, "Title": "Mixed-initiative visual analytics using task-driven recommendations", "DOI": "10.1109/vast.2015.7347625", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347625", "FirstPage": 9.0, "LastPage": 16.0, "PaperType": "C", "Abstract": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.", "AuthorNames-Deduped": "Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorNames": "Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486", "AuthorKeywords": "mixed-initiative visual analytics, task modeling, recommender systems, sensemaking", "AminerCitationCount": 36.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 36.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.08561643835616438, "dl_norm": 0.15545869655458697, "composite": 0.08944582814445828, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2015, "Title": "Collaborative visual analysis with RCloud", "DOI": "10.1109/vast.2015.7347627", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347627", "FirstPage": 25.0, "LastPage": 32.0, "PaperType": "C", "Abstract": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.", "AuthorNames-Deduped": "Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorNames": "Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorAffiliation": "Infovisible;University of Arizona;AT&T Labs;AT&T Labs", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185", "AuthorKeywords": "visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 404.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.0701535907015359, "composite": 0.03303237858032378, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "SciVis", "Year": 2015, "Title": "Automated visualization workflow for simulation experiments", "DOI": "10.1109/scivis.2015.7429509", "Link": "http://dx.doi.org/10.1109/SciVis.2015.7429509", "FirstPage": 153.0, "LastPage": 154.0, "PaperType": "M", "Abstract": "Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.", "AuthorNames-Deduped": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorNames": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorAffiliation": "School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 1.0, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 12.0, "Downloads_Xplore": 137.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.01473640514736405, "composite": 0.006133250311332503, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.3150684931506849, "dl_norm": 0.4522623495226235, "composite": 0.2932129514321295, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1952054794520548, "dl_norm": 0.5635118306351183, "composite": 0.2666562889165629, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods", "DOI": "10.1109/tvcg.2016.2598544", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598544", "FirstPage": 271.0, "LastPage": 280.0, "PaperType": "J", "Abstract": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.", "AuthorNames-Deduped": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne", "AuthorNames": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory", "InternalReferences": "10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591", "AuthorKeywords": "trust;transparency;familiarity;uncertainty;biological data analysis", "AminerCitationCount": 41.0, "CitationCount_CrossRef": 41.0, "PubsCited_CrossRef": 41.0, "Downloads_Xplore": 1844.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1404109589041096, "dl_norm": 0.36903279369032793, "composite": 0.1809153175591532, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations", "DOI": "10.1109/tvcg.2016.2598543", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598543", "FirstPage": 261.0, "LastPage": 270.0, "PaperType": "J", "Abstract": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.", "AuthorNames-Deduped": "Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorNames": "Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorAffiliation": "Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research", "InternalReferences": "10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878", "AuthorKeywords": "Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization", "AminerCitationCount": 39.0, "CitationCount_CrossRef": 33.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 2188.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11301369863013698, "dl_norm": 0.44043171440431717, "composite": 0.18863636363636363, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "DOI": "10.1109/tvcg.2016.2598460", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598460", "FirstPage": 121.0, "LastPage": 130.0, "PaperType": "J", "Abstract": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.", "AuthorNames-Deduped": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook", "AuthorNames": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook", "AuthorAffiliation": "Smith College;Smith College;Smith College;Smith College", "InternalReferences": "10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467", "AuthorKeywords": "Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 16.0, "PubsCited_CrossRef": 87.0, "Downloads_Xplore": 978.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0547945205479452, "dl_norm": 0.18929016189290163, "composite": 0.08418430884184308, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "DOI": "10.1109/tvcg.2016.2599378", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2599378", "FirstPage": 231.0, "LastPage": 240.0, "PaperType": "J", "Abstract": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.", "AuthorNames-Deduped": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorNames": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology", "InternalReferences": "10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1", "AuthorKeywords": "Centralized matching;matching visualization;interaction techniques;visual analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 32.0, "Downloads_Xplore": 557.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.10190950601909506, "composite": 0.04427148194271482, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2017, "Title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics", "DOI": "10.1109/vast.2017.8585669", "Link": "http://dx.doi.org/10.1109/VAST.2017.8585669", "FirstPage": 104.0, "LastPage": 115.0, "PaperType": "C", "Abstract": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.", "AuthorNames-Deduped": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorNames": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorAffiliation": "Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486", "AuthorKeywords": "cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General", "AminerCitationCount": 115.0, "CitationCount_CrossRef": 70.0, "PubsCited_CrossRef": 80.0, "Downloads_Xplore": 1801.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.23972602739726026, "dl_norm": 0.36010792860107926, "composite": 0.2278953922789539, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2017, "Title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "DOI": "10.1109/tvcg.2017.2745078", "Link": "http://dx.doi.org/10.1109/TVCG.2017.2745078", "FirstPage": 288.0, "LastPage": 297.0, "PaperType": "J", "Abstract": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "AuthorNames-Deduped": "Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorNames": "Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136", "AuthorKeywords": "Mixed-initiative visual analytics,multi-attribute ranking,user interaction", "AminerCitationCount": 0.0, "CitationCount_CrossRef": 52.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1535.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1780821917808219, "dl_norm": 0.304898298048983, "composite": 0.18051058530510583, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.6061643835616438, "dl_norm": 0.6583644665836447, "composite": 0.7005915317559153, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.4143835616438356, "dl_norm": 0.5969281859692819, "composite": 0.3862702366127023, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.3116438356164384, "dl_norm": 0.5821917808219178, "composite": 0.5304794520547946, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2018, "Title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "DOI": "10.1109/tvcg.2018.2864769", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "FirstPage": 374.0, "LastPage": 384.0, "PaperType": "J", "Abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "AuthorNames-Deduped": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001", "AuthorNames": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins", "AuthorAffiliation": "Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493", "AuthorKeywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "AminerCitationCount": 47.0, "CitationCount_CrossRef": 40.0, "PubsCited_CrossRef": 69.0, "Downloads_Xplore": 1217.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.136986301369863, "dl_norm": 0.23889580738895808, "composite": 0.14016189290161893, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2019, "Title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "DOI": "10.1109/vast47406.2019.8986948", "Link": "http://dx.doi.org/10.1109/VAST47406.2019.8986948", "FirstPage": 46.0, "LastPage": 56.0, "PaperType": "C", "Abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.", "AuthorNames-Deduped": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorNames": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718", "AuthorKeywords": "Machine learning fairness,visual analytics,intersectional bias,subgroup discovery", "AminerCitationCount": 107.0, "CitationCount_CrossRef": 106.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 2108.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.363013698630137, "dl_norm": 0.42382731423827313, "composite": 0.30865504358655044, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.24315068493150685, "dl_norm": 0.5386052303860523, "composite": 0.28315691158156914, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2019, "Title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "DOI": "10.1109/tvcg.2019.2934654", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "FirstPage": 1001.0, "LastPage": 1011.0, "PaperType": "J", "Abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "AuthorNames-Deduped": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen", "AuthorNames": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen", "AuthorAffiliation": "University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493", "AuthorKeywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "AminerCitationCount": 30.0, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 59.0, "Downloads_Xplore": 1300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06164383561643835, "dl_norm": 0.25612287256122873, "composite": 0.10765877957658779, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.273972602739726, "dl_norm": 0.7592361975923619, "composite": 0.3647571606475716, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "DOI": "10.1109/tvcg.2020.3030467", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "FirstPage": 294.0, "LastPage": 303.0, "PaperType": "J", "Abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "AuthorNames-Deduped": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorNames": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorAffiliation": "Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University", "InternalReferences": "10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487", "AuthorKeywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 36.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 1931.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1232876712328767, "dl_norm": 0.3870900788709008, "composite": 0.17777085927770858, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics", "DOI": "10.1109/tvcg.2020.3030448", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030448", "FirstPage": 443.0, "LastPage": 452.0, "PaperType": "J", "Abstract": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.", "AuthorNames-Deduped": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810", "AuthorKeywords": "Infographics,automatic visualization", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1004.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.10616438356164383, "dl_norm": 0.1946865919468659, "composite": 0.11148816936488168, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2020, "Title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations", "DOI": "10.1109/tvcg.2020.3030387", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030387", "FirstPage": 495.0, "LastPage": 505.0, "PaperType": "J", "Abstract": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.", "AuthorNames-Deduped": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorNames": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorAffiliation": "Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia", "InternalReferences": "10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757", "AuthorKeywords": "visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 28.0, "PubsCited_CrossRef": 81.0, "Downloads_Xplore": 1243.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0958904109589041, "dl_norm": 0.24429223744292236, "composite": 0.12123287671232875, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2020, "Title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "DOI": "10.1109/tvcg.2020.3030347", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "FirstPage": 1775.0, "LastPage": 1785.0, "PaperType": "J", "Abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "AuthorNames-Deduped": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorNames": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorAffiliation": "Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France", "InternalReferences": "10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477", "AuthorKeywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 17.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 754.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05821917808219178, "dl_norm": 0.1427978414279784, "composite": 0.07194894146948941, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "SciVis", "Year": 2020, "Title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization", "DOI": "10.1109/tvcg.2020.3030407", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030407", "FirstPage": 806.0, "LastPage": 816.0, "PaperType": "J", "Abstract": "This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorNames": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorAffiliation": "Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz", "InternalReferences": "10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259", "AuthorKeywords": "Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 10.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 530.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.03424657534246575, "dl_norm": 0.09630552096305521, "composite": 0.046014943960149435, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "SciVis", "Year": 2020, "Title": "IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection", "DOI": "10.1109/tvcg.2020.3030389", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030389", "FirstPage": 775.0, "LastPage": 784.0, "PaperType": "J", "Abstract": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.", "AuthorNames-Deduped": "Juraj P\u00e1lenik;Thomas Spengler;Helwig Hauser", "AuthorNames": "Juraj Palenik;Thomas Spengler;Helwig Hauser", "AuthorAffiliation": "University of Bergen;University of Bergen;University of Bergen", "InternalReferences": "10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190", "AuthorKeywords": "visual parameter space exploration,scientific modelling,atmospheric convection", "AminerCitationCount": 1.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 417.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.07285180572851806, "composite": 0.02528019925280199, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.2363013698630137, "dl_norm": 0.7027812370278124, "composite": 0.5289850560398506, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.21232876712328766, "dl_norm": 0.5246990452469904, "composite": 0.2635740971357409, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Augmenting Sports Videos with VisCommentator", "DOI": "10.1109/tvcg.2021.3114806", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114806", "FirstPage": 824.0, "LastPage": 834.0, "PaperType": "J", "Abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.", "AuthorNames-Deduped": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu", "AuthorNames": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458", "AuthorKeywords": "Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling", "AminerCitationCount": 19.0, "CitationCount_CrossRef": 42.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2151.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.14383561643835616, "dl_norm": 0.4327521793275218, "composite": 0.4017434620174346, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Kori: Interactive Synthesis of Text and Charts in Data Documents", "DOI": "10.1109/tvcg.2021.3114802", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114802", "FirstPage": 184.0, "LastPage": 194.0, "PaperType": "J", "Abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.", "AuthorNames-Deduped": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim", "AuthorNames": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim", "AuthorAffiliation": "University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 1308.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11643835616438356, "dl_norm": 0.2577833125778331, "composite": 0.1355541718555417, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "VizLinter: A Linter and Fixer Framework for Data Visualization", "DOI": "10.1109/tvcg.2021.3114804", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114804", "FirstPage": 206.0, "LastPage": 216.0, "PaperType": "J", "Abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001", "AuthorNames": "Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China", "InternalReferences": "10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166", "AuthorKeywords": "Visualization Linting,Automated Visualization Design,Visualization Optimization", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 32.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 1919.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1095890410958904, "dl_norm": 0.3845994188459942, "composite": 0.17017434620174346, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation", "DOI": "10.1109/tvcg.2021.3114826", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114826", "FirstPage": 162.0, "LastPage": 172.0, "PaperType": "J", "Abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.", "AuthorNames-Deduped": "Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001", "AuthorNames": "Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang", "AuthorAffiliation": "Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1788.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.10616438356164383, "dl_norm": 0.3574097135740971, "composite": 0.16030510585305105, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms", "DOI": "10.1109/tvcg.2021.3114814", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114814", "FirstPage": 346.0, "LastPage": 356.0, "PaperType": "J", "Abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.", "AuthorNames-Deduped": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorNames": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorAffiliation": "University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Visualization Tools,Visualization Recommendation Algorithms", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1106.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.08561643835616438, "dl_norm": 0.21585720215857201, "composite": 0.3075653798256538, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Towards Visual Explainable Active Learning for Zero-Shot Classification", "DOI": "10.1109/tvcg.2021.3114793", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114793", "FirstPage": 791.0, "LastPage": 801.0, "PaperType": "J", "Abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.", "AuthorNames-Deduped": "Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang", "AuthorNames": "Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang", "AuthorAffiliation": "College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China", "InternalReferences": "10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818", "AuthorKeywords": "Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 76.0, "Downloads_Xplore": 1775.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.354711498547115, "composite": 0.1475093399750934, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "DOI": "10.1109/tvcg.2021.3114810", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114810", "FirstPage": 151.0, "LastPage": 161.0, "PaperType": "J", "Abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA", "InternalReferences": "10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437", "AuthorKeywords": "Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 21.0, "PubsCited_CrossRef": 105.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.07191780821917808, "dl_norm": 0.15545869655458697, "composite": 0.08259651307596513, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs", "DOI": "10.1109/tvcg.2021.3114877", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114877", "FirstPage": 400.0, "LastPage": 410.0, "PaperType": "J", "Abstract": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.", "AuthorNames-Deduped": "Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185", "AuthorKeywords": "Glyph-based visualization,machine learning,automatic visualization", "AminerCitationCount": 10.0, "CitationCount_CrossRef": 19.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1101.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06506849315068493, "dl_norm": 0.21481942714819427, "composite": 0.09698007471980075, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks", "DOI": "10.1109/tvcg.2021.3114858", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114858", "FirstPage": 813.0, "LastPage": 823.0, "PaperType": "J", "Abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.", "AuthorNames-Deduped": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau", "AuthorNames": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau", "AuthorAffiliation": "Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States", "InternalReferences": "10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509", "AuthorKeywords": "Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding", "AminerCitationCount": 8.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 830.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05136986301369863, "dl_norm": 0.1585720215857202, "composite": 0.07325653798256537, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics", "DOI": "10.1109/tvcg.2021.3114820", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114820", "FirstPage": 486.0, "LastPage": 496.0, "PaperType": "J", "Abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.", "AuthorNames-Deduped": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorNames": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorAffiliation": "Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493", "AuthorKeywords": "transformers,word embeddings,literature review,web scraper,dataset,visual analytics", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 1087.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05136986301369863, "dl_norm": 0.21191365711913657, "composite": 0.08925902864259028, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "A Mixed-Initiative Approach to Reusing Infographic Charts", "DOI": "10.1109/tvcg.2021.3114856", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114856", "FirstPage": 173.0, "LastPage": 183.0, "PaperType": "J", "Abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.", "AuthorNames-Deduped": "Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732", "AuthorKeywords": "Infographics,Reusable templates,Graphic design,Automatic visualization", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 13.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1211.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04452054794520548, "dl_norm": 0.23765047737650477, "composite": 0.09355541718555416, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization", "DOI": "10.1109/tvcg.2021.3114782", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114782", "FirstPage": 129.0, "LastPage": 139.0, "PaperType": "J", "Abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorNames": "Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136", "AuthorKeywords": "Task-oriented insight preservation,responsive visualization", "AminerCitationCount": 6.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 751.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14217517642175176, "composite": 0.058063511830635114, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Semantic Snapping for Guided Multi-View Visualization Design", "DOI": "10.1109/tvcg.2021.3114860", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114860", "FirstPage": 43.0, "LastPage": 53.0, "PaperType": "J", "Abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \u201caligned\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.", "AuthorNames-Deduped": "Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner", "AuthorNames": "Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner", "AuthorAffiliation": "Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338", "AuthorKeywords": "Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views", "AminerCitationCount": 5.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 896.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.17227065172270653, "composite": 0.06366749688667497, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Visualization Equilibrium", "DOI": "10.1109/tvcg.2021.3114842", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114842", "FirstPage": 465.0, "LastPage": 474.0, "PaperType": "J", "Abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.", "AuthorNames-Deduped": "Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907", "AuthorKeywords": "Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 647.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.12058945620589456, "composite": 0.039601494396014944, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2022, "Title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "DOI": "10.1109/tvcg.2022.3209447", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209447", "FirstPage": 331.0, "LastPage": 341.0, "PaperType": "J", "Abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.", "AuthorNames-Deduped": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254", "AuthorKeywords": "Glyph-based visualization,metaphor,machine learning,automatic visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 68.0, "Downloads_Xplore": 1095.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06164383561643835, "dl_norm": 0.21357409713574096, "composite": 0.09489414694894147, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning", "DOI": "10.1109/tvcg.2022.3209468", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209468", "FirstPage": 690.0, "LastPage": 700.0, "PaperType": "J", "Abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.", "AuthorNames-Deduped": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorNames": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234", "AuthorKeywords": "Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 83.0, "Downloads_Xplore": 1671.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04794520547945205, "dl_norm": 0.3331257783312578, "composite": 0.12391033623910336, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning", "DOI": "10.1109/tvcg.2022.3209461", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209461", "FirstPage": 95.0, "LastPage": 105.0, "PaperType": "J", "Abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.", "AuthorNames-Deduped": "Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001", "AuthorNames": "Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen", "AuthorAffiliation": "School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543", "AuthorKeywords": "Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 1276.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.2511415525114155, "composite": 0.08904109589041095, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0273972602739726, "dl_norm": 0.30531340805313406, "composite": 0.30529265255292654, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.5020755500207555, "composite": 0.16260896637608965, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback", "DOI": "10.1109/tvcg.2023.3327363", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327363", "FirstPage": 131.0, "LastPage": 141.0, "PaperType": "J", "Abstract": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.", "AuthorNames-Deduped": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorNames": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorAffiliation": "New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774", "AuthorKeywords": "Narrative visualization,visual storytelling,conversational agent", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 816.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.15566625155666253, "composite": 0.05868617683686177, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks", "DOI": "10.1109/tvcg.2023.3327170", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327170", "FirstPage": 944.0, "LastPage": 954.0, "PaperType": "J", "Abstract": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.", "AuthorNames-Deduped": "Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorNames": "Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China", "InternalReferences": "0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 665.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.12432544624325446, "composite": 0.04585927770859278, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Mystique: Deconstructing SVG Charts for Layout Reuse", "DOI": "10.1109/tvcg.2023.3327354", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327354", "FirstPage": 447.0, "LastPage": 457.0, "PaperType": "J", "Abstract": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.", "AuthorNames-Deduped": "Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001", "AuthorNames": "Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu", "AuthorAffiliation": "University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States", "InternalReferences": "0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877", "AuthorKeywords": "Chart layout,Reuse,Reverse-engineering,Deconstruction", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 481.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.08613532586135327, "composite": 0.029265255292652555, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning", "DOI": "10.1109/tvcg.2023.3327200", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327200", "FirstPage": 1172.0, "LastPage": 1182.0, "PaperType": "J", "Abstract": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.", "AuthorNames-Deduped": "Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001", "AuthorNames": "Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865", "AuthorKeywords": "Time Series Data,Exploratory Visual Analysis,Reinforcement Learning", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 1050.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.20423412204234123, "composite": 0.06469489414694894, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining", "DOI": "10.1109/tvcg.2023.3326939", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326939", "FirstPage": 1227.0, "LastPage": 1237.0, "PaperType": "J", "Abstract": "Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA", "InternalReferences": "0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475", "AuthorKeywords": "Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 82.0, "Downloads_Xplore": 361.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.06122872561228725, "composite": 0.02179327521793275, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco", "DOI": "10.1109/tvcg.2023.3326527", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326527", "FirstPage": 1063.0, "LastPage": 1073.0, "PaperType": "J", "Abstract": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\u2014a framework to model visualization knowledge\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.", "AuthorNames-Deduped": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorNames": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorAffiliation": "University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA", "InternalReferences": "0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814", "AuthorKeywords": "Graphical Perception Studies,Visualization Recommendation Algorithms", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 371.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.06330427563304275, "composite": 0.022415940224159398, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "The Rational Agent Benchmark for Data Visualization", "DOI": "10.1109/tvcg.2023.3326513", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326513", "FirstPage": 338.0, "LastPage": 347.0, "PaperType": "J", "Abstract": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.", "AuthorNames-Deduped": "Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781", "AuthorKeywords": "Evaluation,decision-making,rational agent,scoring rule", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 33.0, "Downloads_Xplore": 434.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.07638024076380241, "composite": 0.0263387297633873, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams", "DOI": "10.1109/tvcg.2023.3326925", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326925", "FirstPage": 562.0, "LastPage": 572.0, "PaperType": "J", "Abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001", "AuthorNames": "Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428", "AuthorKeywords": "Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 78.0, "Downloads_Xplore": 662.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.12370278123702781, "composite": 0.03882316313823163, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization", "DOI": "10.1109/tvcg.2023.3326583", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326583", "FirstPage": 934.0, "LastPage": 943.0, "PaperType": "J", "Abstract": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell", "AuthorNames": "Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization,responsive visualization,mixed-initiative authoring", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 330.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.0547945205479452, "composite": 0.018150684931506848, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Visual Analytics for Understanding Draco's Knowledge Base", "DOI": "10.1109/tvcg.2023.3326912", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326912", "FirstPage": 392.0, "LastPage": 402.0, "PaperType": "J", "Abstract": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.", "AuthorNames-Deduped": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorNames": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorAffiliation": "VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Visual Analytics,Hypergraph visualization,Rule-based recommendation systems", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 53.0, "Downloads_Xplore": 365.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.06205894562058946, "composite": 0.018617683686176837, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring", "DOI": "10.1109/tvcg.2023.3326585", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326585", "FirstPage": 1128.0, "LastPage": 1138.0, "PaperType": "J", "Abstract": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.", "AuthorNames-Deduped": "Chenglong Wang;John Thompson 0002;Bongshin Lee", "AuthorNames": "Chenglong Wang;John Thompson;Bongshin Lee", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA", "InternalReferences": "0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369", "AuthorKeywords": "AI,visualization authoring,data transformation,programming by example,natural language,large language model", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 63.0, "Downloads_Xplore": 893.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0, "dl_norm": 0.17164798671647988, "composite": 0.25149439601494394, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Guided Visual Analytics for Image Selection in Time and Space", "DOI": "10.1109/tvcg.2023.3326572", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326572", "FirstPage": 66.0, "LastPage": 75.0, "PaperType": "J", "Abstract": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.", "AuthorNames-Deduped": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorNames": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorAffiliation": "TU Wien, Austria;TU Wien, Austria;TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 37.0, "Downloads_Xplore": 1208.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.23702781237027812, "composite": 0.07110834371108343, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts", "DOI": "10.1109/tvcg.2024.3456398", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456398", "FirstPage": 481.0, "LastPage": 491.0, "PaperType": "J", "Abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.", "AuthorNames-Deduped": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorNames": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorAffiliation": "University of California, USA;University of California, USA;University of California, USA;University of California, USA", "InternalReferences": "10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878", "AuthorKeywords": "Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 65.0, "Downloads_Xplore": 386.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.06641760066417601, "composite": 0.02163760896637609, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration", "DOI": "10.1109/tvcg.2024.3456364", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456364", "FirstPage": 547.0, "LastPage": 557.0, "PaperType": "J", "Abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.", "AuthorNames-Deduped": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorNames": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorAffiliation": "Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA", "InternalReferences": "10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798", "AuthorKeywords": "Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 632.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.003424657534246575, "dl_norm": 0.11747613117476131, "composite": 0.2369551681195517, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models", "DOI": "10.1109/tvcg.2024.3456320", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456320", "FirstPage": 1301.0, "LastPage": 1311.0, "PaperType": "J", "Abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.", "AuthorNames-Deduped": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001", "AuthorNames": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Visualization evaluation,automatic visualization,,,large language models,benchmark", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 75.0, "Downloads_Xplore": 625.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.003424657534246575, "dl_norm": 0.11602324616023246, "composite": 0.23651930261519305, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech", "DOI": "10.1109/tvcg.2024.3456358", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456358", "FirstPage": 864.0, "LastPage": 874.0, "PaperType": "J", "Abstract": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.", "AuthorNames-Deduped": "Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorNames": "Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorAffiliation": "Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia", "InternalReferences": "10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275", "AuthorKeywords": "Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 184.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0, "dl_norm": 0.024491490244914902, "composite": 0.2073474470734745, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models", "DOI": "10.1109/tvcg.2024.3456350", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456350", "FirstPage": 710.0, "LastPage": 720.0, "PaperType": "J", "Abstract": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\u2014DracoGPT-Rank and DracoGPT-Recommend\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.", "AuthorNames-Deduped": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorNames": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorAffiliation": "University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527", "AuthorKeywords": "Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 378.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.0647571606475716, "composite": 0.01942714819427148, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent", "DOI": "10.1109/tvcg.2024.3456200", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456200", "FirstPage": 23.0, "LastPage": 33.0, "PaperType": "J", "Abstract": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.", "AuthorNames-Deduped": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu", "AuthorNames": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu", "AuthorAffiliation": "Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806", "AuthorKeywords": "Sports visualization,tactic board,,,tactical analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 813.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.15504358655043587, "composite": 0.04651307596513076, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations", "DOI": "10.1109/tvcg.2024.3456182", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456182", "FirstPage": 754.0, "LastPage": 764.0, "PaperType": "J", "Abstract": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6", "AuthorNames-Deduped": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorNames": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorAffiliation": "Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA", "InternalReferences": "10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513", "AuthorKeywords": "Visual inference,statistical rationality,,,human-machine collaboration", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 71.0, "Downloads_Xplore": 138.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.014943960149439602, "composite": 0.00448318804483188, "window_start": 2022, "window_label": "2022-2024"}], "data-b9820031a9596b0071123fb510e3c2da": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.013906185139061852, "composite": 0.012733499377334994, "window_start": 1995, "window_label": "1995-1997", "label": "Towards a generative theory of diagram design"}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.08426733084267331, "composite": 0.06637608966376089, "window_start": 2004, "window_label": "2004-2006", "label": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Coll"}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1506849315068493, "dl_norm": 0.12453300124533001, "composite": 0.11270236612702365, "window_start": 2007, "window_label": "2007-2009", "label": "Interactive Visual Analysis of Perfusion Data"}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14819427148194272, "composite": 0.059869240348692405, "window_start": 2010, "window_label": "2010-2012", "label": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fl"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 1.0, "dl_norm": 0.8802407638024077, "composite": 0.7640722291407223, "window_start": 2013, "window_label": "2013-2015", "label": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendati"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.6061643835616438, "dl_norm": 0.6583644665836447, "composite": 0.7005915317559153, "window_start": 2016, "window_label": "2016-2018", "label": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extens"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.2363013698630137, "dl_norm": 0.7027812370278124, "composite": 0.5289850560398506, "window_start": 2019, "window_label": "2019-2021", "label": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation"}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0273972602739726, "dl_norm": 0.30531340805313406, "composite": 0.30529265255292654, "window_start": 2022, "window_label": "2022-2024", "label": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_9ef604c3", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
        <p class='text-sm text-gray-600 mt-2 text-center'>
          <p class="text-gray-700 leading-relaxed mb-4">The annotated timeline and milestone table make two points: influential AutoVis advances cluster around tool releases and formal-framework papers (Voyager in 2015 and Draco in 2018 are standout peaks), and impact is highly uneven — a small number of papers account for most citations and downloads. Evaluation coverage is uneven: about 47% of papers report no formal evaluation, while 24% report user studies and only ~11% use benchmarks; this distribution argues strongly for community investments in shared benchmarks and reproducible artifact pipelines. Awards and other notability signals are rare but correlate with high composite importance scores. Together the timeline and table suggest that the field’s most durable advances paired clear algorithmic or representational ideas with usable mixed-initiative interfaces and empirical validation.</p>
        </p>
      </div>
    </div>
    <div class='my-8'>
      <div class='bg-gray-50 rounded-lg shadow-sm p-4'>
        <div class='mb-4 flex justify-center'>
          <div class='inline-block mx-auto overflow-x-auto max-w-full'>
            <div id='vis-6-2'></div>
            
  <div id="vis_0783cfc5"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300, "stroke": null}}, "hconcat": [{"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "TitleShort", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 320}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Year", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 50}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "ImpactTier", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 70}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "EvalType", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 120}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Notable", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 50}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Contrib", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 400}], "data": {"name": "data-9ae1b835bca3256898a6320a0e06271e"}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-9ae1b835bca3256898a6320a0e06271e": [{"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 5028.0, "rank": 1, "TitleShort": "A Design Space of Visualization Tasks", "Contrib": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to p", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 4599.0, "rank": 2, "TitleShort": "Voyager: Exploratory Analysis via Faceted Browsing of Visual", "Contrib": "General visualization tools typically require manual specification of views: analysts must select data variables and the", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3804.0, "rank": 3, "TitleShort": "Calliope: Automatic Visual Data Story Generation from a Spre", "Contrib": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used ", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3521.0, "rank": 4, "TitleShort": "KG4Vis: A Knowledge Graph-Based Approach for Visualization R", "Contrib": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general user", "ImpactTier": "High", "EvalType": "case study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3415.0, "rank": 5, "TitleShort": "Formalizing Visualization Design Knowledge as Constraints: A", "Contrib": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3063.0, "rank": 6, "TitleShort": "Augmenting Visualizations with Interactive Data Facts to Fac", "Contrib": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capa", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2962.0, "rank": 7, "TitleShort": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Net", "Contrib": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acqui", "ImpactTier": "High", "EvalType": "case study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2838.0, "rank": 8, "TitleShort": "Visualization by Demonstration: An Interaction Paradigm for ", "Contrib": "Although data visualization tools continue to improve, during the data exploration process many of them require users to", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2732.0, "rank": 9, "TitleShort": "Text-to-Viz: Automatic Generation of Infographics from Propo", "Contrib": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memo", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2656.0, "rank": 10, "TitleShort": "Accessible Visualization via Natural Language Descriptions: ", "Contrib": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights,", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2492.0, "rank": 11, "TitleShort": "GenoREC: A Recommendation System for Interactive Genomics Da", "Contrib": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large", "ImpactTier": "Medium", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2337.0, "rank": 12, "TitleShort": "Data-Driven Guides: Supporting Expressive Design for Informa", "Contrib": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visual", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis_0783cfc5", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<script>
document.addEventListener('DOMContentLoaded', function () {
  const toc = document.getElementById('toc');
  if (!toc) return;

  const headers = document.querySelectorAll('article h2, article h3');
  headers.forEach(h => {
    let id = h.textContent.trim().replace(/\s+/g, '-').toLowerCase();
    h.setAttribute('id', id);

    const link = document.createElement('a');
    link.href = '#' + id;
    link.textContent = h.textContent;
    link.className = (h.tagName === 'H2'
      ? 'block font-semibold mb-1'      : 'block ml-4 text-gray-500') + ' hover:text-primary-600';

    toc.appendChild(link);
  });
});
</script>
    </article>
  </main>
  <footer class='border-t mt-12 py-6 text-center text-sm text-gray-500'>
    © 2025 Agentic VIS
  </footer>
  <script>
  document.addEventListener('DOMContentLoaded', function () {
  const tocList = document.getElementById('toc-list');
  const headers = document.querySelectorAll('article h2, article h3');
  const links = [];

  headers.forEach((h, idx) => {
    let id = h.textContent.trim().replace(/\s+/g, '-').toLowerCase() + '-' + idx;
    h.setAttribute('id', id);

    const li = document.createElement('li');
    li.className = 'flex items-center space-x-2';

    const dot = document.createElement('span');
    dot.className = 'w-2 h-2 rounded-full border border-gray-300';
    li.appendChild(dot);

    const link = document.createElement('a');
    link.href = '#' + id;

    const text = h.textContent.trim();
    link.textContent = text.length > 40 ? text.slice(0, 37) + '…' : text;
    link.title = text;

    link.className = 'block truncate text-gray-400 hover:text-gray-600';
    if (h.tagName === 'H3') {
      link.className += ' ml-4';
    }

    link.addEventListener('click', function (e) {
      e.preventDefault();
      const target = document.getElementById(id);
      if (target) {
        const y = target.getBoundingClientRect().top + window.scrollY - 100;
        window.scrollTo({ top: y, behavior: 'smooth' });
      }
    });

    li.appendChild(link);
    tocList.appendChild(li);
    links.push({id, link, el: h, dot});
  });

  function onScroll() {
    let scrollPos = document.documentElement.scrollTop || document.body.scrollTop;
    let current;
    links.forEach(item => {
      if (item.el.offsetTop - 120 <= scrollPos) {
        current = item;
      }
    });
    links.forEach(item => {
      item.link.classList.remove('font-semibold', 'text-black');
      item.link.classList.add('text-gray-400');
      item.dot.className = 'w-2 h-2 rounded-full border border-gray-300';
    });
    if (current) {
      current.link.classList.remove('text-gray-400');
      current.link.classList.add('font-semibold', 'text-black');
      current.dot.className = 'w-2 h-2 rounded-full bg-black';
    }
  }
  window.addEventListener('scroll', onScroll);
  onScroll();
});
</script>
<script>
window.addEventListener('scroll', function () {
  const docHeight = document.documentElement.scrollHeight - window.innerHeight;
  const scrollTop = window.scrollY || document.documentElement.scrollTop;
  const progress = (scrollTop / docHeight) * 100;
  document.getElementById('progress-bar').style.width = progress + '%';
});
</script></body>
</html>