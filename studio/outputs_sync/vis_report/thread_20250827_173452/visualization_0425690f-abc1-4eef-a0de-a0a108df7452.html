<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <style>
    #vis.vega-embed {
      width: 100%;
      display: flex;
    }

    #vis.vega-embed details,
    #vis.vega-embed details summary {
      position: relative;
    }
  </style>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega@5"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega-lite@5.20.1"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
</head>
<body>
  <div id="vis"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300, "stroke": null}}, "hconcat": [{"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "TitleShort", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 320}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Year", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 50}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "ImpactTier", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 70}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "EvalType", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 120}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Notable", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 50}, {"mark": {"type": "text", "align": "left", "baseline": "middle"}, "encoding": {"text": {"field": "Contrib", "type": "nominal"}, "y": {"axis": null, "field": "rank", "type": "ordinal"}}, "height": 360, "width": 400}], "data": {"name": "data-9ae1b835bca3256898a6320a0e06271e"}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-9ae1b835bca3256898a6320a0e06271e": [{"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 5028.0, "rank": 1, "TitleShort": "A Design Space of Visualization Tasks", "Contrib": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to p", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 4599.0, "rank": 2, "TitleShort": "Voyager: Exploratory Analysis via Faceted Browsing of Visual", "Contrib": "General visualization tools typically require manual specification of views: analysts must select data variables and the", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3804.0, "rank": 3, "TitleShort": "Calliope: Automatic Visual Data Story Generation from a Spre", "Contrib": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used ", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3521.0, "rank": 4, "TitleShort": "KG4Vis: A Knowledge Graph-Based Approach for Visualization R", "Contrib": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general user", "ImpactTier": "High", "EvalType": "case study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3415.0, "rank": 5, "TitleShort": "Formalizing Visualization Design Knowledge as Constraints: A", "Contrib": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 3063.0, "rank": 6, "TitleShort": "Augmenting Visualizations with Interactive Data Facts to Fac", "Contrib": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capa", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2962.0, "rank": 7, "TitleShort": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Net", "Contrib": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acqui", "ImpactTier": "High", "EvalType": "case study", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2838.0, "rank": 8, "TitleShort": "Visualization by Demonstration: An Interaction Paradigm for ", "Contrib": "Although data visualization tools continue to improve, during the data exploration process many of them require users to", "ImpactTier": "High", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2732.0, "rank": 9, "TitleShort": "Text-to-Viz: Automatic Generation of Infographics from Propo", "Contrib": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memo", "ImpactTier": "High", "EvalType": "user study", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2656.0, "rank": 10, "TitleShort": "Accessible Visualization via Natural Language Descriptions: ", "Contrib": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights,", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2492.0, "rank": 11, "TitleShort": "GenoREC: A Recommendation System for Interactive Genomics Da", "Contrib": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large", "ImpactTier": "Medium", "EvalType": "none", "Notable": "Yes"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "composite": 2337.0, "rank": 12, "TitleShort": "Data-Driven Guides: Supporting Expressive Design for Informa", "Contrib": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visual", "ImpactTier": "High", "EvalType": "benchmark", "Notable": "Yes"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>
</body>
</html>