<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <style>
    #vis.vega-embed {
      width: 100%;
      display: flex;
    }

    #vis.vega-embed details,
    #vis.vega-embed details summary {
      position: relative;
    }
  </style>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega@5"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega-lite@5.20.1"></script>
  <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/vega-embed@6"></script>
</head>
<body>
  <div id="vis"></div>
  <script>
    (function(vegaEmbed) {
      var spec = {"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "layer": [{"data": {"name": "data-9ee8ca1ceb908e3c3b19548c6274874a"}, "mark": {"type": "circle", "opacity": 0.6}, "encoding": {"color": {"field": "Award_bonus", "legend": {"format": "d", "values": [0, 1]}, "scale": {"domain": [0, 1], "range": ["steelblue", "gold"]}, "title": "Award", "type": "nominal"}, "size": {"field": "Downloads_Xplore", "scale": {"range": [20, 200]}, "title": "Downloads (size)", "type": "quantitative"}, "tooltip": [{"field": "Year", "type": "quantitative"}, {"field": "Title", "type": "nominal"}, {"field": "composite", "format": ".3f", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "type": "quantitative"}, {"field": "Downloads_Xplore", "type": "quantitative"}, {"field": "Award", "type": "nominal"}], "x": {"field": "Year", "title": "Year", "type": "quantitative"}, "y": {"field": "composite", "title": "Composite importance score", "type": "quantitative"}}}, {"data": {"name": "data-b9820031a9596b0071123fb510e3c2da"}, "mark": {"type": "point", "color": "red", "filled": true, "shape": "diamond", "size": 180}, "encoding": {"tooltip": [{"field": "Year", "type": "quantitative"}, {"field": "Title", "type": "nominal"}, {"field": "composite", "format": ".3f", "type": "quantitative"}, {"field": "CitationCount_CrossRef", "type": "quantitative"}, {"field": "Downloads_Xplore", "type": "quantitative"}, {"field": "Award", "type": "nominal"}], "x": {"field": "Year", "type": "quantitative"}, "y": {"field": "composite", "type": "quantitative"}}}, {"data": {"name": "data-b9820031a9596b0071123fb510e3c2da"}, "mark": {"type": "text", "dx": 5, "dy": -10, "fontSize": 11, "fontWeight": "bold"}, "encoding": {"text": {"field": "label", "type": "nominal"}, "x": {"field": "Year", "type": "quantitative"}, "y": {"field": "composite", "type": "quantitative"}}}], "height": 320, "resolve": {"scale": {"size": "independent"}}, "title": "Annotated timeline: AutoVis milestone candidates (by composite importance)", "width": 800, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-9ee8ca1ceb908e3c3b19548c6274874a": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.013906185139061852, "composite": 0.012733499377334994, "window_start": 1995, "window_label": "1995-1997"}, {"Conference": "Vis", "Year": 1995, "Title": "Subverting structure: data-driven diagram generation", "DOI": "10.1109/visual.1995.480815", "Link": "http://dx.doi.org/10.1109/VISUAL.1995.480815", "FirstPage": 217.0, "LastPage": null, "PaperType": "C", "Abstract": "Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.", "AuthorNames-Deduped": "Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps", "AuthorNames": "G. Golovchinsky;T. Kamps;K. Reichenberger", "AuthorAffiliation": "Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany", "InternalReferences": "10.1109/infvis.1995.528681;10.1109/infvis.1995.528681", "AuthorKeywords": null, "AminerCitationCount": 21.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 66.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.0, "composite": 0.003424657534246575, "window_start": 1995, "window_label": "1995-1997"}, {"Conference": "Vis", "Year": 2004, "Title": "Non-linear model fitting to parameterize diseased blood vessels", "DOI": "10.1109/visual.2004.72", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.72", "FirstPage": 393.0, "LastPage": 400.0, "PaperType": "C", "Abstract": "Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.", "AuthorNames-Deduped": "Alexandra La Cruz;Mat\u00fas Straka;Arnold K\u00f6chl;Milos Sr\u00e1mek;M. Eduard Gr\u00f6ller;Dominik Fleischmann", "AuthorNames": "A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann", "AuthorAffiliation": "University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA", "InternalReferences": "10.1109/visual.2001.964555", "AuthorKeywords": "Visualization, Segmentation, Blood Vessel Detection", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 11.0, "Downloads_Xplore": 141.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.015566625155666251, "composite": 0.013231631382316313, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "Vis", "Year": 2004, "Title": "Context-Adaptive Mobile Visualization and Information Management", "DOI": "10.1109/visual.2004.19", "Link": "http://dx.doi.org/10.1109/VISUAL.2004.19", "FirstPage": 8.0, "LastPage": 8.0, "PaperType": "M", "Abstract": "This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.", "AuthorNames-Deduped": "Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen", "AuthorNames": "J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen", "AuthorAffiliation": "Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 11.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 4.0, "Downloads_Xplore": 172.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.022000830220008302, "composite": 0.010024906600249066, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.08426733084267331, "composite": 0.06637608966376089, "window_start": 2004, "window_label": "2004-2006"}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1506849315068493, "dl_norm": 0.12453300124533001, "composite": 0.11270236612702365, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "InfoVis", "Year": 2008, "Title": "Multi-Focused Geospatial Analysis Using Probes", "DOI": "10.1109/tvcg.2008.149", "Link": "http://dx.doi.org/10.1109/TVCG.2008.149", "FirstPage": 1165.0, "LastPage": 1172.0, "PaperType": "J", "Abstract": "Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.", "AuthorNames-Deduped": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorNames": "Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang", "AuthorAffiliation": "UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center", "InternalReferences": "10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102", "AuthorKeywords": "Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes", "AminerCitationCount": 73.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 20.0, "Downloads_Xplore": 648.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11643835616438356, "dl_norm": 0.12079701120797011, "composite": 0.0944582814445828, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "VAST", "Year": 2009, "Title": "Articulate: a conversational interface for visual analytics", "DOI": "10.1109/vast.2009.5333099", "Link": "http://dx.doi.org/10.1109/VAST.2009.5333099", "FirstPage": 233.0, "LastPage": 234.0, "PaperType": "M", "Abstract": "While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.", "AuthorNames-Deduped": "Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau", "AuthorNames": "Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau", "AuthorAffiliation": "Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA", "InternalReferences": "0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148", "AuthorKeywords": null, "AminerCitationCount": 3.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 9.0, "Downloads_Xplore": 267.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.041718555417185554, "composite": 0.01594022415940224, "window_start": 2007, "window_label": "2007-2009"}, {"Conference": "VAST", "Year": 2010, "Title": "ALIDA: Using machine learning for intent discernment in visual analytics interfaces", "DOI": "10.1109/vast.2010.5650854", "Link": "http://dx.doi.org/10.1109/VAST.2010.5650854", "FirstPage": 223.0, "LastPage": 224.0, "PaperType": "M", "Abstract": "In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.", "AuthorNames-Deduped": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorNames": "Tera Marie Green;Ross Maciejewski;Steve DiPaola", "AuthorAffiliation": "School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada", "InternalReferences": null, "AuthorKeywords": "artificial intelligence, cognition, intent discernment, volume rendering", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 3.0, "PubsCited_CrossRef": 6.0, "Downloads_Xplore": 391.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.010273972602739725, "dl_norm": 0.06745537567455376, "composite": 0.02537359900373599, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14819427148194272, "composite": 0.059869240348692405, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "VAST", "Year": 2011, "Title": "Exploring agent-based simulations using temporal graphs", "DOI": "10.1109/vast.2011.6102469", "Link": "http://dx.doi.org/10.1109/VAST.2011.6102469", "FirstPage": 271.0, "LastPage": 272.0, "PaperType": "M", "Abstract": "Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.", "AuthorNames-Deduped": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorNames": "R. Jordan Crouser;Jeremy G. Freeman;Remco Chang", "AuthorAffiliation": "Tufts University, USA;Tufts University, USA;Tufts University, USA", "InternalReferences": "0.1109/infvis.2005.1532126", "AuthorKeywords": null, "AminerCitationCount": 0.0, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 8.0, "Downloads_Xplore": 163.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.02013283520132835, "composite": 0.006039850560398506, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "SciVis", "Year": 2012, "Title": "Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization", "DOI": "10.1109/tvcg.2012.203", "Link": "http://dx.doi.org/10.1109/TVCG.2012.203", "FirstPage": 2345.0, "LastPage": 2354.0, "PaperType": "J", "Abstract": "Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.", "AuthorNames-Deduped": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorNames": "Gunnar L\u00e4th\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga", "AuthorAffiliation": "Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\u00f6ping University, Sweden", "InternalReferences": "10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123", "AuthorKeywords": "Direct volume rendering, transfer functions, vessel visualization", "AminerCitationCount": 29.0, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 34.0, "Downloads_Xplore": 513.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04794520547945205, "dl_norm": 0.09277708592777086, "composite": 0.05180572851805729, "window_start": 2010, "window_label": "2010-2012"}, {"Conference": "InfoVis", "Year": 2013, "Title": "A Design Space of Visualization Tasks", "DOI": "10.1109/tvcg.2013.120", "Link": "http://dx.doi.org/10.1109/TVCG.2013.120", "FirstPage": 2366.0, "LastPage": 2375.0, "PaperType": "J", "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.", "AuthorNames-Deduped": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorNames": "Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann", "AuthorAffiliation": "University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany", "InternalReferences": "10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213", "AuthorKeywords": "Task taxonomy, design space, climate impact research, visualization recommendation", "AminerCitationCount": 217.0, "CitationCount_CrossRef": 144.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 4884.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.4931506849315068, "dl_norm": 1.0, "composite": 0.5465753424657533, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "Finding Waldo: Learning about Users from their Interactions", "DOI": "10.1109/tvcg.2014.2346575", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346575", "FirstPage": 1663.0, "LastPage": 1672.0, "PaperType": "J", "Abstract": "Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.", "AuthorNames-Deduped": "Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorNames": "Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang", "AuthorAffiliation": "Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U", "InternalReferences": "10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204", "AuthorKeywords": "User Interactions, Analytic Provenance, Visualization, Applied Machine Learning", "AminerCitationCount": 145.0, "CitationCount_CrossRef": 95.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 2226.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.3253424657534247, "dl_norm": 0.44831880448318806, "composite": 0.29716687422166876, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2014, "Title": "Learning Perceptual Kernels for Visualization Design", "DOI": "10.1109/tvcg.2014.2346978", "Link": "http://dx.doi.org/10.1109/TVCG.2014.2346978", "FirstPage": 1933.0, "LastPage": 1942.0, "PaperType": "J", "Abstract": "Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.", "AuthorNames-Deduped": "\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorNames": "\u00c7a\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer", "AuthorAffiliation": "Stanford University;Stanford University;University of Washington", "InternalReferences": "10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186", "AuthorKeywords": "Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding", "AminerCitationCount": 129.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 1247.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.273972602739726, "dl_norm": 0.24512245745122457, "composite": 0.21052303860523036, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs", "DOI": "10.1109/vast.2014.7042485", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042485", "FirstPage": 93.0, "LastPage": 102.0, "PaperType": "C", "Abstract": "Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.", "AuthorNames-Deduped": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorNames": "Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger", "AuthorAffiliation": "GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt", "InternalReferences": "10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225", "AuthorKeywords": "Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 331.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.055002075550020756, "composite": 0.030199252801992527, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2014, "Title": "An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging", "DOI": "10.1109/vast.2014.7042481", "Link": "http://dx.doi.org/10.1109/VAST.2014.7042481", "FirstPage": 53.0, "LastPage": 62.0, "PaperType": "C", "Abstract": "For cancers such as glioblastoma multiforme, there is an increasing interest in defining \"biological target volumes\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.", "AuthorNames-Deduped": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorNames": "Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\u00e9akh\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\u00fchler", "AuthorAffiliation": "VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria", "InternalReferences": "10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569", "AuthorKeywords": "MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 29.0, "Downloads_Xplore": 300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.048567870485678705, "composite": 0.02313200498132005, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 1.0, "dl_norm": 0.8802407638024077, "composite": 0.7640722291407223, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2015, "Title": "Mixed-initiative visual analytics using task-driven recommendations", "DOI": "10.1109/vast.2015.7347625", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347625", "FirstPage": 9.0, "LastPage": 16.0, "PaperType": "C", "Abstract": "Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.", "AuthorNames-Deduped": "Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorNames": "Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486", "AuthorKeywords": "mixed-initiative visual analytics, task modeling, recommender systems, sensemaking", "AminerCitationCount": 36.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 36.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.08561643835616438, "dl_norm": 0.15545869655458697, "composite": 0.08944582814445828, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "VAST", "Year": 2015, "Title": "Collaborative visual analysis with RCloud", "DOI": "10.1109/vast.2015.7347627", "Link": "http://dx.doi.org/10.1109/VAST.2015.7347627", "FirstPage": 25.0, "LastPage": 32.0, "PaperType": "C", "Abstract": "Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.", "AuthorNames-Deduped": "Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorNames": "Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull", "AuthorAffiliation": "Infovisible;University of Arizona;AT&T Labs;AT&T Labs", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185", "AuthorKeywords": "visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 404.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.0701535907015359, "composite": 0.03303237858032378, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "SciVis", "Year": 2015, "Title": "Automated visualization workflow for simulation experiments", "DOI": "10.1109/scivis.2015.7429509", "Link": "http://dx.doi.org/10.1109/SciVis.2015.7429509", "FirstPage": 153.0, "LastPage": 154.0, "PaperType": "M", "Abstract": "Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.", "AuthorNames-Deduped": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorNames": "Jonathan P. Leidig;Santhosh Dharmapuri", "AuthorAffiliation": "School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University", "InternalReferences": null, "AuthorKeywords": null, "AminerCitationCount": 1.0, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 12.0, "Downloads_Xplore": 137.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.01473640514736405, "composite": 0.006133250311332503, "window_start": 2013, "window_label": "2013-2015"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Data-Driven Guides: Supporting Expressive Design for Information Graphics", "DOI": "10.1109/tvcg.2016.2598620", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598620", "FirstPage": 491.0, "LastPage": 500.0, "PaperType": "J", "Abstract": "In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.", "AuthorNames-Deduped": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorNames": "Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister", "AuthorAffiliation": "John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Information graphics;visualization;design tools;2D graphics", "AminerCitationCount": 114.0, "CitationCount_CrossRef": 92.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2245.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.3150684931506849, "dl_norm": 0.4522623495226235, "composite": 0.2932129514321295, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2016, "Title": "Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration", "DOI": "10.1109/tvcg.2016.2598839", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598839", "FirstPage": 331.0, "LastPage": 340.0, "PaperType": "J", "Abstract": "Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.", "AuthorNames-Deduped": "Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert", "AuthorNames": "Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292", "AuthorKeywords": "Visual Data Exploration;Visualization by Demonstration;Visualization Tools", "AminerCitationCount": 83.0, "CitationCount_CrossRef": 57.0, "PubsCited_CrossRef": 35.0, "Downloads_Xplore": 2781.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1952054794520548, "dl_norm": 0.5635118306351183, "composite": 0.2666562889165629, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods", "DOI": "10.1109/tvcg.2016.2598544", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598544", "FirstPage": 271.0, "LastPage": 280.0, "PaperType": "J", "Abstract": "Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.", "AuthorNames-Deduped": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne", "AuthorNames": "Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne", "AuthorAffiliation": "Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory", "InternalReferences": "10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591", "AuthorKeywords": "trust;transparency;familiarity;uncertainty;biological data analysis", "AminerCitationCount": 41.0, "CitationCount_CrossRef": 41.0, "PubsCited_CrossRef": 41.0, "Downloads_Xplore": 1844.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1404109589041096, "dl_norm": 0.36903279369032793, "composite": 0.1809153175591532, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations", "DOI": "10.1109/tvcg.2016.2598543", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598543", "FirstPage": 261.0, "LastPage": 270.0, "PaperType": "J", "Abstract": "User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.", "AuthorNames-Deduped": "Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorNames": "Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan", "AuthorAffiliation": "Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research", "InternalReferences": "10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878", "AuthorKeywords": "Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization", "AminerCitationCount": 39.0, "CitationCount_CrossRef": 33.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 2188.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11301369863013698, "dl_norm": 0.44043171440431717, "composite": 0.18863636363636363, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems", "DOI": "10.1109/tvcg.2016.2598460", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2598460", "FirstPage": 121.0, "LastPage": 130.0, "PaperType": "J", "Abstract": "Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.", "AuthorNames-Deduped": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook", "AuthorNames": "R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook", "AuthorAffiliation": "Smith College;Smith College;Smith College;Smith College", "InternalReferences": "10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467", "AuthorKeywords": "Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 16.0, "PubsCited_CrossRef": 87.0, "Downloads_Xplore": 978.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0547945205479452, "dl_norm": 0.18929016189290163, "composite": 0.08418430884184308, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2016, "Title": "VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment", "DOI": "10.1109/tvcg.2016.2599378", "Link": "http://dx.doi.org/10.1109/TVCG.2016.2599378", "FirstPage": 231.0, "LastPage": 240.0, "PaperType": "J", "Abstract": "Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.", "AuthorNames-Deduped": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorNames": "Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology", "InternalReferences": "10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1", "AuthorKeywords": "Centralized matching;matching visualization;interaction techniques;visual analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 32.0, "Downloads_Xplore": 557.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.10190950601909506, "composite": 0.04427148194271482, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2017, "Title": "Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics", "DOI": "10.1109/vast.2017.8585669", "Link": "http://dx.doi.org/10.1109/VAST.2017.8585669", "FirstPage": 104.0, "LastPage": 115.0, "PaperType": "C", "Abstract": "Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.", "AuthorNames-Deduped": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorNames": "Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert", "AuthorAffiliation": "Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech", "InternalReferences": "10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486", "AuthorKeywords": "cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General", "AminerCitationCount": 115.0, "CitationCount_CrossRef": 70.0, "PubsCited_CrossRef": 80.0, "Downloads_Xplore": 1801.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.23972602739726026, "dl_norm": 0.36010792860107926, "composite": 0.2278953922789539, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2017, "Title": "Podium: Ranking Data Using Mixed-Initiative Visual Analytics", "DOI": "10.1109/tvcg.2017.2745078", "Link": "http://dx.doi.org/10.1109/TVCG.2017.2745078", "FirstPage": 288.0, "LastPage": 297.0, "PaperType": "J", "Abstract": "People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.", "AuthorNames-Deduped": "Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorNames": "Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136", "AuthorKeywords": "Mixed-initiative visual analytics,multi-attribute ranking,user interaction", "AminerCitationCount": 0.0, "CitationCount_CrossRef": 52.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1535.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1780821917808219, "dl_norm": 0.304898298048983, "composite": 0.18051058530510583, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.6061643835616438, "dl_norm": 0.6583644665836447, "composite": 0.7005915317559153, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication", "DOI": "10.1109/tvcg.2018.2865145", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865145", "FirstPage": 672.0, "LastPage": 681.0, "PaperType": "J", "Abstract": "Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.", "AuthorNames-Deduped": "Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko", "AuthorNames": "Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko", "AuthorAffiliation": "Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication", "AminerCitationCount": 120.0, "CitationCount_CrossRef": 121.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 2942.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.4143835616438356, "dl_norm": 0.5969281859692819, "composite": 0.3862702366127023, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2018, "Title": "DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks", "DOI": "10.1109/tvcg.2018.2864504", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864504", "FirstPage": 288.0, "LastPage": 298.0, "PaperType": "J", "Abstract": "Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.", "AuthorNames-Deduped": "Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007", "AuthorNames": "Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang", "AuthorAffiliation": "The Ohio State University;Visa Research;The Ohio State University;Visa Research", "InternalReferences": "10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683", "AuthorKeywords": "Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics", "AminerCitationCount": 108.0, "CitationCount_CrossRef": 91.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 2871.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.3116438356164384, "dl_norm": 0.5821917808219178, "composite": 0.5304794520547946, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2018, "Title": "Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution", "DOI": "10.1109/tvcg.2018.2864769", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2864769", "FirstPage": 374.0, "LastPage": 384.0, "PaperType": "J", "Abstract": "To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.", "AuthorNames-Deduped": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001", "AuthorNames": "Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins", "AuthorAffiliation": "Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\u00c3\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493", "AuthorKeywords": "User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning", "AminerCitationCount": 47.0, "CitationCount_CrossRef": 40.0, "PubsCited_CrossRef": 69.0, "Downloads_Xplore": 1217.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.136986301369863, "dl_norm": 0.23889580738895808, "composite": 0.14016189290161893, "window_start": 2016, "window_label": "2016-2018"}, {"Conference": "VAST", "Year": 2019, "Title": "FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning", "DOI": "10.1109/vast47406.2019.8986948", "Link": "http://dx.doi.org/10.1109/VAST47406.2019.8986948", "FirstPage": 46.0, "LastPage": 56.0, "PaperType": "C", "Abstract": "The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.", "AuthorNames-Deduped": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorNames": "\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau", "AuthorAffiliation": "Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology", "InternalReferences": "10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718", "AuthorKeywords": "Machine learning fairness,visual analytics,intersectional bias,subgroup discovery", "AminerCitationCount": 107.0, "CitationCount_CrossRef": 106.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 2108.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.363013698630137, "dl_norm": 0.42382731423827313, "composite": 0.30865504358655044, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2019, "Title": "Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements", "DOI": "10.1109/tvcg.2019.2934785", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934785", "FirstPage": 906.0, "LastPage": 916.0, "PaperType": "J", "Abstract": "Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.", "AuthorNames-Deduped": "Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Visualization for the masses,infographic,automatic visualization,presentation,and dissemination", "AminerCitationCount": 79.0, "CitationCount_CrossRef": 71.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 2661.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.24315068493150685, "dl_norm": 0.5386052303860523, "composite": 0.28315691158156914, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2019, "Title": "Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections", "DOI": "10.1109/tvcg.2019.2934654", "Link": "http://dx.doi.org/10.1109/TVCG.2019.2934654", "FirstPage": 1001.0, "LastPage": 1011.0, "PaperType": "J", "Abstract": "We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.", "AuthorNames-Deduped": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen", "AuthorNames": "Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen", "AuthorAffiliation": "University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493", "AuthorKeywords": "Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping", "AminerCitationCount": 30.0, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 59.0, "Downloads_Xplore": 1300.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06164383561643835, "dl_norm": 0.25612287256122873, "composite": 0.10765877957658779, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "DOI": "10.1109/tvcg.2020.3030403", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030403", "FirstPage": 453.0, "LastPage": 463.0, "PaperType": "J", "Abstract": "Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.", "AuthorNames-Deduped": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001", "AuthorNames": "Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Information Visualization,Visual Storytelling,Data Story", "AminerCitationCount": 56.0, "CitationCount_CrossRef": 80.0, "PubsCited_CrossRef": 57.0, "Downloads_Xplore": 3724.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.273972602739726, "dl_norm": 0.7592361975923619, "composite": 0.3647571606475716, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning", "DOI": "10.1109/tvcg.2020.3030467", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030467", "FirstPage": 294.0, "LastPage": 303.0, "PaperType": "J", "Abstract": "Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.", "AuthorNames-Deduped": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorNames": "Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu", "AuthorAffiliation": "Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University", "InternalReferences": "10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487", "AuthorKeywords": "Storyline visualization,reinforcement learning,mixed-initiative design", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 36.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 1931.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1232876712328767, "dl_norm": 0.3870900788709008, "composite": 0.17777085927770858, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "InfoVis", "Year": 2020, "Title": "Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics", "DOI": "10.1109/tvcg.2020.3030448", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030448", "FirstPage": 443.0, "LastPage": 452.0, "PaperType": "J", "Abstract": "Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.", "AuthorNames-Deduped": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia", "InternalReferences": "10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810", "AuthorKeywords": "Infographics,automatic visualization", "AminerCitationCount": 20.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1004.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.10616438356164383, "dl_norm": 0.1946865919468659, "composite": 0.11148816936488168, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2020, "Title": "VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations", "DOI": "10.1109/tvcg.2020.3030387", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030387", "FirstPage": 495.0, "LastPage": 505.0, "PaperType": "J", "Abstract": "Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.", "AuthorNames-Deduped": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorNames": "Michael Oppermann;Robert Kincaid;Tamara Munzner", "AuthorAffiliation": "Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia", "InternalReferences": "10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757", "AuthorKeywords": "visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories", "AminerCitationCount": 26.0, "CitationCount_CrossRef": 28.0, "PubsCited_CrossRef": 81.0, "Downloads_Xplore": 1243.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0958904109589041, "dl_norm": 0.24429223744292236, "composite": 0.12123287671232875, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "VAST", "Year": 2020, "Title": "Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering", "DOI": "10.1109/tvcg.2020.3030347", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030347", "FirstPage": 1775.0, "LastPage": 1785.0, "PaperType": "J", "Abstract": "We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.", "AuthorNames-Deduped": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorNames": "Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia", "AuthorAffiliation": "Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\u00e9 Paris-Saclay, CNRS, Inria, LRI, France", "InternalReferences": "10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477", "AuthorKeywords": "Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 17.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 754.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05821917808219178, "dl_norm": 0.1427978414279784, "composite": 0.07194894146948941, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "SciVis", "Year": 2020, "Title": "Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization", "DOI": "10.1109/tvcg.2020.3030407", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030407", "FirstPage": 806.0, "LastPage": 816.0, "PaperType": "J", "Abstract": "This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorNames": "Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes", "AuthorAffiliation": "Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz", "InternalReferences": "10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259", "AuthorKeywords": "Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 10.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 530.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.03424657534246575, "dl_norm": 0.09630552096305521, "composite": 0.046014943960149435, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "SciVis", "Year": 2020, "Title": "IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection", "DOI": "10.1109/tvcg.2020.3030389", "Link": "http://dx.doi.org/10.1109/TVCG.2020.3030389", "FirstPage": 775.0, "LastPage": 784.0, "PaperType": "J", "Abstract": "Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.", "AuthorNames-Deduped": "Juraj P\u00e1lenik;Thomas Spengler;Helwig Hauser", "AuthorNames": "Juraj Palenik;Thomas Spengler;Helwig Hauser", "AuthorAffiliation": "University of Bergen;University of Bergen;University of Bergen", "InternalReferences": "10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190", "AuthorKeywords": "visual parameter space exploration,scientific modelling,atmospheric convection", "AminerCitationCount": 1.0, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 39.0, "Downloads_Xplore": 417.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.07285180572851806, "composite": 0.02528019925280199, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.2363013698630137, "dl_norm": 0.7027812370278124, "composite": 0.5289850560398506, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content", "DOI": "10.1109/tvcg.2021.3114770", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114770", "FirstPage": 1073.0, "LastPage": 1083.0, "PaperType": "J", "Abstract": "Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.", "AuthorNames-Deduped": "Alan Lundgard;Arvind Satyanarayan", "AuthorNames": "Alan Lundgard;Arvind Satyanarayan", "AuthorAffiliation": "MIT CSAIL, USA;MIT CSAIL, USA", "InternalReferences": "10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375", "AuthorKeywords": "Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability", "AminerCitationCount": 24.0, "CitationCount_CrossRef": 62.0, "PubsCited_CrossRef": 108.0, "Downloads_Xplore": 2594.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.21232876712328766, "dl_norm": 0.5246990452469904, "composite": 0.2635740971357409, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Augmenting Sports Videos with VisCommentator", "DOI": "10.1109/tvcg.2021.3114806", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114806", "FirstPage": 824.0, "LastPage": 834.0, "PaperType": "J", "Abstract": "Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.", "AuthorNames-Deduped": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu", "AuthorNames": "Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458", "AuthorKeywords": "Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling", "AminerCitationCount": 19.0, "CitationCount_CrossRef": 42.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2151.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.14383561643835616, "dl_norm": 0.4327521793275218, "composite": 0.4017434620174346, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Kori: Interactive Synthesis of Text and Charts in Data Documents", "DOI": "10.1109/tvcg.2021.3114802", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114802", "FirstPage": 184.0, "LastPage": 194.0, "PaperType": "J", "Abstract": "Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.", "AuthorNames-Deduped": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim", "AuthorNames": "Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim", "AuthorAffiliation": "University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA", "InternalReferences": "10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647", "AuthorKeywords": "Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents", "AminerCitationCount": 11.0, "CitationCount_CrossRef": 34.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 1308.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.11643835616438356, "dl_norm": 0.2577833125778331, "composite": 0.1355541718555417, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "VizLinter: A Linter and Fixer Framework for Data Visualization", "DOI": "10.1109/tvcg.2021.3114804", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114804", "FirstPage": 206.0, "LastPage": 216.0, "PaperType": "J", "Abstract": "Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001", "AuthorNames": "Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China", "InternalReferences": "10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166", "AuthorKeywords": "Visualization Linting,Automated Visualization Design,Visualization Optimization", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 32.0, "PubsCited_CrossRef": 64.0, "Downloads_Xplore": 1919.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1095890410958904, "dl_norm": 0.3845994188459942, "composite": 0.17017434620174346, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation", "DOI": "10.1109/tvcg.2021.3114826", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114826", "FirstPage": 162.0, "LastPage": 172.0, "PaperType": "J", "Abstract": "We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.", "AuthorNames-Deduped": "Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001", "AuthorNames": "Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang", "AuthorAffiliation": "Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance", "AminerCitationCount": 14.0, "CitationCount_CrossRef": 31.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1788.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.10616438356164383, "dl_norm": 0.3574097135740971, "composite": 0.16030510585305105, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "An Evaluation-Focused Framework for Visualization Recommendation Algorithms", "DOI": "10.1109/tvcg.2021.3114814", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114814", "FirstPage": 346.0, "LastPage": 356.0, "PaperType": "J", "Abstract": "Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.", "AuthorNames-Deduped": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorNames": "Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle", "AuthorAffiliation": "University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Visualization Tools,Visualization Recommendation Algorithms", "AminerCitationCount": 13.0, "CitationCount_CrossRef": 25.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 1106.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.08561643835616438, "dl_norm": 0.21585720215857201, "composite": 0.3075653798256538, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Towards Visual Explainable Active Learning for Zero-Shot Classification", "DOI": "10.1109/tvcg.2021.3114793", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114793", "FirstPage": 791.0, "LastPage": 801.0, "PaperType": "J", "Abstract": "Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.", "AuthorNames-Deduped": "Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang", "AuthorNames": "Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang", "AuthorAffiliation": "College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China", "InternalReferences": "10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818", "AuthorKeywords": "Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics", "AminerCitationCount": 7.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 76.0, "Downloads_Xplore": 1775.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.354711498547115, "composite": 0.1475093399750934, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy", "DOI": "10.1109/tvcg.2021.3114810", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114810", "FirstPage": 151.0, "LastPage": 161.0, "PaperType": "J", "Abstract": "Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA", "InternalReferences": "10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437", "AuthorKeywords": "Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 21.0, "PubsCited_CrossRef": 105.0, "Downloads_Xplore": 815.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.07191780821917808, "dl_norm": 0.15545869655458697, "composite": 0.08259651307596513, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs", "DOI": "10.1109/tvcg.2021.3114877", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114877", "FirstPage": 400.0, "LastPage": 410.0, "PaperType": "J", "Abstract": "Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.", "AuthorNames-Deduped": "Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185", "AuthorKeywords": "Glyph-based visualization,machine learning,automatic visualization", "AminerCitationCount": 10.0, "CitationCount_CrossRef": 19.0, "PubsCited_CrossRef": 73.0, "Downloads_Xplore": 1101.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06506849315068493, "dl_norm": 0.21481942714819427, "composite": 0.09698007471980075, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks", "DOI": "10.1109/tvcg.2021.3114858", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114858", "FirstPage": 813.0, "LastPage": 823.0, "PaperType": "J", "Abstract": "Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \u201cdog faces\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \u201cdog face\u201d and \u201cdog tail\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.", "AuthorNames-Deduped": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau", "AuthorNames": "Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau", "AuthorAffiliation": "Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States", "InternalReferences": "10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509", "AuthorKeywords": "Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding", "AminerCitationCount": 8.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 830.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05136986301369863, "dl_norm": 0.1585720215857202, "composite": 0.07325653798256537, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics", "DOI": "10.1109/tvcg.2021.3114820", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114820", "FirstPage": 486.0, "LastPage": 496.0, "PaperType": "J", "Abstract": "There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.", "AuthorNames-Deduped": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorNames": "Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall", "AuthorAffiliation": "Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States", "InternalReferences": "10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493", "AuthorKeywords": "transformers,word embeddings,literature review,web scraper,dataset,visual analytics", "AminerCitationCount": 9.0, "CitationCount_CrossRef": 15.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 1087.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.05136986301369863, "dl_norm": 0.21191365711913657, "composite": 0.08925902864259028, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "A Mixed-Initiative Approach to Reusing Infographic Charts", "DOI": "10.1109/tvcg.2021.3114856", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114856", "FirstPage": 173.0, "LastPage": 183.0, "PaperType": "J", "Abstract": "Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.", "AuthorNames-Deduped": "Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001", "AuthorNames": "Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang", "AuthorAffiliation": "Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732", "AuthorKeywords": "Infographics,Reusable templates,Graphic design,Automatic visualization", "AminerCitationCount": 4.0, "CitationCount_CrossRef": 13.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 1211.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04452054794520548, "dl_norm": 0.23765047737650477, "composite": 0.09355541718555416, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization", "DOI": "10.1109/tvcg.2021.3114782", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114782", "FirstPage": 129.0, "LastPage": 139.0, "PaperType": "J", "Abstract": "Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorNames": "Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136", "AuthorKeywords": "Task-oriented insight preservation,responsive visualization", "AminerCitationCount": 6.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 751.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14217517642175176, "composite": 0.058063511830635114, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Semantic Snapping for Guided Multi-View Visualization Design", "DOI": "10.1109/tvcg.2021.3114860", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114860", "FirstPage": 43.0, "LastPage": 53.0, "PaperType": "J", "Abstract": "Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \u201caligned\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.", "AuthorNames-Deduped": "Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner", "AuthorNames": "Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner", "AuthorAffiliation": "Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway", "InternalReferences": "10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338", "AuthorKeywords": "Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views", "AminerCitationCount": 5.0, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 50.0, "Downloads_Xplore": 896.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.17227065172270653, "composite": 0.06366749688667497, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2021, "Title": "Visualization Equilibrium", "DOI": "10.1109/tvcg.2021.3114842", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114842", "FirstPage": 465.0, "LastPage": 474.0, "PaperType": "J", "Abstract": "In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.", "AuthorNames-Deduped": "Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907", "AuthorKeywords": "Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 40.0, "Downloads_Xplore": 647.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.12058945620589456, "composite": 0.039601494396014944, "window_start": 2019, "window_label": "2019-2021"}, {"Conference": "Vis", "Year": 2022, "Title": "MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization", "DOI": "10.1109/tvcg.2022.3209447", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209447", "FirstPage": 331.0, "LastPage": 341.0, "PaperType": "J", "Abstract": "Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.", "AuthorNames-Deduped": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu", "AuthorNames": "Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China", "InternalReferences": "10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254", "AuthorKeywords": "Glyph-based visualization,metaphor,machine learning,automatic visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 18.0, "PubsCited_CrossRef": 68.0, "Downloads_Xplore": 1095.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.06164383561643835, "dl_norm": 0.21357409713574096, "composite": 0.09489414694894147, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning", "DOI": "10.1109/tvcg.2022.3209468", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209468", "FirstPage": 690.0, "LastPage": 700.0, "PaperType": "J", "Abstract": "Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.", "AuthorNames-Deduped": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorNames": "Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu", "AuthorAffiliation": "State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234", "AuthorKeywords": "Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 14.0, "PubsCited_CrossRef": 83.0, "Downloads_Xplore": 1671.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.04794520547945205, "dl_norm": 0.3331257783312578, "composite": 0.12391033623910336, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning", "DOI": "10.1109/tvcg.2022.3209461", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209461", "FirstPage": 95.0, "LastPage": 105.0, "PaperType": "J", "Abstract": "Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.", "AuthorNames-Deduped": "Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001", "AuthorNames": "Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen", "AuthorAffiliation": "School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543", "AuthorKeywords": "Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 1276.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0273972602739726, "dl_norm": 0.2511415525114155, "composite": 0.08904109589041095, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0273972602739726, "dl_norm": 0.30531340805313406, "composite": 0.30529265255292654, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2022, "Title": "GenoREC: A Recommendation System for Interactive Genomics Data Visualization", "DOI": "10.1109/tvcg.2022.3209407", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209407", "FirstPage": 570.0, "LastPage": 580.0, "PaperType": "J", "Abstract": "Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.", "AuthorNames-Deduped": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorNames": "Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg", "AuthorAffiliation": "Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US", "InternalReferences": "10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234", "AuthorKeywords": "genomics,visualization,recommendation systems,data,tasks", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 62.0, "Downloads_Xplore": 2485.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.5020755500207555, "composite": 0.16260896637608965, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback", "DOI": "10.1109/tvcg.2023.3327363", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327363", "FirstPage": 131.0, "LastPage": 141.0, "PaperType": "J", "Abstract": "Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.", "AuthorNames-Deduped": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorNames": "Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh", "AuthorAffiliation": "New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774", "AuthorKeywords": "Narrative visualization,visual storytelling,conversational agent", "AminerCitationCount": null, "CitationCount_CrossRef": 7.0, "PubsCited_CrossRef": 79.0, "Downloads_Xplore": 816.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.023972602739726026, "dl_norm": 0.15566625155666253, "composite": 0.05868617683686177, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks", "DOI": "10.1109/tvcg.2023.3327170", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327170", "FirstPage": 944.0, "LastPage": 954.0, "PaperType": "J", "Abstract": "Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.", "AuthorNames-Deduped": "Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorNames": "Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China", "InternalReferences": "0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 58.0, "Downloads_Xplore": 665.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.12432544624325446, "composite": 0.04585927770859278, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Mystique: Deconstructing SVG Charts for Layout Reuse", "DOI": "10.1109/tvcg.2023.3327354", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327354", "FirstPage": 447.0, "LastPage": 457.0, "PaperType": "J", "Abstract": "To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.", "AuthorNames-Deduped": "Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001", "AuthorNames": "Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu", "AuthorAffiliation": "University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States", "InternalReferences": "0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877", "AuthorKeywords": "Chart layout,Reuse,Reverse-engineering,Deconstruction", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 47.0, "Downloads_Xplore": 481.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.08613532586135327, "composite": 0.029265255292652555, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning", "DOI": "10.1109/tvcg.2023.3327200", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3327200", "FirstPage": 1172.0, "LastPage": 1182.0, "PaperType": "J", "Abstract": "The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.", "AuthorNames-Deduped": "Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001", "AuthorNames": "Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865", "AuthorKeywords": "Time Series Data,Exploratory Visual Analysis,Reinforcement Learning", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 77.0, "Downloads_Xplore": 1050.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.20423412204234123, "composite": 0.06469489414694894, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining", "DOI": "10.1109/tvcg.2023.3326939", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326939", "FirstPage": 1227.0, "LastPage": 1237.0, "PaperType": "J", "Abstract": "Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.", "AuthorNames-Deduped": "Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai", "AuthorNames": "Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai", "AuthorAffiliation": "University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA", "InternalReferences": "0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475", "AuthorKeywords": "Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 82.0, "Downloads_Xplore": 361.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.06122872561228725, "composite": 0.02179327521793275, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco", "DOI": "10.1109/tvcg.2023.3326527", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326527", "FirstPage": 1063.0, "LastPage": 1073.0, "PaperType": "J", "Abstract": "Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\u2014a framework to model visualization knowledge\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.", "AuthorNames-Deduped": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorNames": "Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle", "AuthorAffiliation": "University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA", "InternalReferences": "0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814", "AuthorKeywords": "Graphical Perception Studies,Visualization Recommendation Algorithms", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 51.0, "Downloads_Xplore": 371.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.06330427563304275, "composite": 0.022415940224159398, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "The Rational Agent Benchmark for Data Visualization", "DOI": "10.1109/tvcg.2023.3326513", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326513", "FirstPage": 338.0, "LastPage": 347.0, "PaperType": "J", "Abstract": "Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.", "AuthorNames-Deduped": "Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman", "AuthorNames": "Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman", "AuthorAffiliation": "Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA", "InternalReferences": "0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781", "AuthorKeywords": "Evaluation,decision-making,rational agent,scoring rule", "AminerCitationCount": null, "CitationCount_CrossRef": 2.0, "PubsCited_CrossRef": 33.0, "Downloads_Xplore": 434.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.00684931506849315, "dl_norm": 0.07638024076380241, "composite": 0.0263387297633873, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams", "DOI": "10.1109/tvcg.2023.3326925", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326925", "FirstPage": 562.0, "LastPage": 572.0, "PaperType": "J", "Abstract": "Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.", "AuthorNames-Deduped": "Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001", "AuthorNames": "Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao", "AuthorAffiliation": "Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China", "InternalReferences": "0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428", "AuthorKeywords": "Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 78.0, "Downloads_Xplore": 662.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.12370278123702781, "composite": 0.03882316313823163, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization", "DOI": "10.1109/tvcg.2023.3326583", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326583", "FirstPage": 934.0, "LastPage": 943.0, "PaperType": "J", "Abstract": "Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.", "AuthorNames-Deduped": "Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell", "AuthorNames": "Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell", "AuthorAffiliation": "Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA", "InternalReferences": "0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423", "AuthorKeywords": "Visualization,responsive visualization,mixed-initiative authoring", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 330.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.0547945205479452, "composite": 0.018150684931506848, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Visual Analytics for Understanding Draco's Knowledge Base", "DOI": "10.1109/tvcg.2023.3326912", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326912", "FirstPage": 392.0, "LastPage": 402.0, "PaperType": "J", "Abstract": "Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.", "AuthorNames-Deduped": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorNames": "Johanna Schmidt;Bernhard Pointner;Silvia Miksch", "AuthorAffiliation": "VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Visual Analytics,Hypergraph visualization,Rule-based recommendation systems", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 53.0, "Downloads_Xplore": 365.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.06205894562058946, "composite": 0.018617683686176837, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Data Formulator: AI-Powered Concept-Driven Visualization Authoring", "DOI": "10.1109/tvcg.2023.3326585", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326585", "FirstPage": 1128.0, "LastPage": 1138.0, "PaperType": "J", "Abstract": "With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.", "AuthorNames-Deduped": "Chenglong Wang;John Thompson 0002;Bongshin Lee", "AuthorNames": "Chenglong Wang;John Thompson;Bongshin Lee", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA", "InternalReferences": "0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369", "AuthorKeywords": "AI,visualization authoring,data transformation,programming by example,natural language,large language model", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 63.0, "Downloads_Xplore": 893.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0, "dl_norm": 0.17164798671647988, "composite": 0.25149439601494394, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2023, "Title": "Guided Visual Analytics for Image Selection in Time and Space", "DOI": "10.1109/tvcg.2023.3326572", "Link": "http://dx.doi.org/10.1109/TVCG.2023.3326572", "FirstPage": 66.0, "LastPage": 75.0, "PaperType": "J", "Abstract": "Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.", "AuthorNames-Deduped": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorNames": "Ignacio P\u00e9rez-Messina;Davide Ceneda;Silvia Miksch", "AuthorAffiliation": "TU Wien, Austria;TU Wien, Austria;TU Wien, Austria", "InternalReferences": "0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146", "AuthorKeywords": "Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 37.0, "Downloads_Xplore": 1208.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.23702781237027812, "composite": 0.07110834371108343, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts", "DOI": "10.1109/tvcg.2024.3456398", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456398", "FirstPage": 481.0, "LastPage": 491.0, "PaperType": "J", "Abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.", "AuthorNames-Deduped": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorNames": "Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma", "AuthorAffiliation": "University of California, USA;University of California, USA;University of California, USA;University of California, USA", "InternalReferences": "10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878", "AuthorKeywords": "Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 65.0, "Downloads_Xplore": 386.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.003424657534246575, "dl_norm": 0.06641760066417601, "composite": 0.02163760896637609, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration", "DOI": "10.1109/tvcg.2024.3456364", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456364", "FirstPage": 547.0, "LastPage": 557.0, "PaperType": "J", "Abstract": "The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.", "AuthorNames-Deduped": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorNames": "Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang", "AuthorAffiliation": "Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA", "InternalReferences": "10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798", "AuthorKeywords": "Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 632.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.003424657534246575, "dl_norm": 0.11747613117476131, "composite": 0.2369551681195517, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "VisEval: A Benchmark for Data Visualization in the Era of Large Language Models", "DOI": "10.1109/tvcg.2024.3456320", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456320", "FirstPage": 1301.0, "LastPage": 1311.0, "PaperType": "J", "Abstract": "Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.", "AuthorNames-Deduped": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001", "AuthorNames": "Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang", "AuthorAffiliation": "Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668", "AuthorKeywords": "Visualization evaluation,automatic visualization,,,large language models,benchmark", "AminerCitationCount": null, "CitationCount_CrossRef": 1.0, "PubsCited_CrossRef": 75.0, "Downloads_Xplore": 625.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.003424657534246575, "dl_norm": 0.11602324616023246, "composite": 0.23651930261519305, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech", "DOI": "10.1109/tvcg.2024.3456358", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456358", "FirstPage": 864.0, "LastPage": 874.0, "PaperType": "J", "Abstract": "Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.", "AuthorNames-Deduped": "Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorNames": "Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott", "AuthorAffiliation": "Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia", "InternalReferences": "10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275", "AuthorKeywords": "Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 184.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0, "dl_norm": 0.024491490244914902, "composite": 0.2073474470734745, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "DracoGPT: Extracting Visualization Design Preferences from Large Language Models", "DOI": "10.1109/tvcg.2024.3456350", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456350", "FirstPage": 710.0, "LastPage": 720.0, "PaperType": "J", "Abstract": "Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\u2014DracoGPT-Rank and DracoGPT-Recommend\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.", "AuthorNames-Deduped": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorNames": "Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer", "AuthorAffiliation": "University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA", "InternalReferences": "10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527", "AuthorKeywords": "Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 45.0, "Downloads_Xplore": 378.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.0647571606475716, "composite": 0.01942714819427148, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Smartboard: Visual Exploration of Team Tactics with LLM Agent", "DOI": "10.1109/tvcg.2024.3456200", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456200", "FirstPage": 23.0, "LastPage": 33.0, "PaperType": "J", "Abstract": "Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.", "AuthorNames-Deduped": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu", "AuthorNames": "Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu", "AuthorAffiliation": "Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China", "InternalReferences": "10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806", "AuthorKeywords": "Sports visualization,tactic board,,,tactical analysis", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 74.0, "Downloads_Xplore": 813.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.15504358655043587, "composite": 0.04651307596513076, "window_start": 2022, "window_label": "2022-2024"}, {"Conference": "Vis", "Year": 2024, "Title": "Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations", "DOI": "10.1109/tvcg.2024.3456182", "Link": "http://dx.doi.org/10.1109/TVCG.2024.3456182", "FirstPage": 754.0, "LastPage": 764.0, "PaperType": "J", "Abstract": "People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6", "AuthorNames-Deduped": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorNames": "Ratanond Koonchanok;Michael E. Papka;Khairi Reda", "AuthorAffiliation": "Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA", "InternalReferences": "10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513", "AuthorKeywords": "Visual inference,statistical rationality,,,human-machine collaboration", "AminerCitationCount": null, "CitationCount_CrossRef": 0.0, "PubsCited_CrossRef": 71.0, "Downloads_Xplore": 138.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0, "dl_norm": 0.014943960149439602, "composite": 0.00448318804483188, "window_start": 2022, "window_label": "2022-2024"}], "data-b9820031a9596b0071123fb510e3c2da": [{"Conference": "InfoVis", "Year": 1995, "Title": "Towards a generative theory of diagram design", "DOI": "10.1109/infvis.1995.528681", "Link": "http://dx.doi.org/10.1109/INFVIS.1995.528681", "FirstPage": 11.0, "LastPage": 18.0, "PaperType": "C", "Abstract": "We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.", "AuthorNames-Deduped": "Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky", "AuthorNames": "K. Reichenberger;T. Kamps;G. Golovchinsky", "AuthorAffiliation": "Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada", "InternalReferences": "10.1109/visual.1995.480815;10.1109/visual.1995.480815", "AuthorKeywords": null, "AminerCitationCount": 22.0, "CitationCount_CrossRef": 5.0, "PubsCited_CrossRef": 18.0, "Downloads_Xplore": 133.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.017123287671232876, "dl_norm": 0.013906185139061852, "composite": 0.012733499377334994, "window_start": 1995, "window_label": "1995-1997", "label": "Towards a generative theory of diagram design"}, {"Conference": "VAST", "Year": 2006, "Title": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information", "DOI": "10.1109/vast.2006.261415", "Link": "http://dx.doi.org/10.1109/VAST.2006.261415", "FirstPage": 137.0, "LastPage": 144.0, "PaperType": "C", "Abstract": "We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication", "AuthorNames-Deduped": "Paul E. Keel", "AuthorNames": "Paul E. Keel", "AuthorAffiliation": "Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK", "InternalReferences": null, "AuthorKeywords": "Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 24.0, "PubsCited_CrossRef": 23.0, "Downloads_Xplore": 472.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.0821917808219178, "dl_norm": 0.08426733084267331, "composite": 0.06637608966376089, "window_start": 2004, "window_label": "2004-2006", "label": "Collaborative Visual Analytics: Inferring from the Spatial Organization and Coll"}, {"Conference": "Vis", "Year": 2007, "Title": "Interactive Visual Analysis of Perfusion Data", "DOI": "10.1109/tvcg.2007.70569", "Link": "http://dx.doi.org/10.1109/TVCG.2007.70569", "FirstPage": 1392.0, "LastPage": 1399.0, "PaperType": "J", "Abstract": "Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.", "AuthorNames-Deduped": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorNames": "Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim", "AuthorAffiliation": "Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany", "InternalReferences": "10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739", "AuthorKeywords": "Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis", "AminerCitationCount": 100.0, "CitationCount_CrossRef": 44.0, "PubsCited_CrossRef": 28.0, "Downloads_Xplore": 666.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.1506849315068493, "dl_norm": 0.12453300124533001, "composite": 0.11270236612702365, "window_start": 2007, "window_label": "2007-2009", "label": "Interactive Visual Analysis of Perfusion Data"}, {"Conference": "Vis", "Year": 2011, "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics", "DOI": "10.1109/tvcg.2011.217", "Link": "http://dx.doi.org/10.1109/TVCG.2011.217", "FirstPage": 1882.0, "LastPage": 1891.0, "PaperType": "J", "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.", "AuthorNames-Deduped": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorNames": "Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert", "AuthorAffiliation": "University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany", "InternalReferences": "10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911", "AuthorKeywords": "Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization", "AminerCitationCount": 22.0, "CitationCount_CrossRef": 9.0, "PubsCited_CrossRef": 38.0, "Downloads_Xplore": 780.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 0.030821917808219176, "dl_norm": 0.14819427148194272, "composite": 0.059869240348692405, "window_start": 2010, "window_label": "2010-2012", "label": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fl"}, {"Conference": "InfoVis", "Year": 2015, "Title": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations", "DOI": "10.1109/tvcg.2015.2467191", "Link": "http://dx.doi.org/10.1109/TVCG.2015.2467191", "FirstPage": 649.0, "LastPage": 658.0, "PaperType": "J", "Abstract": "General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.", "AuthorNames-Deduped": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer", "AuthorNames": "Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington", "InternalReferences": "10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297", "AuthorKeywords": "User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems", "AminerCitationCount": 487.0, "CitationCount_CrossRef": 292.0, "PubsCited_CrossRef": 48.0, "Downloads_Xplore": 4307.0, "Award": null, "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 0, "cit_norm": 1.0, "dl_norm": 0.8802407638024077, "composite": 0.7640722291407223, "window_start": 2013, "window_label": "2013-2015", "label": "Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendati"}, {"Conference": "InfoVis", "Year": 2018, "Title": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco", "DOI": "10.1109/tvcg.2018.2865240", "Link": "http://dx.doi.org/10.1109/TVCG.2018.2865240", "FirstPage": 438.0, "LastPage": 448.0, "PaperType": "J", "Abstract": "There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.", "AuthorNames-Deduped": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer", "AuthorNames": "Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer", "AuthorAffiliation": "University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191", "AuthorKeywords": "Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming", "AminerCitationCount": 225.0, "CitationCount_CrossRef": 177.0, "PubsCited_CrossRef": 67.0, "Downloads_Xplore": 3238.0, "Award": "BP", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.6061643835616438, "dl_norm": 0.6583644665836447, "composite": 0.7005915317559153, "window_start": 2016, "window_label": "2016-2018", "label": "Formalizing Visualization Design Knowledge as Constraints: Actionable and Extens"}, {"Conference": "Vis", "Year": 2021, "Title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation", "DOI": "10.1109/tvcg.2021.3114863", "Link": "http://dx.doi.org/10.1109/TVCG.2021.3114863", "FirstPage": 195.0, "LastPage": 205.0, "PaperType": "J", "Abstract": "Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.", "AuthorNames-Deduped": "Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorNames": "Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu", "AuthorAffiliation": "Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong", "InternalReferences": "10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185", "AuthorKeywords": "Data visualization,Visualization recommendation,Knowledge graph", "AminerCitationCount": 17.0, "CitationCount_CrossRef": 69.0, "PubsCited_CrossRef": 60.0, "Downloads_Xplore": 3452.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.2363013698630137, "dl_norm": 0.7027812370278124, "composite": 0.5289850560398506, "window_start": 2019, "window_label": "2019-2021", "label": "KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation"}, {"Conference": "Vis", "Year": 2022, "Title": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition", "DOI": "10.1109/tvcg.2022.3209421", "Link": "http://dx.doi.org/10.1109/TVCG.2022.3209421", "FirstPage": 1135.0, "LastPage": 1145.0, "PaperType": "J", "Abstract": "Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.", "AuthorNames-Deduped": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorNames": "Aditeya Pandey;Arjun Srinivasan;Vidya Setlur", "AuthorAffiliation": "Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany", "InternalReferences": "10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826", "AuthorKeywords": "Dashboards,intent,recommendations,direct manipulation,multi-view coordination", "AminerCitationCount": null, "CitationCount_CrossRef": 8.0, "PubsCited_CrossRef": 55.0, "Downloads_Xplore": 1537.0, "Award": "HM", "GraphicsReplicabilityStamp": null, "AutoVis_Flag": 1, "Award_bonus": 1, "cit_norm": 0.0273972602739726, "dl_norm": 0.30531340805313406, "composite": 0.30529265255292654, "window_start": 2022, "window_label": "2022-2024", "label": "MEDLEY: Intent-based Recommendations to Support Dashboard Composition"}]}};
      var embedOpt = {"mode": "vega-lite"};

      function showError(el, error){
          el.innerHTML = ('<div style="color:red;">'
                          + '<p>JavaScript Error: ' + error.message + '</p>'
                          + "<p>This usually means there's a typo in your chart specification. "
                          + "See the javascript console for the full traceback.</p>"
                          + '</div>');
          throw error;
      }
      const el = document.getElementById('vis');
      vegaEmbed("#vis", spec, embedOpt)
        .catch(error => showError(el, error));
    })(vegaEmbed);

  </script>
</body>
</html>