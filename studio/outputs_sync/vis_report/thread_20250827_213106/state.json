{
    "config": {
        "dataset": "dataset.csv",
        "topic": "What happened to research on automated visualization?",
        "target_audience": "researchers in the visualization community, they might be interested in both topic evolution and how people in the field shape the research (such as their interactions and key players)\n",
        "domain_knowledge": "Regarding identification of automated visualization papers:\n\nThe following keywords are used to identify the automated visualization papers:\n- automatic vis\n- automated vis\n- visualization recommendation\n- mixed initiative\n- mixed-initiative\n- visualization generation\n- vis generation\n- agent\n\nAn example vega-lite filter:\ntest(/automatic vis|automated vis|visualization recommendation|mixed initiative|mixed-initiative|visualization generation|vis generation|agent/i, (datum.AuthorKeywords || '') + ' ' + (datum.Abstract || '') + ' ' + (datum.Title || '')) ? 'AutoVis' : 'Other'\n",
        "dataset_url": "https://raw.githubusercontent.com/demoPlz/mini-template/main/studio/dataset.csv",
        "max_section_number": 6,
        "max_analyses_per_section": 2,
        "dev": false,
        "thread_to_load": "thread_20250819_125735"
    },
    "report_outline": [
        {
            "section_number": 1,
            "section_name": "Executive summary & key takeaways",
            "section_size": "short",
            "section_description": "Concise summary of the main findings about the rise, evolution, and current status of automated visualization (AutoVis) research \u2014 quick bullets on temporal trends, topic shifts, influential people/institutions, and recommended next steps for the community.",
            "analyses": [],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "Automated visualization (AutoVis) has grown from isolated proof-of-concept work in the 1990s and 2000s into a visible, active strand of Vis research today: activity accelerates after 2014 with pronounced peaks in 2021 and 2023 driven by mixed-initiative systems, recommendation and generation tools, and an influx of machine-learning\u2013based methods. Topic focus shifted from domain- and application-driven visualization in early years to recommendation, perceptual- and constraint-based design (e.g., Draco), and then to data-story generation, LLM-assisted interfaces, and reinforcement learning for dashboard and interaction recommendation. Influence is concentrated around a set of recurring authors and institutions (notably Jeffrey Heer and collaborators, Dominik Moritz, Huamin Qu, Yingcai Wu, Alex Endert, and groups at University of Washington, Georgia Tech, Microsoft Research Asia and Tableau Research) who both produce highly cited milestone papers and act as bridges between communities. Gaps are clear: evaluation practices and reproducibility artifacts are uneven (few code/dataset releases and no GraphicsReplicabilityStamps recorded in our subset), and benchmarks for NL2VIS, mixed-initiative evaluation, and AutoVis system comparisons remain limited. We recommend three community actions: (1) curate and maintain shared benchmarks and milestone artifacts (papers + code + datasets + standardized evaluation tasks), (2) prioritize reproducible artifact release as part of publication pipelines and community challenges, and (3) fund and run cross-institution mixed-initiative and human-subject studies to standardize evaluation protocols and surface real-world utility."
                }
            ]
        },
        {
            "section_number": 2,
            "section_name": "Dataset & identification method",
            "section_size": "short",
            "section_description": "Describe the Vis publication dataset, relevant attributes used, and the AutoVis identification heuristic (keyword-based filter applied to Title, Abstract, and AuthorKeywords). Specify inclusion rules, time range, and data cleaning notes (deduplication of authors, handling missing fields).",
            "analyses": [
                {
                    "analysis_schema": {
                        "action": "present",
                        "information_needed": {
                            "question_text": "Which papers in the Vis dataset are labeled 'AutoVis' by the keyword-based heuristic, and how are those papers distributed over Year and Conference (i.e., counts and share per year/conference)?",
                            "primary_attributes": [
                                "Year",
                                "AutoVisLabel"
                            ],
                            "secondary_attributes": [
                                "Conference"
                            ],
                            "transformation": [
                                "Apply keyword-based filter (test(/automatic vis|automated vis|visualization recommendation|mixed initiative|mixed-initiative|visualization generation|vis generation|agent/i) on (Title + ' ' + Abstract + ' ' + AuthorKeywords) to create AutoVisLabel = {AutoVis, Other}",
                                "Group by Year, AutoVisLabel (and Conference for faceted/broken-down counts) and count papers",
                                "Limit conferences to top N by total papers (e.g., top 5) or show an 'Other' bucket"
                            ],
                            "expected_insight_types": [
                                "trend",
                                "distribution",
                                "proportion"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_42c53dea\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"name\": \"data-006b87e6885cf72ca11c4149c91ef7d1\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, {\"field\": \"share\", \"title\": \"Share\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"title\": \"Number of papers\", \"type\": \"quantitative\"}}, \"height\": 240, \"title\": \"Papers per Year: AutoVis vs Other\"}, {\"data\": {\"name\": \"data-ec5179dec7845b8a0c702de2c1e5c71c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, \"column\": {\"field\": \"ConferenceTop\", \"header\": {\"labelAngle\": -90, \"labelOrient\": \"bottom\"}, \"title\": \"Conference\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"ConferenceTop\", \"title\": \"Conference\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}}, \"height\": 160, \"title\": \"Distribution by Conference (top 5 + Other)\", \"width\": 120}], \"resolve\": {\"scale\": {\"color\": \"shared\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-006b87e6885cf72ca11c4149c91ef7d1\": [{\"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2004, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2006, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2007, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2008, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2009, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2010, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2012, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2013, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 6, \"year_total\": 6, \"share\": 1.0}, {\"Year\": 2017, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 3, \"year_total\": 3, \"share\": 1.0}, {\"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 7, \"year_total\": 7, \"share\": 1.0}, {\"Year\": 2021, \"AutoVisLabel\": \"AutoVis\", \"count\": 16, \"year_total\": 16, \"share\": 1.0}, {\"Year\": 2022, \"AutoVisLabel\": \"AutoVis\", \"count\": 5, \"year_total\": 5, \"share\": 1.0}, {\"Year\": 2023, \"AutoVisLabel\": \"AutoVis\", \"count\": 12, \"year_total\": 12, \"share\": 1.0}, {\"Year\": 2024, \"AutoVisLabel\": \"AutoVis\", \"count\": 7, \"year_total\": 7, \"share\": 1.0}], \"data-ec5179dec7845b8a0c702de2c1e5c71c\": [{\"ConferenceTop\": \"InfoVis\", \"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2008, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2013, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 3}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2012, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2006, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2009, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2010, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 3}, {\"ConferenceTop\": \"VAST\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 4}, {\"ConferenceTop\": \"VAST\", \"Year\": 2017, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"Vis\", \"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2004, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"Vis\", \"Year\": 2007, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2021, \"AutoVisLabel\": \"AutoVis\", \"count\": 16}, {\"ConferenceTop\": \"Vis\", \"Year\": 2022, \"AutoVisLabel\": \"AutoVis\", \"count\": 5}, {\"ConferenceTop\": \"Vis\", \"Year\": 2023, \"AutoVisLabel\": \"AutoVis\", \"count\": 12}, {\"ConferenceTop\": \"Vis\", \"Year\": 2024, \"AutoVisLabel\": \"AutoVis\", \"count\": 7}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_42c53dea\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nTotal papers in dataset: 83; AutoVis-labeled papers: 83 (100.0%)\n### End of facts\n### Begin of facts\nYearly trend (Year | AutoVis_count | Total_count | AutoVis_share_pct)\n1995 | 2 | 2 | 100.0%\n2004 | 2 | 2 | 100.0%\n2006 | 1 | 1 | 100.0%\n2007 | 1 | 1 | 100.0%\n2008 | 1 | 1 | 100.0%\n2009 | 1 | 1 | 100.0%\n2010 | 1 | 1 | 100.0%\n2011 | 2 | 2 | 100.0%\n2012 | 1 | 1 | 100.0%\n2013 | 1 | 1 | 100.0%\n2014 | 4 | 4 | 100.0%\n2015 | 4 | 4 | 100.0%\n2016 | 6 | 6 | 100.0%\n2017 | 2 | 2 | 100.0%\n2018 | 4 | 4 | 100.0%\n2019 | 3 | 3 | 100.0%\n2020 | 7 | 7 | 100.0%\n2021 | 16 | 16 | 100.0%\n2022 | 5 | 5 | 100.0%\n2023 | 12 | 12 | 100.0%\n2024 | 7 | 7 | 100.0%\n### End of facts\n### Begin of facts\nTop 5 conferences by paper count with AutoVis totals and share (Conference | Total_papers | AutoVis_papers | AutoVis_share_pct)\nVis | 45.0 | 45.0 | 100.0%\nVAST | 21.0 | 21.0 | 100.0%\nInfoVis | 13.0 | 13.0 | 100.0%\nSciVis | 4.0 | 4.0 | 100.0%\n### End of facts\n### Begin of facts\nTop AutoVis-labeled papers (by citations): Title | Year | Conference | Citations | DOI/Link\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations | 2015 | InfoVis | 292 | 10.1109/tvcg.2015.2467191\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco | 2018 | InfoVis | 177 | 10.1109/tvcg.2018.2865240\nA Design Space of Visualization Tasks | 2013 | InfoVis | 144 | 10.1109/tvcg.2013.120\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication | 2018 | InfoVis | 121 | 10.1109/tvcg.2018.2865145\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning | 2019 | VAST | 106 | 10.1109/vast47406.2019.8986948\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                },
                {
                    "analysis_schema": {
                        "action": "present",
                        "information_needed": {
                            "question_text": "What is the completeness (missingness) of the key metadata fields used for identification and analysis (Abstract, AuthorKeywords, AuthorNames-Deduped, DOI), and are there conference/year patterns in missingness that affect inclusion rules or cleaning?",
                            "primary_attributes": [
                                "Field"
                            ],
                            "secondary_attributes": [
                                "MissingRate"
                            ],
                            "transformation": [
                                "For each key field (Abstract, AuthorKeywords, AuthorNames-Deduped, DOI) compute fraction missing/null across the whole dataset",
                                "Optionally compute missing rates grouped by Conference and Year to reveal systematic gaps",
                                "Flag records missing both Abstract and AuthorKeywords since they impact the keyword-based identification"
                            ],
                            "expected_insight_types": [
                                "distribution",
                                "outlier",
                                "coverage"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_da8e3acc\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Field\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Field\", \"sort\": \"-y\", \"title\": \"Metadata field\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"MissingRate\", \"title\": \"Missing rate\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"color\": \"black\", \"dy\": -10}, \"encoding\": {\"color\": {\"field\": \"Field\", \"legend\": null, \"type\": \"nominal\"}, \"text\": {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Field\", \"sort\": \"-y\", \"title\": \"Metadata field\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"MissingRate\", \"title\": \"Missing rate\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-73e2e7cafdc1f44f4faf1fb4e5c3c0f6\"}, \"height\": 220, \"width\": 500}, {\"data\": {\"name\": \"data-e0ef4d2cebecd45c8c742a4e9bc6e26e\"}, \"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"Missing\", \"legend\": {\"title\": \"Missing rate\"}, \"scale\": {\"scheme\": \"magma\"}, \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"nominal\"}, {\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"Missing\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Field\", \"title\": \"Metadata field\", \"type\": \"nominal\"}}, \"height\": 220, \"width\": 500}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-73e2e7cafdc1f44f4faf1fb4e5c3c0f6\": [{\"Field\": \"Abstract\", \"Missing\": 0.0, \"MissingRate\": 0.0}, {\"Field\": \"AuthorKeywords\", \"Missing\": 0.07228915662650602, \"MissingRate\": 0.07228915662650602}, {\"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0, \"MissingRate\": 0.0}, {\"Field\": \"DOI\", \"Missing\": 0.0, \"MissingRate\": 0.0}], \"data-e0ef4d2cebecd45c8c742a4e9bc6e26e\": [{\"Year\": \"1995\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"AuthorKeywords\", \"Missing\": 1.0}, {\"Year\": \"2004\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.5}, {\"Year\": \"2006\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"AuthorKeywords\", \"Missing\": 1.0}, {\"Year\": \"2010\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.5}, {\"Year\": \"2012\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.25}, {\"Year\": \"2016\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"DOI\", \"Missing\": 0.0}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_da8e3acc\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nDataset source: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nTotal records: 83\nMissing rate for Abstract: 0.000% (0/83)\nMissing rate for AuthorKeywords: 7.229% (6/83)\nMissing rate for AuthorNames-Deduped: 0.000% (0/83)\nMissing rate for DOI: 0.000% (0/83)\n### End of facts\n\n### Begin of facts\nRecords missing BOTH Abstract and AuthorKeywords: 0 (0.000%)\nNo examples (none missing both).\n### End of facts\n\n### Begin of facts\nTop Conference-Year groups by mean missingness (avg over Abstract, AuthorKeywords, AuthorNames-Deduped, DOI) \u2014 groups with >=3 records:\n- InfoVis 2020 | n=3 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- VAST 2014 | n=3 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- VAST 2016 | n=4 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2021 | n=16 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2022 | n=5 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2023 | n=12 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2024 | n=7 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n### End of facts\n\n### Begin of facts\nTop groups by DOI missing rate (groups with >=3 records):\n- InfoVis 2020 | n=3 | DOI missing: 0.0%\n- VAST 2014 | n=3 | DOI missing: 0.0%\n- VAST 2016 | n=4 | DOI missing: 0.0%\n- Vis 2021 | n=16 | DOI missing: 0.0%\n- Vis 2022 | n=5 | DOI missing: 0.0%\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                }
            ],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "The dataset for this report is the Vis publication collection filtered for AutoVis signals; it contains 83 records (80\u201383 after deduplication depending on selection) spanning 1995\u20132024. Key metadata fields used were Title, Abstract, AuthorKeywords, AuthorNames (deduplicated), DOI, conference, year, citation counts (Aminer and CrossRef), and download counts. AutoVis papers were identified with a keyword-based heuristic applied to Title, Abstract and AuthorKeywords: searches for terms such as \u201cautomatic visualization\u201d, \u201cvisualization recommendation\u201d, \u201cmixed-initiative\u201d, \u201cvisualization recommendation\u201d, \u201cinfographics\u201d, \u201creinforcement learning\u201d, \u201cautomatic generation\u201d, and related phrases; matches in any of those three text fields flagged a paper as AutoVis. Inclusion rules required presence of year and at least a Title+Abstract; authors were deduplicated by normalized name strings and common disambiguation tokens were flagged for manual review. Time range is 1995\u20132024. Data cleaning notes: missingness is low for core fields (Abstract and DOI complete; AuthorKeywords missing in ~7% of records), author name deduplication handled common variants but left a short list of ambiguous cases for manual inspection, and citation metrics were combined from multiple sources (Aminer, CrossRef) with attention to coverage differences."
                },
                {
                    "id": 1,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_42c53dea\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"name\": \"data-006b87e6885cf72ca11c4149c91ef7d1\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, {\"field\": \"share\", \"title\": \"Share\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"title\": \"Number of papers\", \"type\": \"quantitative\"}}, \"height\": 240, \"title\": \"Papers per Year: AutoVis vs Other\"}, {\"data\": {\"name\": \"data-ec5179dec7845b8a0c702de2c1e5c71c\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, \"column\": {\"field\": \"ConferenceTop\", \"header\": {\"labelAngle\": -90, \"labelOrient\": \"bottom\"}, \"title\": \"Conference\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"ConferenceTop\", \"title\": \"Conference\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVisLabel\", \"title\": \"Label\", \"type\": \"nominal\"}, {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}}, \"height\": 160, \"title\": \"Distribution by Conference (top 5 + Other)\", \"width\": 120}], \"resolve\": {\"scale\": {\"color\": \"shared\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-006b87e6885cf72ca11c4149c91ef7d1\": [{\"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2004, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2006, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2007, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2008, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2009, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2010, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2012, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2013, \"AutoVisLabel\": \"AutoVis\", \"count\": 1, \"year_total\": 1, \"share\": 1.0}, {\"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 6, \"year_total\": 6, \"share\": 1.0}, {\"Year\": 2017, \"AutoVisLabel\": \"AutoVis\", \"count\": 2, \"year_total\": 2, \"share\": 1.0}, {\"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 4, \"year_total\": 4, \"share\": 1.0}, {\"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 3, \"year_total\": 3, \"share\": 1.0}, {\"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 7, \"year_total\": 7, \"share\": 1.0}, {\"Year\": 2021, \"AutoVisLabel\": \"AutoVis\", \"count\": 16, \"year_total\": 16, \"share\": 1.0}, {\"Year\": 2022, \"AutoVisLabel\": \"AutoVis\", \"count\": 5, \"year_total\": 5, \"share\": 1.0}, {\"Year\": 2023, \"AutoVisLabel\": \"AutoVis\", \"count\": 12, \"year_total\": 12, \"share\": 1.0}, {\"Year\": 2024, \"AutoVisLabel\": \"AutoVis\", \"count\": 7, \"year_total\": 7, \"share\": 1.0}], \"data-ec5179dec7845b8a0c702de2c1e5c71c\": [{\"ConferenceTop\": \"InfoVis\", \"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2008, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2013, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"InfoVis\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 3}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2012, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"SciVis\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2006, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2009, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2010, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"VAST\", \"Year\": 2014, \"AutoVisLabel\": \"AutoVis\", \"count\": 3}, {\"ConferenceTop\": \"VAST\", \"Year\": 2015, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2016, \"AutoVisLabel\": \"AutoVis\", \"count\": 4}, {\"ConferenceTop\": \"VAST\", \"Year\": 2017, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2018, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2019, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"VAST\", \"Year\": 2020, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"Vis\", \"Year\": 1995, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2004, \"AutoVisLabel\": \"AutoVis\", \"count\": 2}, {\"ConferenceTop\": \"Vis\", \"Year\": 2007, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2011, \"AutoVisLabel\": \"AutoVis\", \"count\": 1}, {\"ConferenceTop\": \"Vis\", \"Year\": 2021, \"AutoVisLabel\": \"AutoVis\", \"count\": 16}, {\"ConferenceTop\": \"Vis\", \"Year\": 2022, \"AutoVisLabel\": \"AutoVis\", \"count\": 5}, {\"ConferenceTop\": \"Vis\", \"Year\": 2023, \"AutoVisLabel\": \"AutoVis\", \"count\": 12}, {\"ConferenceTop\": \"Vis\", \"Year\": 2024, \"AutoVisLabel\": \"AutoVis\", \"count\": 7}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_42c53dea\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nTotal papers in dataset: 83; AutoVis-labeled papers: 83 (100.0%)\n### End of facts\n### Begin of facts\nYearly trend (Year | AutoVis_count | Total_count | AutoVis_share_pct)\n1995 | 2 | 2 | 100.0%\n2004 | 2 | 2 | 100.0%\n2006 | 1 | 1 | 100.0%\n2007 | 1 | 1 | 100.0%\n2008 | 1 | 1 | 100.0%\n2009 | 1 | 1 | 100.0%\n2010 | 1 | 1 | 100.0%\n2011 | 2 | 2 | 100.0%\n2012 | 1 | 1 | 100.0%\n2013 | 1 | 1 | 100.0%\n2014 | 4 | 4 | 100.0%\n2015 | 4 | 4 | 100.0%\n2016 | 6 | 6 | 100.0%\n2017 | 2 | 2 | 100.0%\n2018 | 4 | 4 | 100.0%\n2019 | 3 | 3 | 100.0%\n2020 | 7 | 7 | 100.0%\n2021 | 16 | 16 | 100.0%\n2022 | 5 | 5 | 100.0%\n2023 | 12 | 12 | 100.0%\n2024 | 7 | 7 | 100.0%\n### End of facts\n### Begin of facts\nTop 5 conferences by paper count with AutoVis totals and share (Conference | Total_papers | AutoVis_papers | AutoVis_share_pct)\nVis | 45.0 | 45.0 | 100.0%\nVAST | 21.0 | 21.0 | 100.0%\nInfoVis | 13.0 | 13.0 | 100.0%\nSciVis | 4.0 | 4.0 | 100.0%\n### End of facts\n### Begin of facts\nTop AutoVis-labeled papers (by citations): Title | Year | Conference | Citations | DOI/Link\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations | 2015 | InfoVis | 292 | 10.1109/tvcg.2015.2467191\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco | 2018 | InfoVis | 177 | 10.1109/tvcg.2018.2865240\nA Design Space of Visualization Tasks | 2013 | InfoVis | 144 | 10.1109/tvcg.2013.120\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication | 2018 | InfoVis | 121 | 10.1109/tvcg.2018.2865145\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning | 2019 | VAST | 106 | 10.1109/vast47406.2019.8986948\n### End of facts\n",
                    "text": "The first visualization contrasts AutoVis paper counts by year and their distribution across top conferences. The chart shows a sparse early presence (single or a few AutoVis works before 2014), steady growth from 2014 onward, and pronounced surges around 2015\u20132016 and especially 2020\u20132023 with a single-year peak of 16 AutoVis papers in 2021 and another high year in 2023 (12 papers). Conference breakout emphasizes that the bulk of AutoVis work appears in the Vis umbrella venues \u2014 Vis, VAST, and InfoVis \u2014 with Vis contributing the largest share (45 papers) and VAST and InfoVis also important. These patterns indicate that AutoVis moved from occasional, domain-specific demonstrations to a community-substantiated research thread concentrated in flagship venues over the last decade."
                },
                {
                    "id": 2,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_da8e3acc\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Field\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Field\", \"sort\": \"-y\", \"title\": \"Metadata field\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"MissingRate\", \"title\": \"Missing rate\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"text\", \"color\": \"black\", \"dy\": -10}, \"encoding\": {\"color\": {\"field\": \"Field\", \"legend\": null, \"type\": \"nominal\"}, \"text\": {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"MissingRate\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Field\", \"sort\": \"-y\", \"title\": \"Metadata field\", \"type\": \"nominal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"MissingRate\", \"title\": \"Missing rate\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-73e2e7cafdc1f44f4faf1fb4e5c3c0f6\"}, \"height\": 220, \"width\": 500}, {\"data\": {\"name\": \"data-e0ef4d2cebecd45c8c742a4e9bc6e26e\"}, \"mark\": {\"type\": \"rect\"}, \"encoding\": {\"color\": {\"field\": \"Missing\", \"legend\": {\"title\": \"Missing rate\"}, \"scale\": {\"scheme\": \"magma\"}, \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"nominal\"}, {\"field\": \"Field\", \"type\": \"nominal\"}, {\"field\": \"Missing\", \"format\": \".0%\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Field\", \"title\": \"Metadata field\", \"type\": \"nominal\"}}, \"height\": 220, \"width\": 500}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-73e2e7cafdc1f44f4faf1fb4e5c3c0f6\": [{\"Field\": \"Abstract\", \"Missing\": 0.0, \"MissingRate\": 0.0}, {\"Field\": \"AuthorKeywords\", \"Missing\": 0.07228915662650602, \"MissingRate\": 0.07228915662650602}, {\"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0, \"MissingRate\": 0.0}, {\"Field\": \"DOI\", \"Missing\": 0.0, \"MissingRate\": 0.0}], \"data-e0ef4d2cebecd45c8c742a4e9bc6e26e\": [{\"Year\": \"1995\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"Abstract\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"AuthorKeywords\", \"Missing\": 1.0}, {\"Year\": \"2004\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.5}, {\"Year\": \"2006\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"AuthorKeywords\", \"Missing\": 1.0}, {\"Year\": \"2010\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.5}, {\"Year\": \"2012\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.25}, {\"Year\": \"2016\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"AuthorKeywords\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"AuthorNames-Deduped\", \"Missing\": 0.0}, {\"Year\": \"1995\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2004\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2006\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2007\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2008\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2009\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2010\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2011\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2012\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2013\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2014\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2015\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2016\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2017\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2018\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2019\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2020\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2021\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2022\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2023\", \"Field\": \"DOI\", \"Missing\": 0.0}, {\"Year\": \"2024\", \"Field\": \"DOI\", \"Missing\": 0.0}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_da8e3acc\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nDataset source: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nTotal records: 83\nMissing rate for Abstract: 0.000% (0/83)\nMissing rate for AuthorKeywords: 7.229% (6/83)\nMissing rate for AuthorNames-Deduped: 0.000% (0/83)\nMissing rate for DOI: 0.000% (0/83)\n### End of facts\n\n### Begin of facts\nRecords missing BOTH Abstract and AuthorKeywords: 0 (0.000%)\nNo examples (none missing both).\n### End of facts\n\n### Begin of facts\nTop Conference-Year groups by mean missingness (avg over Abstract, AuthorKeywords, AuthorNames-Deduped, DOI) \u2014 groups with >=3 records:\n- InfoVis 2020 | n=3 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- VAST 2014 | n=3 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- VAST 2016 | n=4 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2021 | n=16 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2022 | n=5 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2023 | n=12 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n- Vis 2024 | n=7 | mean_missing=0.0% | Abstract: 0.0%, AuthorKeywords: 0.0%, AuthorNames-Deduped: 0.0%, DOI: 0.0%\n### End of facts\n\n### Begin of facts\nTop groups by DOI missing rate (groups with >=3 records):\n- InfoVis 2020 | n=3 | DOI missing: 0.0%\n- VAST 2014 | n=3 | DOI missing: 0.0%\n- VAST 2016 | n=4 | DOI missing: 0.0%\n- Vis 2021 | n=16 | DOI missing: 0.0%\n- Vis 2022 | n=5 | DOI missing: 0.0%\n### End of facts\n",
                    "text": "The dataset-quality visualization highlights metadata completeness and year-by-year missingness: Abstract, DOI, and deduplicated author names are complete across records, while AuthorKeywords are the only field with nontrivial missingness (about 7.2% overall) and uneven earlier-year coverage. Year-level groups with \u22653 records (several recent Vis and VAST years) show essentially zero mean missingness, indicating reliable recent metadata; older, isolated records are more likely to lack keywords. Practically, this means text-based heuristics relying on Title+Abstract are robust for the majority of the corpus, but relying solely on AuthorKeywords would miss early or incomplete records."
                }
            ]
        },
        {
            "section_number": 3,
            "section_name": "Temporal trends: when did AutoVis rise or fall?",
            "section_size": "medium",
            "section_description": "Quantitative timeline of AutoVis publications and their share of overall Vis papers by year. Visuals planned: annual counts (bar/line), share of AutoVis vs Other (stacked area), moving averages to highlight peaks and declines, and a table of milestone papers (highly-cited or award-winning). This section highlights when interest peaked, whether it sustained or declined, and how impact (citations/downloads) changed over time.",
            "analyses": [
                {
                    "analysis_schema": {
                        "action": "present",
                        "information_needed": {
                            "question_text": "When did AutoVis rise or fall relative to other Vis papers (annual counts and share), and where are the major peaks and declines?",
                            "primary_attributes": [
                                "Year",
                                "Category (AutoVis vs Other)"
                            ],
                            "secondary_attributes": [
                                "PublicationCount",
                                "ShareOfTotal"
                            ],
                            "transformation": [
                                "Classify each paper as 'AutoVis' or 'Other' using keyword filter on (Title + Abstract + AuthorKeywords)",
                                "Aggregate counts grouped by Year and Category",
                                "Compute yearly AutoVis share = AutoVis_count / Total_count and compute a 3-year moving average for counts and share"
                            ],
                            "expected_insight_types": [
                                "trend",
                                "peaks",
                                "decline/sustain",
                                "relative share over time",
                                "timing of maxima/minima"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_3875c1e9\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Category\", \"scale\": {\"range\": [\"#1f77b4\", \"#ff7f0e\"]}, \"title\": \"Category\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"Category\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"stack\": \"zero\", \"title\": \"Number of papers\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"line\", \"size\": 2}, \"encoding\": {\"color\": {\"field\": \"Category\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"Category\", \"type\": \"nominal\"}, {\"field\": \"count_ma\", \"title\": \"Count (3-yr MA)\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"type\": \"ordinal\"}, \"y\": {\"field\": \"count_ma\", \"title\": \"3-year moving average\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9b182fc48e01b05d5f3ed9e566204315\"}, \"height\": 320, \"title\": \"Yearly paper counts: AutoVis vs Other (with 3-year MA)\", \"width\": 700}, {\"layer\": [{\"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVis_share\", \"title\": \"AutoVis share\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"AutoVis_share\", \"title\": \"Share of AutoVis papers\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"strokeDash\": [6, 6]}, \"encoding\": {\"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"type\": \"ordinal\"}, \"y\": {\"field\": \"AutoVis_share_ma\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-aab9ced04171000f2c5bc07b52840cf0\"}, \"height\": 160, \"title\": \"AutoVis share of total papers (with 3-year MA)\", \"width\": 700}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-9b182fc48e01b05d5f3ed9e566204315\": [{\"Year\": 1995, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 2.0}, {\"Year\": 1995, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2004, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 2.0}, {\"Year\": 2004, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2006, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 1.3333333333333333}, {\"Year\": 2006, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2007, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.6666666666666666}, {\"Year\": 2007, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.6666666666666666}, {\"Year\": 2008, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2008, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 1.0}, {\"Year\": 2009, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.6666666666666666}, {\"Year\": 2009, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2010, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2010, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.6666666666666666}, {\"Year\": 2011, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2011, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2012, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2012, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 1.0}, {\"Year\": 2013, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2013, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 1.0}, {\"Year\": 2014, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2014, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2015, \"Category\": \"AutoVis\", \"count\": 4, \"count_ma\": 2.0}, {\"Year\": 2015, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 1.0}, {\"Year\": 2016, \"Category\": \"AutoVis\", \"count\": 3, \"count_ma\": 3.0}, {\"Year\": 2016, \"Category\": \"Other\", \"count\": 3, \"count_ma\": 1.6666666666666667}, {\"Year\": 2017, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 2.3333333333333335}, {\"Year\": 2017, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.6666666666666667}, {\"Year\": 2018, \"Category\": \"AutoVis\", \"count\": 3, \"count_ma\": 2.0}, {\"Year\": 2018, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 2.0}, {\"Year\": 2019, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.6666666666666667}, {\"Year\": 2019, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 1.3333333333333333}, {\"Year\": 2020, \"Category\": \"AutoVis\", \"count\": 4, \"count_ma\": 2.6666666666666665}, {\"Year\": 2020, \"Category\": \"Other\", \"count\": 3, \"count_ma\": 2.0}, {\"Year\": 2021, \"Category\": \"AutoVis\", \"count\": 9, \"count_ma\": 4.666666666666667}, {\"Year\": 2021, \"Category\": \"Other\", \"count\": 7, \"count_ma\": 4.0}, {\"Year\": 2022, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 4.666666666666667}, {\"Year\": 2022, \"Category\": \"Other\", \"count\": 4, \"count_ma\": 4.666666666666667}, {\"Year\": 2023, \"Category\": \"AutoVis\", \"count\": 6, \"count_ma\": 5.333333333333333}, {\"Year\": 2023, \"Category\": \"Other\", \"count\": 6, \"count_ma\": 5.666666666666667}, {\"Year\": 2024, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 2.6666666666666665}, {\"Year\": 2024, \"Category\": \"Other\", \"count\": 6, \"count_ma\": 5.333333333333333}], \"data-aab9ced04171000f2c5bc07b52840cf0\": [{\"Year\": 1995, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2004, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2006, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2007, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2008, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2009, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2010, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2011, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2012, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2013, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2014, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2015, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2016, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2017, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2018, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2019, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2020, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2021, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2022, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2023, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2024, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_3875c1e9\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nTime span analyzed: 1995\u20132024 | Total papers: 83 | AutoVis papers: 83 | Overall AutoVis share: 1.000\n### End of facts\n### Begin of facts\nPeak AutoVis count: 16 papers in 2021\nPeak AutoVis share (of vis papers): 100.000% in 1995\n### End of facts\n### Begin of facts\n3-year moving average peak (AutoVis count): 11.00 in 2023\nTop 5 years by AutoVis count: 2021:16; 2023:12; 2024:7; 2020:7; 2016:6\n### End of facts\n### Begin of facts\nLargest one-year change in AutoVis count: -11 (year 2022, previous year 2021)\nLargest one-year drop in AutoVis share: 0.000% (year 1995)\n### End of facts\n### Begin of facts\nYearly summary (first 5 rows):\nCategory  AutoVis  Other  Total  ShareOfTotal  AutoVis_MA3  Share_MA3\nYear                                                                 \n1995            2      0      2           1.0     2.000000        1.0\n2004            2      0      2           1.0     2.000000        1.0\n2006            1      0      1           1.0     1.666667        1.0\n2007            1      0      1           1.0     1.333333        1.0\n2008            1      0      1           1.0     1.000000        1.0\n### End of facts\n### Begin of facts\nYearly summary (last 5 rows):\nCategory  AutoVis  Other  Total  ShareOfTotal  AutoVis_MA3  Share_MA3\nYear                                                                 \n2020            7      0      7           1.0     4.666667        1.0\n2021           16      0     16           1.0     8.666667        1.0\n2022            5      0      5           1.0     9.333333        1.0\n2023           12      0     12           1.0    11.000000        1.0\n2024            7      0      7           1.0     8.000000        1.0\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                },
                {
                    "analysis_schema": {
                        "action": "explore",
                        "information_needed": {
                            "question_text": "Which papers should be selected as 'milestone' AutoVis papers for a compact table (high-impact or award-winning)?",
                            "key_uncertainty": "What concrete thresholds and selection rules balance coverage and conciseness \u2014 which citation field to use (AminerCitationCount vs CitationCount_CrossRef), what citation/download cutoff or top-N rule, and whether to include award-flagged papers regardless of metrics?",
                            "expected_outputs": [
                                "Recommendation for selection rule (e.g., include all Award!='' plus top N papers by chosen citation metric OR papers above an absolute citation threshold); suggested numeric thresholds (e.g., top 1% of AutoVis by AminerCitationCount or citation >= X) after inspecting distribution",
                                "Decision on which citation metric to use (AminerCitationCount or CitationCount_CrossRef) and whether to combine with Downloads_Xplore for recency bias",
                                "A candidate list size for the milestone table (e.g., 10\u201320) and a short candidate set (top ~20 by chosen metric and award winners) to review for final inclusion"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_bcb7f009\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"name\": \"data-97b427d34fd148fb396950ae148f8bb7\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"award_flag\", \"scale\": {\"domain\": [false, true], \"range\": [\"steelblue\", \"orange\"]}, \"title\": \"Award Winner\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Title\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AuthorNames-Deduped\", \"title\": \"Authors\", \"type\": \"nominal\"}, {\"field\": \"citation_pref\", \"title\": \"Preferred citations\", \"type\": \"quantitative\"}, {\"field\": \"AminerCitationCount\", \"type\": \"quantitative\"}, {\"field\": \"CitationCount_CrossRef\", \"type\": \"quantitative\"}, {\"field\": \"Downloads_Xplore\", \"type\": \"quantitative\"}, {\"field\": \"Award\", \"type\": \"nominal\"}], \"x\": {\"field\": \"citation_pref\", \"title\": \"Preferred citation count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Title\", \"sort\": {\"field\": \"citation_pref\", \"order\": \"descending\"}, \"title\": \"Paper Title\", \"type\": \"nominal\"}}, \"height\": 600, \"title\": \"Top 20 candidate milestone AutoVis papers (by preferred citation metric)\", \"width\": 800}, {\"data\": {\"name\": \"data-54c313becad736b6a64fe20de391da14\"}, \"mark\": {\"type\": \"circle\"}, \"encoding\": {\"color\": {\"field\": \"award_flag\", \"scale\": {\"domain\": [false, true], \"range\": [\"steelblue\", \"orange\"]}, \"title\": \"Award Winner\", \"type\": \"nominal\"}, \"size\": {\"field\": \"Downloads_Xplore\", \"scale\": {\"range\": [10, 400]}, \"title\": \"Downloads (Xplore)\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Title\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AuthorNames-Deduped\", \"title\": \"Authors\", \"type\": \"nominal\"}, {\"field\": \"AminerCitationCount\", \"type\": \"quantitative\"}, {\"field\": \"CitationCount_CrossRef\", \"type\": \"quantitative\"}, {\"field\": \"Downloads_Xplore\", \"type\": \"quantitative\"}, {\"field\": \"Award\", \"type\": \"nominal\"}], \"x\": {\"field\": \"AminerCitationCount\", \"title\": \"AminerCitationCount\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CitationCount_CrossRef\", \"title\": \"CitationCount (CrossRef)\", \"type\": \"quantitative\"}}, \"height\": 320, \"name\": \"view_1\", \"title\": \"Aminer vs CrossRef citations (point size = downloads)\", \"width\": 800}], \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_1\"]}], \"resolve\": {\"scale\": {\"color\": \"shared\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-97b427d34fd148fb396950ae148f8bb7\": [{\"Conference\": \"InfoVis\", \"Year\": 2015, \"Title\": \"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations\", \"DOI\": \"10.1109/tvcg.2015.2467191\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2015.2467191\", \"FirstPage\": 649.0, \"LastPage\": 658.0, \"PaperType\": \"J\", \"Abstract\": \"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.\", \"AuthorNames-Deduped\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297\", \"AuthorKeywords\": \"User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems\", \"AminerCitationCount\": 487.0, \"CitationCount_CrossRef\": 292.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 4307.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 487.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco\", \"DOI\": \"10.1109/tvcg.2018.2865240\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865240\", \"FirstPage\": 438.0, \"LastPage\": 448.0, \"PaperType\": \"J\", \"Abstract\": \"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.\", \"AuthorNames-Deduped\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming\", \"AminerCitationCount\": 225.0, \"CitationCount_CrossRef\": 177.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 3238.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 225.0}, {\"Conference\": \"InfoVis\", \"Year\": 2013, \"Title\": \"A Design Space of Visualization Tasks\", \"DOI\": \"10.1109/tvcg.2013.120\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2013.120\", \"FirstPage\": 2366.0, \"LastPage\": 2375.0, \"PaperType\": \"J\", \"Abstract\": \"Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.\", \"AuthorNames-Deduped\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorNames\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorAffiliation\": \"University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany\", \"InternalReferences\": \"10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213\", \"AuthorKeywords\": \"Task taxonomy, design space, climate impact research, visualization recommendation\", \"AminerCitationCount\": 217.0, \"CitationCount_CrossRef\": 144.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 4884.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 217.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Finding Waldo: Learning about Users from their Interactions\", \"DOI\": \"10.1109/tvcg.2014.2346575\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346575\", \"FirstPage\": 1663.0, \"LastPage\": 1672.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.\", \"AuthorNames-Deduped\": \"Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorNames\": \"Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorAffiliation\": \"Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U\", \"InternalReferences\": \"10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204\", \"AuthorKeywords\": \"User Interactions, Analytic Provenance, Visualization, Applied Machine Learning\", \"AminerCitationCount\": 145.0, \"CitationCount_CrossRef\": 95.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 2226.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 145.0}, {\"Conference\": \"InfoVis\", \"Year\": 2014, \"Title\": \"Learning Perceptual Kernels for Visualization Design\", \"DOI\": \"10.1109/tvcg.2014.2346978\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346978\", \"FirstPage\": 1933.0, \"LastPage\": 1942.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.\", \"AuthorNames-Deduped\": \"\\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorNames\": \"\\u00c7a\\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorAffiliation\": \"Stanford University;Stanford University;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186\", \"AuthorKeywords\": \"Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding\", \"AminerCitationCount\": 129.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 1247.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 129.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication\", \"DOI\": \"10.1109/tvcg.2018.2865145\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865145\", \"FirstPage\": 672.0, \"LastPage\": 681.0, \"PaperType\": \"J\", \"Abstract\": \"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.\", \"AuthorNames-Deduped\": \"Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\", \"AuthorNames\": \"Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication\", \"AminerCitationCount\": 120.0, \"CitationCount_CrossRef\": 121.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 2942.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 120.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics\", \"DOI\": \"10.1109/vast.2017.8585669\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2017.8585669\", \"FirstPage\": 104.0, \"LastPage\": 115.0, \"PaperType\": \"C\", \"Abstract\": \"Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.\", \"AuthorNames-Deduped\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorNames\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorAffiliation\": \"Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General\", \"AminerCitationCount\": 115.0, \"CitationCount_CrossRef\": 70.0, \"PubsCited_CrossRef\": 80.0, \"Downloads_Xplore\": 1801.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 115.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Data-Driven Guides: Supporting Expressive Design for Information Graphics\", \"DOI\": \"10.1109/tvcg.2016.2598620\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598620\", \"FirstPage\": 491.0, \"LastPage\": 500.0, \"PaperType\": \"J\", \"Abstract\": \"In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.\", \"AuthorNames-Deduped\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorNames\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorAffiliation\": \"John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Information graphics;visualization;design tools;2D graphics\", \"AminerCitationCount\": 114.0, \"CitationCount_CrossRef\": 92.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2245.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 114.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks\", \"DOI\": \"10.1109/tvcg.2018.2864504\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864504\", \"FirstPage\": 288.0, \"LastPage\": 298.0, \"PaperType\": \"J\", \"Abstract\": \"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.\", \"AuthorNames-Deduped\": \"Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\", \"AuthorNames\": \"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang\", \"AuthorAffiliation\": \"The Ohio State University;Visa Research;The Ohio State University;Visa Research\", \"InternalReferences\": \"10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683\", \"AuthorKeywords\": \"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics\", \"AminerCitationCount\": 108.0, \"CitationCount_CrossRef\": 91.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2871.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 108.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning\", \"DOI\": \"10.1109/vast47406.2019.8986948\", \"Link\": \"http://dx.doi.org/10.1109/VAST47406.2019.8986948\", \"FirstPage\": 46.0, \"LastPage\": 56.0, \"PaperType\": \"C\", \"Abstract\": \"The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.\", \"AuthorNames-Deduped\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorNames\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718\", \"AuthorKeywords\": \"Machine learning fairness,visual analytics,intersectional bias,subgroup discovery\", \"AminerCitationCount\": 107.0, \"CitationCount_CrossRef\": 106.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 2108.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 107.0}, {\"Conference\": \"Vis\", \"Year\": 2007, \"Title\": \"Interactive Visual Analysis of Perfusion Data\", \"DOI\": \"10.1109/tvcg.2007.70569\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2007.70569\", \"FirstPage\": 1392.0, \"LastPage\": 1399.0, \"PaperType\": \"J\", \"Abstract\": \"Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorNames\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorAffiliation\": \"Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany\", \"InternalReferences\": \"10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739\", \"AuthorKeywords\": \"Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis\", \"AminerCitationCount\": 100.0, \"CitationCount_CrossRef\": 44.0, \"PubsCited_CrossRef\": 28.0, \"Downloads_Xplore\": 666.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 100.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration\", \"DOI\": \"10.1109/tvcg.2016.2598839\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598839\", \"FirstPage\": 331.0, \"LastPage\": 340.0, \"PaperType\": \"J\", \"Abstract\": \"Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.\", \"AuthorNames-Deduped\": \"Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Visual Data Exploration;Visualization by Demonstration;Visualization Tools\", \"AminerCitationCount\": 83.0, \"CitationCount_CrossRef\": 57.0, \"PubsCited_CrossRef\": 35.0, \"Downloads_Xplore\": 2781.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 83.0}, {\"Conference\": \"InfoVis\", \"Year\": 2019, \"Title\": \"Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements\", \"DOI\": \"10.1109/tvcg.2019.2934785\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934785\", \"FirstPage\": 906.0, \"LastPage\": 916.0, \"PaperType\": \"J\", \"Abstract\": \"Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Visualization for the masses,infographic,automatic visualization,presentation,and dissemination\", \"AminerCitationCount\": 79.0, \"CitationCount_CrossRef\": 71.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 2661.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 79.0}, {\"Conference\": \"InfoVis\", \"Year\": 2008, \"Title\": \"Multi-Focused Geospatial Analysis Using Probes\", \"DOI\": \"10.1109/tvcg.2008.149\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2008.149\", \"FirstPage\": 1165.0, \"LastPage\": 1172.0, \"PaperType\": \"J\", \"Abstract\": \"Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.\", \"AuthorNames-Deduped\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorNames\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorAffiliation\": \"UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center\", \"InternalReferences\": \"10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102\", \"AuthorKeywords\": \"Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes\", \"AminerCitationCount\": 73.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 20.0, \"Downloads_Xplore\": 648.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 73.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Calliope: Automatic Visual Data Story Generation from a Spreadsheet\", \"DOI\": \"10.1109/tvcg.2020.3030403\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030403\", \"FirstPage\": 453.0, \"LastPage\": 463.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.\", \"AuthorNames-Deduped\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\", \"AuthorNames\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Information Visualization,Visual Storytelling,Data Story\", \"AminerCitationCount\": 56.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 57.0, \"Downloads_Xplore\": 3724.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 56.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Podium: Ranking Data Using Mixed-Initiative Visual Analytics\", \"DOI\": \"10.1109/tvcg.2017.2745078\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2017.2745078\", \"FirstPage\": 288.0, \"LastPage\": 297.0, \"PaperType\": \"J\", \"Abstract\": \"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.\", \"AuthorNames-Deduped\": \"Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Mixed-initiative visual analytics,multi-attribute ranking,user interaction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 52.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1535.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 52.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution\", \"DOI\": \"10.1109/tvcg.2018.2864769\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864769\", \"FirstPage\": 374.0, \"LastPage\": 384.0, \"PaperType\": \"J\", \"Abstract\": \"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001\", \"AuthorNames\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins\", \"AuthorAffiliation\": \"Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning\", \"AminerCitationCount\": 47.0, \"CitationCount_CrossRef\": 40.0, \"PubsCited_CrossRef\": 69.0, \"Downloads_Xplore\": 1217.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 47.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods\", \"DOI\": \"10.1109/tvcg.2016.2598544\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598544\", \"FirstPage\": 271.0, \"LastPage\": 280.0, \"PaperType\": \"J\", \"Abstract\": \"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.\", \"AuthorNames-Deduped\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne\", \"AuthorNames\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory\", \"InternalReferences\": \"10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591\", \"AuthorKeywords\": \"trust;transparency;familiarity;uncertainty;biological data analysis\", \"AminerCitationCount\": 41.0, \"CitationCount_CrossRef\": 41.0, \"PubsCited_CrossRef\": 41.0, \"Downloads_Xplore\": 1844.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 41.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations\", \"DOI\": \"10.1109/tvcg.2016.2598543\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598543\", \"FirstPage\": 261.0, \"LastPage\": 270.0, \"PaperType\": \"J\", \"Abstract\": \"User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.\", \"AuthorNames-Deduped\": \"Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorNames\": \"Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorAffiliation\": \"Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research\", \"InternalReferences\": \"10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878\", \"AuthorKeywords\": \"Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization\", \"AminerCitationCount\": 39.0, \"CitationCount_CrossRef\": 33.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 2188.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 39.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Mixed-initiative visual analytics using task-driven recommendations\", \"DOI\": \"10.1109/vast.2015.7347625\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347625\", \"FirstPage\": 9.0, \"LastPage\": 16.0, \"PaperType\": \"C\", \"Abstract\": \"Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.\", \"AuthorNames-Deduped\": \"Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorNames\": \"Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"mixed-initiative visual analytics, task modeling, recommender systems, sensemaking\", \"AminerCitationCount\": 36.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 36.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 36.0}], \"data-54c313becad736b6a64fe20de391da14\": [{\"Conference\": \"InfoVis\", \"Year\": 1995, \"Title\": \"Towards a generative theory of diagram design\", \"DOI\": \"10.1109/infvis.1995.528681\", \"Link\": \"http://dx.doi.org/10.1109/INFVIS.1995.528681\", \"FirstPage\": 11.0, \"LastPage\": 18.0, \"PaperType\": \"C\", \"Abstract\": \"We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.\", \"AuthorNames-Deduped\": \"Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky\", \"AuthorNames\": \"K. Reichenberger;T. Kamps;G. Golovchinsky\", \"AuthorAffiliation\": \"Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada\", \"InternalReferences\": \"10.1109/visual.1995.480815;10.1109/visual.1995.480815\", \"AuthorKeywords\": null, \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 18.0, \"Downloads_Xplore\": 133.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"Vis\", \"Year\": 1995, \"Title\": \"Subverting structure: data-driven diagram generation\", \"DOI\": \"10.1109/visual.1995.480815\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.1995.480815\", \"FirstPage\": 217.0, \"LastPage\": null, \"PaperType\": \"C\", \"Abstract\": \"Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.\", \"AuthorNames-Deduped\": \"Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps\", \"AuthorNames\": \"G. Golovchinsky;T. Kamps;K. Reichenberger\", \"AuthorAffiliation\": \"Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany\", \"InternalReferences\": \"10.1109/infvis.1995.528681;10.1109/infvis.1995.528681\", \"AuthorKeywords\": null, \"AminerCitationCount\": 21.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 11.0, \"Downloads_Xplore\": 66.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 21.0}, {\"Conference\": \"Vis\", \"Year\": 2004, \"Title\": \"Non-linear model fitting to parameterize diseased blood vessels\", \"DOI\": \"10.1109/visual.2004.72\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.2004.72\", \"FirstPage\": 393.0, \"LastPage\": 400.0, \"PaperType\": \"C\", \"Abstract\": \"Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.\", \"AuthorNames-Deduped\": \"Alexandra La Cruz;Mat\\u00fas Straka;Arnold K\\u00f6chl;Milos Sr\\u00e1mek;M. Eduard Gr\\u00f6ller;Dominik Fleischmann\", \"AuthorNames\": \"A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann\", \"AuthorAffiliation\": \"University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA\", \"InternalReferences\": \"10.1109/visual.2001.964555\", \"AuthorKeywords\": \"Visualization, Segmentation, Blood Vessel Detection\", \"AminerCitationCount\": 29.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 11.0, \"Downloads_Xplore\": 141.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 29.0}, {\"Conference\": \"Vis\", \"Year\": 2004, \"Title\": \"Context-Adaptive Mobile Visualization and Information Management\", \"DOI\": \"10.1109/visual.2004.19\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.2004.19\", \"FirstPage\": 8.0, \"LastPage\": 8.0, \"PaperType\": \"M\", \"Abstract\": \"This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.\", \"AuthorNames-Deduped\": \"Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen\", \"AuthorNames\": \"J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen\", \"AuthorAffiliation\": \"Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany\", \"InternalReferences\": null, \"AuthorKeywords\": null, \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 4.0, \"Downloads_Xplore\": 172.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"VAST\", \"Year\": 2006, \"Title\": \"Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information\", \"DOI\": \"10.1109/vast.2006.261415\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2006.261415\", \"FirstPage\": 137.0, \"LastPage\": 144.0, \"PaperType\": \"C\", \"Abstract\": \"We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication\", \"AuthorNames-Deduped\": \"Paul E. Keel\", \"AuthorNames\": \"Paul E. Keel\", \"AuthorAffiliation\": \"Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK\", \"InternalReferences\": null, \"AuthorKeywords\": \"Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making\", \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 24.0, \"PubsCited_CrossRef\": 23.0, \"Downloads_Xplore\": 472.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"Vis\", \"Year\": 2007, \"Title\": \"Interactive Visual Analysis of Perfusion Data\", \"DOI\": \"10.1109/tvcg.2007.70569\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2007.70569\", \"FirstPage\": 1392.0, \"LastPage\": 1399.0, \"PaperType\": \"J\", \"Abstract\": \"Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorNames\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorAffiliation\": \"Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany\", \"InternalReferences\": \"10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739\", \"AuthorKeywords\": \"Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis\", \"AminerCitationCount\": 100.0, \"CitationCount_CrossRef\": 44.0, \"PubsCited_CrossRef\": 28.0, \"Downloads_Xplore\": 666.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 100.0}, {\"Conference\": \"InfoVis\", \"Year\": 2008, \"Title\": \"Multi-Focused Geospatial Analysis Using Probes\", \"DOI\": \"10.1109/tvcg.2008.149\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2008.149\", \"FirstPage\": 1165.0, \"LastPage\": 1172.0, \"PaperType\": \"J\", \"Abstract\": \"Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.\", \"AuthorNames-Deduped\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorNames\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorAffiliation\": \"UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center\", \"InternalReferences\": \"10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102\", \"AuthorKeywords\": \"Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes\", \"AminerCitationCount\": 73.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 20.0, \"Downloads_Xplore\": 648.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 73.0}, {\"Conference\": \"VAST\", \"Year\": 2009, \"Title\": \"Articulate: a conversational interface for visual analytics\", \"DOI\": \"10.1109/vast.2009.5333099\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2009.5333099\", \"FirstPage\": 233.0, \"LastPage\": 234.0, \"PaperType\": \"M\", \"Abstract\": \"While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.\", \"AuthorNames-Deduped\": \"Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau\", \"AuthorNames\": \"Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau\", \"AuthorAffiliation\": \"Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA\", \"InternalReferences\": \"0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148\", \"AuthorKeywords\": null, \"AminerCitationCount\": 3.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 9.0, \"Downloads_Xplore\": 267.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 3.0}, {\"Conference\": \"VAST\", \"Year\": 2010, \"Title\": \"ALIDA: Using machine learning for intent discernment in visual analytics interfaces\", \"DOI\": \"10.1109/vast.2010.5650854\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2010.5650854\", \"FirstPage\": 223.0, \"LastPage\": 224.0, \"PaperType\": \"M\", \"Abstract\": \"In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.\", \"AuthorNames-Deduped\": \"Tera Marie Green;Ross Maciejewski;Steve DiPaola\", \"AuthorNames\": \"Tera Marie Green;Ross Maciejewski;Steve DiPaola\", \"AuthorAffiliation\": \"School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada\", \"InternalReferences\": null, \"AuthorKeywords\": \"artificial intelligence, cognition, intent discernment, volume rendering\", \"AminerCitationCount\": 4.0, \"CitationCount_CrossRef\": 3.0, \"PubsCited_CrossRef\": 6.0, \"Downloads_Xplore\": 391.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 4.0}, {\"Conference\": \"Vis\", \"Year\": 2011, \"Title\": \"Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics\", \"DOI\": \"10.1109/tvcg.2011.217\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2011.217\", \"FirstPage\": 1882.0, \"LastPage\": 1891.0, \"PaperType\": \"J\", \"Abstract\": \"In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert\", \"AuthorNames\": \"Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert\", \"AuthorAffiliation\": \"University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany\", \"InternalReferences\": \"10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911\", \"AuthorKeywords\": \"Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization\", \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 9.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 780.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"VAST\", \"Year\": 2011, \"Title\": \"Exploring agent-based simulations using temporal graphs\", \"DOI\": \"10.1109/vast.2011.6102469\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2011.6102469\", \"FirstPage\": 271.0, \"LastPage\": 272.0, \"PaperType\": \"M\", \"Abstract\": \"Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.\", \"AuthorNames-Deduped\": \"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang\", \"AuthorNames\": \"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang\", \"AuthorAffiliation\": \"Tufts University, USA;Tufts University, USA;Tufts University, USA\", \"InternalReferences\": \"0.1109/infvis.2005.1532126\", \"AuthorKeywords\": null, \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 8.0, \"Downloads_Xplore\": 163.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"SciVis\", \"Year\": 2012, \"Title\": \"Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization\", \"DOI\": \"10.1109/tvcg.2012.203\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2012.203\", \"FirstPage\": 2345.0, \"LastPage\": 2354.0, \"PaperType\": \"J\", \"Abstract\": \"Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.\", \"AuthorNames-Deduped\": \"Gunnar L\\u00e4th\\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga\", \"AuthorNames\": \"Gunnar L\\u00e4th\\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga\", \"AuthorAffiliation\": \"Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\\u00f6ping University, Sweden\", \"InternalReferences\": \"10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123\", \"AuthorKeywords\": \"Direct volume rendering, transfer functions, vessel visualization\", \"AminerCitationCount\": 29.0, \"CitationCount_CrossRef\": 14.0, \"PubsCited_CrossRef\": 34.0, \"Downloads_Xplore\": 513.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 29.0}, {\"Conference\": \"InfoVis\", \"Year\": 2013, \"Title\": \"A Design Space of Visualization Tasks\", \"DOI\": \"10.1109/tvcg.2013.120\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2013.120\", \"FirstPage\": 2366.0, \"LastPage\": 2375.0, \"PaperType\": \"J\", \"Abstract\": \"Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.\", \"AuthorNames-Deduped\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorNames\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorAffiliation\": \"University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany\", \"InternalReferences\": \"10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213\", \"AuthorKeywords\": \"Task taxonomy, design space, climate impact research, visualization recommendation\", \"AminerCitationCount\": 217.0, \"CitationCount_CrossRef\": 144.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 4884.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 217.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Finding Waldo: Learning about Users from their Interactions\", \"DOI\": \"10.1109/tvcg.2014.2346575\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346575\", \"FirstPage\": 1663.0, \"LastPage\": 1672.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.\", \"AuthorNames-Deduped\": \"Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorNames\": \"Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorAffiliation\": \"Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U\", \"InternalReferences\": \"10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204\", \"AuthorKeywords\": \"User Interactions, Analytic Provenance, Visualization, Applied Machine Learning\", \"AminerCitationCount\": 145.0, \"CitationCount_CrossRef\": 95.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 2226.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 145.0}, {\"Conference\": \"InfoVis\", \"Year\": 2014, \"Title\": \"Learning Perceptual Kernels for Visualization Design\", \"DOI\": \"10.1109/tvcg.2014.2346978\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346978\", \"FirstPage\": 1933.0, \"LastPage\": 1942.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.\", \"AuthorNames-Deduped\": \"\\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorNames\": \"\\u00c7a\\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorAffiliation\": \"Stanford University;Stanford University;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186\", \"AuthorKeywords\": \"Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding\", \"AminerCitationCount\": 129.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 1247.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 129.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs\", \"DOI\": \"10.1109/vast.2014.7042485\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2014.7042485\", \"FirstPage\": 93.0, \"LastPage\": 102.0, \"PaperType\": \"C\", \"Abstract\": \"Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.\", \"AuthorNames-Deduped\": \"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger\", \"AuthorNames\": \"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger\", \"AuthorAffiliation\": \"GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt\", \"InternalReferences\": \"10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225\", \"AuthorKeywords\": \"Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization\", \"AminerCitationCount\": 14.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 51.0, \"Downloads_Xplore\": 331.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging\", \"DOI\": \"10.1109/vast.2014.7042481\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2014.7042481\", \"FirstPage\": 53.0, \"LastPage\": 62.0, \"PaperType\": \"C\", \"Abstract\": \"For cancers such as glioblastoma multiforme, there is an increasing interest in defining \\\"biological target volumes\\\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.\", \"AuthorNames-Deduped\": \"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\\u00e9akh\\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\\u00fchler\", \"AuthorNames\": \"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\\u00e9akh\\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\\u00fchler\", \"AuthorAffiliation\": \"VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria\", \"InternalReferences\": \"10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569\", \"AuthorKeywords\": \"MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems\", \"AminerCitationCount\": 17.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 29.0, \"Downloads_Xplore\": 300.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 17.0}, {\"Conference\": \"InfoVis\", \"Year\": 2015, \"Title\": \"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations\", \"DOI\": \"10.1109/tvcg.2015.2467191\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2015.2467191\", \"FirstPage\": 649.0, \"LastPage\": 658.0, \"PaperType\": \"J\", \"Abstract\": \"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.\", \"AuthorNames-Deduped\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297\", \"AuthorKeywords\": \"User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems\", \"AminerCitationCount\": 487.0, \"CitationCount_CrossRef\": 292.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 4307.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 487.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Mixed-initiative visual analytics using task-driven recommendations\", \"DOI\": \"10.1109/vast.2015.7347625\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347625\", \"FirstPage\": 9.0, \"LastPage\": 16.0, \"PaperType\": \"C\", \"Abstract\": \"Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.\", \"AuthorNames-Deduped\": \"Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorNames\": \"Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"mixed-initiative visual analytics, task modeling, recommender systems, sensemaking\", \"AminerCitationCount\": 36.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 36.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 36.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Collaborative visual analysis with RCloud\", \"DOI\": \"10.1109/vast.2015.7347627\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347627\", \"FirstPage\": 25.0, \"LastPage\": 32.0, \"PaperType\": \"C\", \"Abstract\": \"Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.\", \"AuthorNames-Deduped\": \"Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull\", \"AuthorNames\": \"Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull\", \"AuthorAffiliation\": \"Infovisible;University of Arizona;AT&T Labs;AT&T Labs\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work\", \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 40.0, \"Downloads_Xplore\": 404.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"SciVis\", \"Year\": 2015, \"Title\": \"Automated visualization workflow for simulation experiments\", \"DOI\": \"10.1109/scivis.2015.7429509\", \"Link\": \"http://dx.doi.org/10.1109/SciVis.2015.7429509\", \"FirstPage\": 153.0, \"LastPage\": 154.0, \"PaperType\": \"M\", \"Abstract\": \"Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.\", \"AuthorNames-Deduped\": \"Jonathan P. Leidig;Santhosh Dharmapuri\", \"AuthorNames\": \"Jonathan P. Leidig;Santhosh Dharmapuri\", \"AuthorAffiliation\": \"School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University\", \"InternalReferences\": null, \"AuthorKeywords\": null, \"AminerCitationCount\": 1.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 12.0, \"Downloads_Xplore\": 137.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Data-Driven Guides: Supporting Expressive Design for Information Graphics\", \"DOI\": \"10.1109/tvcg.2016.2598620\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598620\", \"FirstPage\": 491.0, \"LastPage\": 500.0, \"PaperType\": \"J\", \"Abstract\": \"In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.\", \"AuthorNames-Deduped\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorNames\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorAffiliation\": \"John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Information graphics;visualization;design tools;2D graphics\", \"AminerCitationCount\": 114.0, \"CitationCount_CrossRef\": 92.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2245.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 114.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration\", \"DOI\": \"10.1109/tvcg.2016.2598839\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598839\", \"FirstPage\": 331.0, \"LastPage\": 340.0, \"PaperType\": \"J\", \"Abstract\": \"Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.\", \"AuthorNames-Deduped\": \"Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Visual Data Exploration;Visualization by Demonstration;Visualization Tools\", \"AminerCitationCount\": 83.0, \"CitationCount_CrossRef\": 57.0, \"PubsCited_CrossRef\": 35.0, \"Downloads_Xplore\": 2781.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 83.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods\", \"DOI\": \"10.1109/tvcg.2016.2598544\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598544\", \"FirstPage\": 271.0, \"LastPage\": 280.0, \"PaperType\": \"J\", \"Abstract\": \"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.\", \"AuthorNames-Deduped\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne\", \"AuthorNames\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory\", \"InternalReferences\": \"10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591\", \"AuthorKeywords\": \"trust;transparency;familiarity;uncertainty;biological data analysis\", \"AminerCitationCount\": 41.0, \"CitationCount_CrossRef\": 41.0, \"PubsCited_CrossRef\": 41.0, \"Downloads_Xplore\": 1844.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 41.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations\", \"DOI\": \"10.1109/tvcg.2016.2598543\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598543\", \"FirstPage\": 261.0, \"LastPage\": 270.0, \"PaperType\": \"J\", \"Abstract\": \"User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.\", \"AuthorNames-Deduped\": \"Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorNames\": \"Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorAffiliation\": \"Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research\", \"InternalReferences\": \"10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878\", \"AuthorKeywords\": \"Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization\", \"AminerCitationCount\": 39.0, \"CitationCount_CrossRef\": 33.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 2188.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 39.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems\", \"DOI\": \"10.1109/tvcg.2016.2598460\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598460\", \"FirstPage\": 121.0, \"LastPage\": 130.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.\", \"AuthorNames-Deduped\": \"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook\", \"AuthorNames\": \"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook\", \"AuthorAffiliation\": \"Smith College;Smith College;Smith College;Smith College\", \"InternalReferences\": \"10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467\", \"AuthorKeywords\": \"Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking\", \"AminerCitationCount\": 20.0, \"CitationCount_CrossRef\": 16.0, \"PubsCited_CrossRef\": 87.0, \"Downloads_Xplore\": 978.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 20.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment\", \"DOI\": \"10.1109/tvcg.2016.2599378\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2599378\", \"FirstPage\": 231.0, \"LastPage\": 240.0, \"PaperType\": \"J\", \"Abstract\": \"Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.\", \"AuthorNames-Deduped\": \"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu\", \"AuthorNames\": \"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology\", \"InternalReferences\": \"10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1\", \"AuthorKeywords\": \"Centralized matching;matching visualization;interaction techniques;visual analytics\", \"AminerCitationCount\": 7.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 32.0, \"Downloads_Xplore\": 557.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics\", \"DOI\": \"10.1109/vast.2017.8585669\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2017.8585669\", \"FirstPage\": 104.0, \"LastPage\": 115.0, \"PaperType\": \"C\", \"Abstract\": \"Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.\", \"AuthorNames-Deduped\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorNames\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorAffiliation\": \"Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General\", \"AminerCitationCount\": 115.0, \"CitationCount_CrossRef\": 70.0, \"PubsCited_CrossRef\": 80.0, \"Downloads_Xplore\": 1801.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 115.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Podium: Ranking Data Using Mixed-Initiative Visual Analytics\", \"DOI\": \"10.1109/tvcg.2017.2745078\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2017.2745078\", \"FirstPage\": 288.0, \"LastPage\": 297.0, \"PaperType\": \"J\", \"Abstract\": \"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.\", \"AuthorNames-Deduped\": \"Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Mixed-initiative visual analytics,multi-attribute ranking,user interaction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 52.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1535.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 52.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco\", \"DOI\": \"10.1109/tvcg.2018.2865240\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865240\", \"FirstPage\": 438.0, \"LastPage\": 448.0, \"PaperType\": \"J\", \"Abstract\": \"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.\", \"AuthorNames-Deduped\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming\", \"AminerCitationCount\": 225.0, \"CitationCount_CrossRef\": 177.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 3238.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 225.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication\", \"DOI\": \"10.1109/tvcg.2018.2865145\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865145\", \"FirstPage\": 672.0, \"LastPage\": 681.0, \"PaperType\": \"J\", \"Abstract\": \"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.\", \"AuthorNames-Deduped\": \"Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\", \"AuthorNames\": \"Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication\", \"AminerCitationCount\": 120.0, \"CitationCount_CrossRef\": 121.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 2942.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 120.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks\", \"DOI\": \"10.1109/tvcg.2018.2864504\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864504\", \"FirstPage\": 288.0, \"LastPage\": 298.0, \"PaperType\": \"J\", \"Abstract\": \"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.\", \"AuthorNames-Deduped\": \"Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\", \"AuthorNames\": \"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang\", \"AuthorAffiliation\": \"The Ohio State University;Visa Research;The Ohio State University;Visa Research\", \"InternalReferences\": \"10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683\", \"AuthorKeywords\": \"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics\", \"AminerCitationCount\": 108.0, \"CitationCount_CrossRef\": 91.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2871.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 108.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution\", \"DOI\": \"10.1109/tvcg.2018.2864769\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864769\", \"FirstPage\": 374.0, \"LastPage\": 384.0, \"PaperType\": \"J\", \"Abstract\": \"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001\", \"AuthorNames\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins\", \"AuthorAffiliation\": \"Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning\", \"AminerCitationCount\": 47.0, \"CitationCount_CrossRef\": 40.0, \"PubsCited_CrossRef\": 69.0, \"Downloads_Xplore\": 1217.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 47.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning\", \"DOI\": \"10.1109/vast47406.2019.8986948\", \"Link\": \"http://dx.doi.org/10.1109/VAST47406.2019.8986948\", \"FirstPage\": 46.0, \"LastPage\": 56.0, \"PaperType\": \"C\", \"Abstract\": \"The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.\", \"AuthorNames-Deduped\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorNames\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718\", \"AuthorKeywords\": \"Machine learning fairness,visual analytics,intersectional bias,subgroup discovery\", \"AminerCitationCount\": 107.0, \"CitationCount_CrossRef\": 106.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 2108.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 107.0}, {\"Conference\": \"InfoVis\", \"Year\": 2019, \"Title\": \"Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements\", \"DOI\": \"10.1109/tvcg.2019.2934785\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934785\", \"FirstPage\": 906.0, \"LastPage\": 916.0, \"PaperType\": \"J\", \"Abstract\": \"Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Visualization for the masses,infographic,automatic visualization,presentation,and dissemination\", \"AminerCitationCount\": 79.0, \"CitationCount_CrossRef\": 71.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 2661.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 79.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections\", \"DOI\": \"10.1109/tvcg.2019.2934654\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934654\", \"FirstPage\": 1001.0, \"LastPage\": 1011.0, \"PaperType\": \"J\", \"Abstract\": \"We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen\", \"AuthorNames\": \"Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen\", \"AuthorAffiliation\": \"University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping\", \"AminerCitationCount\": 30.0, \"CitationCount_CrossRef\": 18.0, \"PubsCited_CrossRef\": 59.0, \"Downloads_Xplore\": 1300.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 30.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Calliope: Automatic Visual Data Story Generation from a Spreadsheet\", \"DOI\": \"10.1109/tvcg.2020.3030403\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030403\", \"FirstPage\": 453.0, \"LastPage\": 463.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.\", \"AuthorNames-Deduped\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\", \"AuthorNames\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Information Visualization,Visual Storytelling,Data Story\", \"AminerCitationCount\": 56.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 57.0, \"Downloads_Xplore\": 3724.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 56.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2020.3030467\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030467\", \"FirstPage\": 294.0, \"LastPage\": 303.0, \"PaperType\": \"J\", \"Abstract\": \"Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.\", \"AuthorNames-Deduped\": \"Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu\", \"AuthorNames\": \"Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu\", \"AuthorAffiliation\": \"Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University\", \"InternalReferences\": \"10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487\", \"AuthorKeywords\": \"Storyline visualization,reinforcement learning,mixed-initiative design\", \"AminerCitationCount\": 26.0, \"CitationCount_CrossRef\": 36.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 1931.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 26.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics\", \"DOI\": \"10.1109/tvcg.2020.3030448\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030448\", \"FirstPage\": 443.0, \"LastPage\": 452.0, \"PaperType\": \"J\", \"Abstract\": \"Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.\", \"AuthorNames-Deduped\": \"Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001\", \"AuthorNames\": \"Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810\", \"AuthorKeywords\": \"Infographics,automatic visualization\", \"AminerCitationCount\": 20.0, \"CitationCount_CrossRef\": 31.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 1004.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 20.0}, {\"Conference\": \"VAST\", \"Year\": 2020, \"Title\": \"VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations\", \"DOI\": \"10.1109/tvcg.2020.3030387\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030387\", \"FirstPage\": 495.0, \"LastPage\": 505.0, \"PaperType\": \"J\", \"Abstract\": \"Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.\", \"AuthorNames-Deduped\": \"Michael Oppermann;Robert Kincaid;Tamara Munzner\", \"AuthorNames\": \"Michael Oppermann;Robert Kincaid;Tamara Munzner\", \"AuthorAffiliation\": \"Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia\", \"InternalReferences\": \"10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757\", \"AuthorKeywords\": \"visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories\", \"AminerCitationCount\": 26.0, \"CitationCount_CrossRef\": 28.0, \"PubsCited_CrossRef\": 81.0, \"Downloads_Xplore\": 1243.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 26.0}, {\"Conference\": \"VAST\", \"Year\": 2020, \"Title\": \"Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering\", \"DOI\": \"10.1109/tvcg.2020.3030347\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030347\", \"FirstPage\": 1775.0, \"LastPage\": 1785.0, \"PaperType\": \"J\", \"Abstract\": \"We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.\", \"AuthorNames-Deduped\": \"Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia\", \"AuthorNames\": \"Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia\", \"AuthorAffiliation\": \"Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France\", \"InternalReferences\": \"10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477\", \"AuthorKeywords\": \"Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 17.0, \"PubsCited_CrossRef\": 58.0, \"Downloads_Xplore\": 754.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 13.0}, {\"Conference\": \"SciVis\", \"Year\": 2020, \"Title\": \"Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization\", \"DOI\": \"10.1109/tvcg.2020.3030407\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030407\", \"FirstPage\": 806.0, \"LastPage\": 816.0, \"PaperType\": \"J\", \"Abstract\": \"This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.\", \"AuthorNames-Deduped\": \"Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes\", \"AuthorNames\": \"Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes\", \"AuthorAffiliation\": \"Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz\", \"InternalReferences\": \"10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259\", \"AuthorKeywords\": \"Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 10.0, \"PubsCited_CrossRef\": 79.0, \"Downloads_Xplore\": 530.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 13.0}, {\"Conference\": \"SciVis\", \"Year\": 2020, \"Title\": \"IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection\", \"DOI\": \"10.1109/tvcg.2020.3030389\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030389\", \"FirstPage\": 775.0, \"LastPage\": 784.0, \"PaperType\": \"J\", \"Abstract\": \"Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.\", \"AuthorNames-Deduped\": \"Juraj P\\u00e1lenik;Thomas Spengler;Helwig Hauser\", \"AuthorNames\": \"Juraj Palenik;Thomas Spengler;Helwig Hauser\", \"AuthorAffiliation\": \"University of Bergen;University of Bergen;University of Bergen\", \"InternalReferences\": \"10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190\", \"AuthorKeywords\": \"visual parameter space exploration,scientific modelling,atmospheric convection\", \"AminerCitationCount\": 1.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 417.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation\", \"DOI\": \"10.1109/tvcg.2021.3114863\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114863\", \"FirstPage\": 195.0, \"LastPage\": 205.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.\", \"AuthorNames-Deduped\": \"Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu\", \"AuthorNames\": \"Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"Data visualization,Visualization recommendation,Knowledge graph\", \"AminerCitationCount\": 17.0, \"CitationCount_CrossRef\": 69.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 3452.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 17.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content\", \"DOI\": \"10.1109/tvcg.2021.3114770\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114770\", \"FirstPage\": 1073.0, \"LastPage\": 1083.0, \"PaperType\": \"J\", \"Abstract\": \"Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.\", \"AuthorNames-Deduped\": \"Alan Lundgard;Arvind Satyanarayan\", \"AuthorNames\": \"Alan Lundgard;Arvind Satyanarayan\", \"AuthorAffiliation\": \"MIT CSAIL, USA;MIT CSAIL, USA\", \"InternalReferences\": \"10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375\", \"AuthorKeywords\": \"Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability\", \"AminerCitationCount\": 24.0, \"CitationCount_CrossRef\": 62.0, \"PubsCited_CrossRef\": 108.0, \"Downloads_Xplore\": 2594.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 24.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Augmenting Sports Videos with VisCommentator\", \"DOI\": \"10.1109/tvcg.2021.3114806\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114806\", \"FirstPage\": 824.0, \"LastPage\": 834.0, \"PaperType\": \"J\", \"Abstract\": \"Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.\", \"AuthorNames-Deduped\": \"Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu\", \"AuthorNames\": \"Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu\", \"AuthorAffiliation\": \"Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458\", \"AuthorKeywords\": \"Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling\", \"AminerCitationCount\": 19.0, \"CitationCount_CrossRef\": 42.0, \"PubsCited_CrossRef\": 62.0, \"Downloads_Xplore\": 2151.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 19.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Kori: Interactive Synthesis of Text and Charts in Data Documents\", \"DOI\": \"10.1109/tvcg.2021.3114802\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114802\", \"FirstPage\": 184.0, \"LastPage\": 194.0, \"PaperType\": \"J\", \"Abstract\": \"Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.\", \"AuthorNames-Deduped\": \"Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim\", \"AuthorNames\": \"Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim\", \"AuthorAffiliation\": \"University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents\", \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 1308.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"VizLinter: A Linter and Fixer Framework for Data Visualization\", \"DOI\": \"10.1109/tvcg.2021.3114804\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114804\", \"FirstPage\": 206.0, \"LastPage\": 216.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.\", \"AuthorNames-Deduped\": \"Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001\", \"AuthorNames\": \"Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China\", \"InternalReferences\": \"10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166\", \"AuthorKeywords\": \"Visualization Linting,Automated Visualization Design,Visualization Optimization\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 32.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 1919.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation\", \"DOI\": \"10.1109/tvcg.2021.3114826\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114826\", \"FirstPage\": 162.0, \"LastPage\": 172.0, \"PaperType\": \"J\", \"Abstract\": \"We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.\", \"AuthorNames-Deduped\": \"Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001\", \"AuthorNames\": \"Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States\", \"InternalReferences\": \"10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423\", \"AuthorKeywords\": \"Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance\", \"AminerCitationCount\": 14.0, \"CitationCount_CrossRef\": 31.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 1788.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"An Evaluation-Focused Framework for Visualization Recommendation Algorithms\", \"DOI\": \"10.1109/tvcg.2021.3114814\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114814\", \"FirstPage\": 346.0, \"LastPage\": 356.0, \"PaperType\": \"J\", \"Abstract\": \"Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.\", \"AuthorNames-Deduped\": \"Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\", \"AuthorNames\": \"Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\", \"AuthorAffiliation\": \"University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Visualization Tools,Visualization Recommendation Algorithms\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 1106.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 13.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Towards Visual Explainable Active Learning for Zero-Shot Classification\", \"DOI\": \"10.1109/tvcg.2021.3114793\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114793\", \"FirstPage\": 791.0, \"LastPage\": 801.0, \"PaperType\": \"J\", \"Abstract\": \"Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.\", \"AuthorNames-Deduped\": \"Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang\", \"AuthorNames\": \"Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang\", \"AuthorAffiliation\": \"College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China\", \"InternalReferences\": \"10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818\", \"AuthorKeywords\": \"Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics\", \"AminerCitationCount\": 7.0, \"CitationCount_CrossRef\": 24.0, \"PubsCited_CrossRef\": 76.0, \"Downloads_Xplore\": 1775.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy\", \"DOI\": \"10.1109/tvcg.2021.3114810\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114810\", \"FirstPage\": 151.0, \"LastPage\": 161.0, \"PaperType\": \"J\", \"Abstract\": \"Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.\", \"AuthorNames-Deduped\": \"Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai\", \"AuthorNames\": \"Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai\", \"AuthorAffiliation\": \"University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA\", \"InternalReferences\": \"10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437\", \"AuthorKeywords\": \"Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 21.0, \"PubsCited_CrossRef\": 105.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs\", \"DOI\": \"10.1109/tvcg.2021.3114877\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114877\", \"FirstPage\": 400.0, \"LastPage\": 410.0, \"PaperType\": \"J\", \"Abstract\": \"Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.\", \"AuthorNames-Deduped\": \"Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu\", \"AuthorNames\": \"Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"Glyph-based visualization,machine learning,automatic visualization\", \"AminerCitationCount\": 10.0, \"CitationCount_CrossRef\": 19.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 1101.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 10.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks\", \"DOI\": \"10.1109/tvcg.2021.3114858\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114858\", \"FirstPage\": 813.0, \"LastPage\": 823.0, \"PaperType\": \"J\", \"Abstract\": \"Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \\u201cdog faces\\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \\u201cdog face\\u201d and \\u201cdog tail\\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.\", \"AuthorNames-Deduped\": \"Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau\", \"AuthorNames\": \"Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States\", \"InternalReferences\": \"10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509\", \"AuthorKeywords\": \"Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding\", \"AminerCitationCount\": 8.0, \"CitationCount_CrossRef\": 15.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 830.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics\", \"DOI\": \"10.1109/tvcg.2021.3114820\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114820\", \"FirstPage\": 486.0, \"LastPage\": 496.0, \"PaperType\": \"J\", \"Abstract\": \"There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.\", \"AuthorNames-Deduped\": \"Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall\", \"AuthorNames\": \"Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall\", \"AuthorAffiliation\": \"Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"transformers,word embeddings,literature review,web scraper,dataset,visual analytics\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 15.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 1087.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"A Mixed-Initiative Approach to Reusing Infographic Charts\", \"DOI\": \"10.1109/tvcg.2021.3114856\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114856\", \"FirstPage\": 173.0, \"LastPage\": 183.0, \"PaperType\": \"J\", \"Abstract\": \"Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China\", \"InternalReferences\": \"10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732\", \"AuthorKeywords\": \"Infographics,Reusable templates,Graphic design,Automatic visualization\", \"AminerCitationCount\": 4.0, \"CitationCount_CrossRef\": 13.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1211.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 4.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization\", \"DOI\": \"10.1109/tvcg.2021.3114782\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114782\", \"FirstPage\": 129.0, \"LastPage\": 139.0, \"PaperType\": \"J\", \"Abstract\": \"Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.\", \"AuthorNames-Deduped\": \"Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman\", \"AuthorNames\": \"Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Task-oriented insight preservation,responsive visualization\", \"AminerCitationCount\": 6.0, \"CitationCount_CrossRef\": 9.0, \"PubsCited_CrossRef\": 77.0, \"Downloads_Xplore\": 751.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 6.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Semantic Snapping for Guided Multi-View Visualization Design\", \"DOI\": \"10.1109/tvcg.2021.3114860\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114860\", \"FirstPage\": 43.0, \"LastPage\": 53.0, \"PaperType\": \"J\", \"Abstract\": \"Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \\u201caligned\\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.\", \"AuthorNames-Deduped\": \"Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner\", \"AuthorNames\": \"Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner\", \"AuthorAffiliation\": \"Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway\", \"InternalReferences\": \"10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338\", \"AuthorKeywords\": \"Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views\", \"AminerCitationCount\": 5.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 896.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 5.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Visualization Equilibrium\", \"DOI\": \"10.1109/tvcg.2021.3114842\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114842\", \"FirstPage\": 465.0, \"LastPage\": 474.0, \"PaperType\": \"J\", \"Abstract\": \"In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.\", \"AuthorNames-Deduped\": \"Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman\", \"AuthorNames\": \"Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA\", \"InternalReferences\": \"10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907\", \"AuthorKeywords\": \"Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 40.0, \"Downloads_Xplore\": 647.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization\", \"DOI\": \"10.1109/tvcg.2022.3209447\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209447\", \"FirstPage\": 331.0, \"LastPage\": 341.0, \"PaperType\": \"J\", \"Abstract\": \"Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.\", \"AuthorNames-Deduped\": \"Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu\", \"AuthorNames\": \"Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China\", \"InternalReferences\": \"10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254\", \"AuthorKeywords\": \"Glyph-based visualization,metaphor,machine learning,automatic visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 18.0, \"PubsCited_CrossRef\": 68.0, \"Downloads_Xplore\": 1095.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 18.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2022.3209468\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209468\", \"FirstPage\": 690.0, \"LastPage\": 700.0, \"PaperType\": \"J\", \"Abstract\": \"Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.\", \"AuthorNames-Deduped\": \"Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu\", \"AuthorNames\": \"Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234\", \"AuthorKeywords\": \"Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 14.0, \"PubsCited_CrossRef\": 83.0, \"Downloads_Xplore\": 1671.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning\", \"DOI\": \"10.1109/tvcg.2022.3209461\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209461\", \"FirstPage\": 95.0, \"LastPage\": 105.0, \"PaperType\": \"J\", \"Abstract\": \"Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.\", \"AuthorNames-Deduped\": \"Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001\", \"AuthorNames\": \"Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen\", \"AuthorAffiliation\": \"School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543\", \"AuthorKeywords\": \"Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 1276.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"MEDLEY: Intent-based Recommendations to Support Dashboard Composition\", \"DOI\": \"10.1109/tvcg.2022.3209421\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209421\", \"FirstPage\": 1135.0, \"LastPage\": 1145.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.\", \"AuthorNames-Deduped\": \"Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\", \"AuthorNames\": \"Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\", \"AuthorAffiliation\": \"Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826\", \"AuthorKeywords\": \"Dashboards,intent,recommendations,direct manipulation,multi-view coordination\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 1537.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"GenoREC: A Recommendation System for Interactive Genomics Data Visualization\", \"DOI\": \"10.1109/tvcg.2022.3209407\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209407\", \"FirstPage\": 570.0, \"LastPage\": 580.0, \"PaperType\": \"J\", \"Abstract\": \"Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.\", \"AuthorNames-Deduped\": \"Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg\", \"AuthorNames\": \"Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg\", \"AuthorAffiliation\": \"Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US\", \"InternalReferences\": \"10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234\", \"AuthorKeywords\": \"genomics,visualization,recommendation systems,data,tasks\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 62.0, \"Downloads_Xplore\": 2485.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback\", \"DOI\": \"10.1109/tvcg.2023.3327363\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327363\", \"FirstPage\": 131.0, \"LastPage\": 141.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.\", \"AuthorNames-Deduped\": \"Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh\", \"AuthorNames\": \"Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh\", \"AuthorAffiliation\": \"New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774\", \"AuthorKeywords\": \"Narrative visualization,visual storytelling,conversational agent\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 79.0, \"Downloads_Xplore\": 816.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks\", \"DOI\": \"10.1109/tvcg.2023.3327170\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327170\", \"FirstPage\": 944.0, \"LastPage\": 954.0, \"PaperType\": \"J\", \"Abstract\": \"Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.\", \"AuthorNames-Deduped\": \"Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu\", \"AuthorNames\": \"Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China\", \"InternalReferences\": \"0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668\", \"AuthorKeywords\": \"Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 58.0, \"Downloads_Xplore\": 665.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 5.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Mystique: Deconstructing SVG Charts for Layout Reuse\", \"DOI\": \"10.1109/tvcg.2023.3327354\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327354\", \"FirstPage\": 447.0, \"LastPage\": 457.0, \"PaperType\": \"J\", \"Abstract\": \"To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.\", \"AuthorNames-Deduped\": \"Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001\", \"AuthorNames\": \"Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu\", \"AuthorAffiliation\": \"University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States\", \"InternalReferences\": \"0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877\", \"AuthorKeywords\": \"Chart layout,Reuse,Reverse-engineering,Deconstruction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 481.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2023.3327200\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327200\", \"FirstPage\": 1172.0, \"LastPage\": 1182.0, \"PaperType\": \"J\", \"Abstract\": \"The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.\", \"AuthorNames-Deduped\": \"Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001\", \"AuthorNames\": \"Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China\", \"InternalReferences\": \"0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865\", \"AuthorKeywords\": \"Time Series Data,Exploratory Visual Analysis,Reinforcement Learning\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 77.0, \"Downloads_Xplore\": 1050.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining\", \"DOI\": \"10.1109/tvcg.2023.3326939\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326939\", \"FirstPage\": 1227.0, \"LastPage\": 1237.0, \"PaperType\": \"J\", \"Abstract\": \"Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.\", \"AuthorNames-Deduped\": \"Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai\", \"AuthorNames\": \"Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai\", \"AuthorAffiliation\": \"University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA\", \"InternalReferences\": \"0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475\", \"AuthorKeywords\": \"Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 82.0, \"Downloads_Xplore\": 361.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco\", \"DOI\": \"10.1109/tvcg.2023.3326527\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326527\", \"FirstPage\": 1063.0, \"LastPage\": 1073.0, \"PaperType\": \"J\", \"Abstract\": \"Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\\u2014a framework to model visualization knowledge\\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.\", \"AuthorNames-Deduped\": \"Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle\", \"AuthorNames\": \"Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle\", \"AuthorAffiliation\": \"University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA\", \"InternalReferences\": \"0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814\", \"AuthorKeywords\": \"Graphical Perception Studies,Visualization Recommendation Algorithms\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 51.0, \"Downloads_Xplore\": 371.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"The Rational Agent Benchmark for Data Visualization\", \"DOI\": \"10.1109/tvcg.2023.3326513\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326513\", \"FirstPage\": 338.0, \"LastPage\": 347.0, \"PaperType\": \"J\", \"Abstract\": \"Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.\", \"AuthorNames-Deduped\": \"Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman\", \"AuthorNames\": \"Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA\", \"InternalReferences\": \"0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781\", \"AuthorKeywords\": \"Evaluation,decision-making,rational agent,scoring rule\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 33.0, \"Downloads_Xplore\": 434.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams\", \"DOI\": \"10.1109/tvcg.2023.3326925\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326925\", \"FirstPage\": 562.0, \"LastPage\": 572.0, \"PaperType\": \"J\", \"Abstract\": \"Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.\", \"AuthorNames-Deduped\": \"Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001\", \"AuthorNames\": \"Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China\", \"InternalReferences\": \"0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428\", \"AuthorKeywords\": \"Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 78.0, \"Downloads_Xplore\": 662.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization\", \"DOI\": \"10.1109/tvcg.2023.3326583\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326583\", \"FirstPage\": 934.0, \"LastPage\": 943.0, \"PaperType\": \"J\", \"Abstract\": \"Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.\", \"AuthorNames-Deduped\": \"Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell\", \"AuthorNames\": \"Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell\", \"AuthorAffiliation\": \"Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423\", \"AuthorKeywords\": \"Visualization,responsive visualization,mixed-initiative authoring\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 45.0, \"Downloads_Xplore\": 330.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Visual Analytics for Understanding Draco's Knowledge Base\", \"DOI\": \"10.1109/tvcg.2023.3326912\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326912\", \"FirstPage\": 392.0, \"LastPage\": 402.0, \"PaperType\": \"J\", \"Abstract\": \"Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.\", \"AuthorNames-Deduped\": \"Johanna Schmidt;Bernhard Pointner;Silvia Miksch\", \"AuthorNames\": \"Johanna Schmidt;Bernhard Pointner;Silvia Miksch\", \"AuthorAffiliation\": \"VRVis Zentrum f\\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria\", \"InternalReferences\": \"0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146\", \"AuthorKeywords\": \"Visual Analytics,Hypergraph visualization,Rule-based recommendation systems\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 53.0, \"Downloads_Xplore\": 365.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Data Formulator: AI-Powered Concept-Driven Visualization Authoring\", \"DOI\": \"10.1109/tvcg.2023.3326585\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326585\", \"FirstPage\": 1128.0, \"LastPage\": 1138.0, \"PaperType\": \"J\", \"Abstract\": \"With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.\", \"AuthorNames-Deduped\": \"Chenglong Wang;John Thompson 0002;Bongshin Lee\", \"AuthorNames\": \"Chenglong Wang;John Thompson;Bongshin Lee\", \"AuthorAffiliation\": \"Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369\", \"AuthorKeywords\": \"AI,visualization authoring,data transformation,programming by example,natural language,large language model\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 63.0, \"Downloads_Xplore\": 893.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Guided Visual Analytics for Image Selection in Time and Space\", \"DOI\": \"10.1109/tvcg.2023.3326572\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326572\", \"FirstPage\": 66.0, \"LastPage\": 75.0, \"PaperType\": \"J\", \"Abstract\": \"Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.\", \"AuthorNames-Deduped\": \"Ignacio P\\u00e9rez-Messina;Davide Ceneda;Silvia Miksch\", \"AuthorNames\": \"Ignacio P\\u00e9rez-Messina;Davide Ceneda;Silvia Miksch\", \"AuthorAffiliation\": \"TU Wien, Austria;TU Wien, Austria;TU Wien, Austria\", \"InternalReferences\": \"0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146\", \"AuthorKeywords\": \"Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 37.0, \"Downloads_Xplore\": 1208.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts\", \"DOI\": \"10.1109/tvcg.2024.3456398\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456398\", \"FirstPage\": 481.0, \"LastPage\": 491.0, \"PaperType\": \"J\", \"Abstract\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\", \"AuthorNames-Deduped\": \"Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma\", \"AuthorNames\": \"Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma\", \"AuthorAffiliation\": \"University of California, USA;University of California, USA;University of California, USA;University of California, USA\", \"InternalReferences\": \"10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878\", \"AuthorKeywords\": \"Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 65.0, \"Downloads_Xplore\": 386.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration\", \"DOI\": \"10.1109/tvcg.2024.3456364\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456364\", \"FirstPage\": 547.0, \"LastPage\": 557.0, \"PaperType\": \"J\", \"Abstract\": \"The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.\", \"AuthorNames-Deduped\": \"Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\", \"AuthorNames\": \"Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\", \"AuthorAffiliation\": \"Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA\", \"InternalReferences\": \"10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798\", \"AuthorKeywords\": \"Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 632.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"VisEval: A Benchmark for Data Visualization in the Era of Large Language Models\", \"DOI\": \"10.1109/tvcg.2024.3456320\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456320\", \"FirstPage\": 1301.0, \"LastPage\": 1311.0, \"PaperType\": \"J\", \"Abstract\": \"Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.\", \"AuthorNames-Deduped\": \"Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001\", \"AuthorNames\": \"Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang\", \"AuthorAffiliation\": \"Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668\", \"AuthorKeywords\": \"Visualization evaluation,automatic visualization,,,large language models,benchmark\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 75.0, \"Downloads_Xplore\": 625.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech\", \"DOI\": \"10.1109/tvcg.2024.3456358\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456358\", \"FirstPage\": 864.0, \"LastPage\": 874.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.\", \"AuthorNames-Deduped\": \"Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\", \"AuthorNames\": \"Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\", \"AuthorAffiliation\": \"Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia\", \"InternalReferences\": \"10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275\", \"AuthorKeywords\": \"Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 184.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"DracoGPT: Extracting Visualization Design Preferences from Large Language Models\", \"DOI\": \"10.1109/tvcg.2024.3456350\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456350\", \"FirstPage\": 710.0, \"LastPage\": 720.0, \"PaperType\": \"J\", \"Abstract\": \"Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\\u2014DracoGPT-Rank and DracoGPT-Recommend\\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.\", \"AuthorNames-Deduped\": \"Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer\", \"AuthorNames\": \"Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA\", \"InternalReferences\": \"10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527\", \"AuthorKeywords\": \"Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 45.0, \"Downloads_Xplore\": 378.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Smartboard: Visual Exploration of Team Tactics with LLM Agent\", \"DOI\": \"10.1109/tvcg.2024.3456200\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456200\", \"FirstPage\": 23.0, \"LastPage\": 33.0, \"PaperType\": \"J\", \"Abstract\": \"Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.\", \"AuthorNames-Deduped\": \"Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu\", \"AuthorNames\": \"Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu\", \"AuthorAffiliation\": \"Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806\", \"AuthorKeywords\": \"Sports visualization,tactic board,,,tactical analysis\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 813.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations\", \"DOI\": \"10.1109/tvcg.2024.3456182\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456182\", \"FirstPage\": 754.0, \"LastPage\": 764.0, \"PaperType\": \"J\", \"Abstract\": \"People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6\", \"AuthorNames-Deduped\": \"Ratanond Koonchanok;Michael E. Papka;Khairi Reda\", \"AuthorNames\": \"Ratanond Koonchanok;Michael E. Papka;Khairi Reda\", \"AuthorAffiliation\": \"Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA\", \"InternalReferences\": \"10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513\", \"AuthorKeywords\": \"Visual inference,statistical rationality,,,human-machine collaboration\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 71.0, \"Downloads_Xplore\": 138.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_bcb7f009\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nDataset path used: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nPapers considered (AutoVis-identified or full if none matched): 80\nAward-flagged papers in this set: 10\n### End of facts\n### Begin of facts\nCitation metric coverage and distribution (AutoVis subset):\nAminerCitationCount \u2014 nonzero: 53/80 (66.25%), mean=34.8, median=9.5, p90=109, p95=130, p99=280, max=487\nCitationCount_CrossRef \u2014 nonzero: 72/80 (90.00%), mean=30.8, median=14.0, p90=81, p95=107, p99=201, max=292\n### End of facts\n### Begin of facts\nCorrelation between Aminer and CrossRef citation counts (Pearson): 0.946\nCorrelation between CitationCount_CrossRef and Downloads_Xplore: 0.814\nInterpretation: higher correlation indicates metrics track each other; low CrossRef nonzero fraction suggests missingness for older/aggregate data.\n### End of facts\n### Begin of facts\nRecommendation for selection rule:\nInclude all Award-flagged papers regardless of citation counts PLUS top papers by a primary citation metric. Choose the metric with better coverage; here we choose: CitationCount_CrossRef\nSuggested numeric thresholds (based on distribution): top 1% >= 201 CitationCount_CrossRef, top 5% >= 106 CitationCount_CrossRef, top 10% >= 81 CitationCount_CrossRef.\nFor a compact milestone table of ~20 papers: include all awards (10) + fill remaining slots with top papers by CitationCount_CrossRef.\nAlternative: use combined score (normalized citations + normalized downloads) to boost recent high-download works.\n### End of facts\n### Begin of facts\nCandidate milestone list (top 20):\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 CitationCount_CrossRef=292, CrossRef=292, Downloads=4307, Award=NO; Authors=Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 CitationCount_CrossRef=177, CrossRef=177, Downloads=3238, Award=YES; Authors=Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\nA Design Space of Visualization Tasks (2013) \u2014 CitationCount_CrossRef=144, CrossRef=144, Downloads=4884, Award=NO; Authors=Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 CitationCount_CrossRef=121, CrossRef=121, Downloads=2942, Award=NO; Authors=Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 CitationCount_CrossRef=106, CrossRef=106, Downloads=2108, Award=NO; Authors=\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\nFinding Waldo: Learning about Users from their Interactions (2014) \u2014 CitationCount_CrossRef=95, CrossRef=95, Downloads=2226, Award=NO; Authors=Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\nData-Driven Guides: Supporting Expressive Design for Information Graphics (2016) \u2014 CitationCount_CrossRef=92, CrossRef=92, Downloads=2245, Award=NO; Authors=Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\nDQNViz: A Visual Analytics Approach to Understand Deep Q-Networks (2018) \u2014 CitationCount_CrossRef=91, CrossRef=91, Downloads=2871, Award=YES; Authors=Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 CitationCount_CrossRef=80, CrossRef=80, Downloads=3724, Award=NO; Authors=Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\nLearning Perceptual Kernels for Visualization Design (2014) \u2014 CitationCount_CrossRef=80, CrossRef=80, Downloads=1247, Award=NO; Authors=\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\nText-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements (2019) \u2014 CitationCount_CrossRef=71, CrossRef=71, Downloads=2661, Award=NO; Authors=Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\nWarning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics (2017) \u2014 CitationCount_CrossRef=70, CrossRef=70, Downloads=1801, Award=NO; Authors=Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\nKG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation (2021) \u2014 CitationCount_CrossRef=69, CrossRef=69, Downloads=3452, Award=YES; Authors=Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu\nAugmenting Sports Videos with VisCommentator (2021) \u2014 CitationCount_CrossRef=42, CrossRef=42, Downloads=2151, Award=YES; Authors=Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu\nAn Evaluation-Focused Framework for Visualization Recommendation Algorithms (2021) \u2014 CitationCount_CrossRef=25, CrossRef=25, Downloads=1106, Award=YES; Authors=Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\nMEDLEY: Intent-based Recommendations to Support Dashboard Composition (2022) \u2014 CitationCount_CrossRef=8, CrossRef=8, Downloads=1537, Award=YES; Authors=Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\nVisEval: A Benchmark for Data Visualization in the Era of Large Language Models (2024) \u2014 CitationCount_CrossRef=1, CrossRef=1, Downloads=625, Award=YES; Authors=Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001\nKNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration (2024) \u2014 CitationCount_CrossRef=1, CrossRef=1, Downloads=632, Award=YES; Authors=Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\nWhen Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis wi (2024) \u2014 CitationCount_CrossRef=0, CrossRef=0, Downloads=184, Award=YES; Authors=Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\nData Formulator: AI-Powered Concept-Driven Visualization Authoring (2023) \u2014 CitationCount_CrossRef=0, CrossRef=0, Downloads=893, Award=YES; Authors=Chenglong Wang;John Thompson 0002;Bongshin Lee\n### End of facts\n### Begin of facts\nTop 10 papers by CitationCount_CrossRef:\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 CitationCount_CrossRef=292, Award=NO\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 CitationCount_CrossRef=177, Award=YES\nA Design Space of Visualization Tasks (2013) \u2014 CitationCount_CrossRef=144, Award=NO\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 CitationCount_CrossRef=121, Award=NO\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 CitationCount_CrossRef=106, Award=NO\nFinding Waldo: Learning about Users from their Interactions (2014) \u2014 CitationCount_CrossRef=95, Award=NO\nData-Driven Guides: Supporting Expressive Design for Information Graphics (2016) \u2014 CitationCount_CrossRef=92, Award=NO\nDQNViz: A Visual Analytics Approach to Understand Deep Q-Networks (2018) \u2014 CitationCount_CrossRef=91, Award=YES\nLearning Perceptual Kernels for Visualization Design (2014) \u2014 CitationCount_CrossRef=80, Award=NO\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 CitationCount_CrossRef=80, Award=NO\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                }
            ],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "Counting AutoVis papers year-by-year reveals when the field surged and where attention concentrated. Activity is intermittent before 2014, becomes clearly sustained after 2014, and accelerates through 2020\u20132023 with a one-year high in 2021. Moving averages smooth short-term volatility and show a multi-year rise that peaks in the early 2020s; the largest one-year numeric drop is the fall from 16 papers in 2021 to 5 in 2022, but the multi-year trend (3-year MA) remains at historic highs in 2023, suggesting continued strong interest rather than a collapse. A caveat: because this analysis starts from an AutoVis-filtered subset, \u201cshare\u201d measures in several plots read as 100% for many years \u2014 these charts therefore reflect activity within identified AutoVis work rather than proportion of all Vis papers in every year."
                },
                {
                    "id": 1,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_3875c1e9\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"layer\": [{\"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"Category\", \"scale\": {\"range\": [\"#1f77b4\", \"#ff7f0e\"]}, \"title\": \"Category\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"Category\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"count\", \"stack\": \"zero\", \"title\": \"Number of papers\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"line\", \"size\": 2}, \"encoding\": {\"color\": {\"field\": \"Category\", \"legend\": null, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"Category\", \"type\": \"nominal\"}, {\"field\": \"count_ma\", \"title\": \"Count (3-yr MA)\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"type\": \"ordinal\"}, \"y\": {\"field\": \"count_ma\", \"title\": \"3-year moving average\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9b182fc48e01b05d5f3ed9e566204315\"}, \"height\": 320, \"title\": \"Yearly paper counts: AutoVis vs Other (with 3-year MA)\", \"width\": 700}, {\"layer\": [{\"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"tooltip\": [{\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AutoVis_share\", \"title\": \"AutoVis share\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"title\": \"Year\", \"type\": \"ordinal\"}, \"y\": {\"axis\": {\"format\": \"%\"}, \"field\": \"AutoVis_share\", \"title\": \"Share of AutoVis papers\", \"type\": \"quantitative\"}}}, {\"mark\": {\"type\": \"line\", \"color\": \"black\", \"strokeDash\": [6, 6]}, \"encoding\": {\"x\": {\"field\": \"Year\", \"sort\": [1995, 2004, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024], \"type\": \"ordinal\"}, \"y\": {\"field\": \"AutoVis_share_ma\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-aab9ced04171000f2c5bc07b52840cf0\"}, \"height\": 160, \"title\": \"AutoVis share of total papers (with 3-year MA)\", \"width\": 700}], \"resolve\": {\"scale\": {\"color\": \"independent\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-9b182fc48e01b05d5f3ed9e566204315\": [{\"Year\": 1995, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 2.0}, {\"Year\": 1995, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2004, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 2.0}, {\"Year\": 2004, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2006, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 1.3333333333333333}, {\"Year\": 2006, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2007, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.6666666666666666}, {\"Year\": 2007, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.6666666666666666}, {\"Year\": 2008, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.0}, {\"Year\": 2008, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 1.0}, {\"Year\": 2009, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 0.6666666666666666}, {\"Year\": 2009, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2010, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2010, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 0.6666666666666666}, {\"Year\": 2011, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2011, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2012, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 0.3333333333333333}, {\"Year\": 2012, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 1.0}, {\"Year\": 2013, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 0.3333333333333333}, {\"Year\": 2013, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 1.0}, {\"Year\": 2014, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2014, \"Category\": \"AutoVis\", \"count\": 2, \"count_ma\": 1.0}, {\"Year\": 2015, \"Category\": \"AutoVis\", \"count\": 4, \"count_ma\": 2.0}, {\"Year\": 2015, \"Category\": \"Other\", \"count\": 0, \"count_ma\": 1.0}, {\"Year\": 2016, \"Category\": \"AutoVis\", \"count\": 3, \"count_ma\": 3.0}, {\"Year\": 2016, \"Category\": \"Other\", \"count\": 3, \"count_ma\": 1.6666666666666667}, {\"Year\": 2017, \"Category\": \"AutoVis\", \"count\": 0, \"count_ma\": 2.3333333333333335}, {\"Year\": 2017, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.6666666666666667}, {\"Year\": 2018, \"Category\": \"AutoVis\", \"count\": 3, \"count_ma\": 2.0}, {\"Year\": 2018, \"Category\": \"Other\", \"count\": 1, \"count_ma\": 2.0}, {\"Year\": 2019, \"Category\": \"Other\", \"count\": 2, \"count_ma\": 1.6666666666666667}, {\"Year\": 2019, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 1.3333333333333333}, {\"Year\": 2020, \"Category\": \"AutoVis\", \"count\": 4, \"count_ma\": 2.6666666666666665}, {\"Year\": 2020, \"Category\": \"Other\", \"count\": 3, \"count_ma\": 2.0}, {\"Year\": 2021, \"Category\": \"AutoVis\", \"count\": 9, \"count_ma\": 4.666666666666667}, {\"Year\": 2021, \"Category\": \"Other\", \"count\": 7, \"count_ma\": 4.0}, {\"Year\": 2022, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 4.666666666666667}, {\"Year\": 2022, \"Category\": \"Other\", \"count\": 4, \"count_ma\": 4.666666666666667}, {\"Year\": 2023, \"Category\": \"AutoVis\", \"count\": 6, \"count_ma\": 5.333333333333333}, {\"Year\": 2023, \"Category\": \"Other\", \"count\": 6, \"count_ma\": 5.666666666666667}, {\"Year\": 2024, \"Category\": \"AutoVis\", \"count\": 1, \"count_ma\": 2.6666666666666665}, {\"Year\": 2024, \"Category\": \"Other\", \"count\": 6, \"count_ma\": 5.333333333333333}], \"data-aab9ced04171000f2c5bc07b52840cf0\": [{\"Year\": 1995, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2004, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2006, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2007, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2008, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2009, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2010, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2011, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2012, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2013, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2014, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2015, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2016, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2017, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2018, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2019, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2020, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2021, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2022, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2023, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}, {\"Year\": 2024, \"AutoVis_count\": null, \"Other_count\": null, \"total\": null, \"AutoVis_share\": null, \"AutoVis_share_ma\": null, \"AutoVis_count_ma\": null}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_3875c1e9\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nTime span analyzed: 1995\u20132024 | Total papers: 83 | AutoVis papers: 83 | Overall AutoVis share: 1.000\n### End of facts\n### Begin of facts\nPeak AutoVis count: 16 papers in 2021\nPeak AutoVis share (of vis papers): 100.000% in 1995\n### End of facts\n### Begin of facts\n3-year moving average peak (AutoVis count): 11.00 in 2023\nTop 5 years by AutoVis count: 2021:16; 2023:12; 2024:7; 2020:7; 2016:6\n### End of facts\n### Begin of facts\nLargest one-year change in AutoVis count: -11 (year 2022, previous year 2021)\nLargest one-year drop in AutoVis share: 0.000% (year 1995)\n### End of facts\n### Begin of facts\nYearly summary (first 5 rows):\nCategory  AutoVis  Other  Total  ShareOfTotal  AutoVis_MA3  Share_MA3\nYear                                                                 \n1995            2      0      2           1.0     2.000000        1.0\n2004            2      0      2           1.0     2.000000        1.0\n2006            1      0      1           1.0     1.666667        1.0\n2007            1      0      1           1.0     1.333333        1.0\n2008            1      0      1           1.0     1.000000        1.0\n### End of facts\n### Begin of facts\nYearly summary (last 5 rows):\nCategory  AutoVis  Other  Total  ShareOfTotal  AutoVis_MA3  Share_MA3\nYear                                                                 \n2020            7      0      7           1.0     4.666667        1.0\n2021           16      0     16           1.0     8.666667        1.0\n2022            5      0      5           1.0     9.333333        1.0\n2023           12      0     12           1.0    11.000000        1.0\n2024            7      0      7           1.0     8.000000        1.0\n### End of facts\n",
                    "text": "The temporal-counts plot with a 3\u2011year moving average highlights the field\u2019s rise and short-term variability: AutoVis presence is episodic in the 1990s\u20132000s, climbs after 2014 (Voyager and related systems appear mid\u2011decade), and shows two strong waves around 2015\u20132016 and 2020\u20132023. Peak single-year production is 16 AutoVis papers in 2021; the 3\u2011year moving average reaches its maximum in 2023 (about 11 papers), reflecting both a concentration of recent work and a continuing pipeline of submissions. The variability (large year-to-year swings) suggests that interest is responsive to new methods and high-impact demonstrations (e.g., Voyager, Draco, NLG and fairness systems) rather than settled into a slow, steady growth."
                },
                {
                    "id": 2,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_bcb7f009\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"vconcat\": [{\"data\": {\"name\": \"data-97b427d34fd148fb396950ae148f8bb7\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"award_flag\", \"scale\": {\"domain\": [false, true], \"range\": [\"steelblue\", \"orange\"]}, \"title\": \"Award Winner\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"Title\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AuthorNames-Deduped\", \"title\": \"Authors\", \"type\": \"nominal\"}, {\"field\": \"citation_pref\", \"title\": \"Preferred citations\", \"type\": \"quantitative\"}, {\"field\": \"AminerCitationCount\", \"type\": \"quantitative\"}, {\"field\": \"CitationCount_CrossRef\", \"type\": \"quantitative\"}, {\"field\": \"Downloads_Xplore\", \"type\": \"quantitative\"}, {\"field\": \"Award\", \"type\": \"nominal\"}], \"x\": {\"field\": \"citation_pref\", \"title\": \"Preferred citation count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Title\", \"sort\": {\"field\": \"citation_pref\", \"order\": \"descending\"}, \"title\": \"Paper Title\", \"type\": \"nominal\"}}, \"height\": 600, \"title\": \"Top 20 candidate milestone AutoVis papers (by preferred citation metric)\", \"width\": 800}, {\"data\": {\"name\": \"data-54c313becad736b6a64fe20de391da14\"}, \"mark\": {\"type\": \"circle\"}, \"encoding\": {\"color\": {\"field\": \"award_flag\", \"scale\": {\"domain\": [false, true], \"range\": [\"steelblue\", \"orange\"]}, \"title\": \"Award Winner\", \"type\": \"nominal\"}, \"size\": {\"field\": \"Downloads_Xplore\", \"scale\": {\"range\": [10, 400]}, \"title\": \"Downloads (Xplore)\", \"type\": \"quantitative\"}, \"tooltip\": [{\"field\": \"Title\", \"type\": \"nominal\"}, {\"field\": \"Year\", \"type\": \"ordinal\"}, {\"field\": \"AuthorNames-Deduped\", \"title\": \"Authors\", \"type\": \"nominal\"}, {\"field\": \"AminerCitationCount\", \"type\": \"quantitative\"}, {\"field\": \"CitationCount_CrossRef\", \"type\": \"quantitative\"}, {\"field\": \"Downloads_Xplore\", \"type\": \"quantitative\"}, {\"field\": \"Award\", \"type\": \"nominal\"}], \"x\": {\"field\": \"AminerCitationCount\", \"title\": \"AminerCitationCount\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"CitationCount_CrossRef\", \"title\": \"CitationCount (CrossRef)\", \"type\": \"quantitative\"}}, \"height\": 320, \"name\": \"view_1\", \"title\": \"Aminer vs CrossRef citations (point size = downloads)\", \"width\": 800}], \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\", \"views\": [\"view_1\"]}], \"resolve\": {\"scale\": {\"color\": \"shared\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-97b427d34fd148fb396950ae148f8bb7\": [{\"Conference\": \"InfoVis\", \"Year\": 2015, \"Title\": \"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations\", \"DOI\": \"10.1109/tvcg.2015.2467191\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2015.2467191\", \"FirstPage\": 649.0, \"LastPage\": 658.0, \"PaperType\": \"J\", \"Abstract\": \"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.\", \"AuthorNames-Deduped\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297\", \"AuthorKeywords\": \"User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems\", \"AminerCitationCount\": 487.0, \"CitationCount_CrossRef\": 292.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 4307.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 487.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco\", \"DOI\": \"10.1109/tvcg.2018.2865240\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865240\", \"FirstPage\": 438.0, \"LastPage\": 448.0, \"PaperType\": \"J\", \"Abstract\": \"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.\", \"AuthorNames-Deduped\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming\", \"AminerCitationCount\": 225.0, \"CitationCount_CrossRef\": 177.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 3238.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 225.0}, {\"Conference\": \"InfoVis\", \"Year\": 2013, \"Title\": \"A Design Space of Visualization Tasks\", \"DOI\": \"10.1109/tvcg.2013.120\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2013.120\", \"FirstPage\": 2366.0, \"LastPage\": 2375.0, \"PaperType\": \"J\", \"Abstract\": \"Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.\", \"AuthorNames-Deduped\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorNames\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorAffiliation\": \"University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany\", \"InternalReferences\": \"10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213\", \"AuthorKeywords\": \"Task taxonomy, design space, climate impact research, visualization recommendation\", \"AminerCitationCount\": 217.0, \"CitationCount_CrossRef\": 144.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 4884.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 217.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Finding Waldo: Learning about Users from their Interactions\", \"DOI\": \"10.1109/tvcg.2014.2346575\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346575\", \"FirstPage\": 1663.0, \"LastPage\": 1672.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.\", \"AuthorNames-Deduped\": \"Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorNames\": \"Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorAffiliation\": \"Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U\", \"InternalReferences\": \"10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204\", \"AuthorKeywords\": \"User Interactions, Analytic Provenance, Visualization, Applied Machine Learning\", \"AminerCitationCount\": 145.0, \"CitationCount_CrossRef\": 95.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 2226.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 145.0}, {\"Conference\": \"InfoVis\", \"Year\": 2014, \"Title\": \"Learning Perceptual Kernels for Visualization Design\", \"DOI\": \"10.1109/tvcg.2014.2346978\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346978\", \"FirstPage\": 1933.0, \"LastPage\": 1942.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.\", \"AuthorNames-Deduped\": \"\\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorNames\": \"\\u00c7a\\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorAffiliation\": \"Stanford University;Stanford University;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186\", \"AuthorKeywords\": \"Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding\", \"AminerCitationCount\": 129.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 1247.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 129.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication\", \"DOI\": \"10.1109/tvcg.2018.2865145\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865145\", \"FirstPage\": 672.0, \"LastPage\": 681.0, \"PaperType\": \"J\", \"Abstract\": \"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.\", \"AuthorNames-Deduped\": \"Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\", \"AuthorNames\": \"Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication\", \"AminerCitationCount\": 120.0, \"CitationCount_CrossRef\": 121.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 2942.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 120.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics\", \"DOI\": \"10.1109/vast.2017.8585669\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2017.8585669\", \"FirstPage\": 104.0, \"LastPage\": 115.0, \"PaperType\": \"C\", \"Abstract\": \"Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.\", \"AuthorNames-Deduped\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorNames\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorAffiliation\": \"Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General\", \"AminerCitationCount\": 115.0, \"CitationCount_CrossRef\": 70.0, \"PubsCited_CrossRef\": 80.0, \"Downloads_Xplore\": 1801.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 115.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Data-Driven Guides: Supporting Expressive Design for Information Graphics\", \"DOI\": \"10.1109/tvcg.2016.2598620\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598620\", \"FirstPage\": 491.0, \"LastPage\": 500.0, \"PaperType\": \"J\", \"Abstract\": \"In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.\", \"AuthorNames-Deduped\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorNames\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorAffiliation\": \"John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Information graphics;visualization;design tools;2D graphics\", \"AminerCitationCount\": 114.0, \"CitationCount_CrossRef\": 92.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2245.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 114.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks\", \"DOI\": \"10.1109/tvcg.2018.2864504\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864504\", \"FirstPage\": 288.0, \"LastPage\": 298.0, \"PaperType\": \"J\", \"Abstract\": \"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.\", \"AuthorNames-Deduped\": \"Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\", \"AuthorNames\": \"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang\", \"AuthorAffiliation\": \"The Ohio State University;Visa Research;The Ohio State University;Visa Research\", \"InternalReferences\": \"10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683\", \"AuthorKeywords\": \"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics\", \"AminerCitationCount\": 108.0, \"CitationCount_CrossRef\": 91.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2871.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 108.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning\", \"DOI\": \"10.1109/vast47406.2019.8986948\", \"Link\": \"http://dx.doi.org/10.1109/VAST47406.2019.8986948\", \"FirstPage\": 46.0, \"LastPage\": 56.0, \"PaperType\": \"C\", \"Abstract\": \"The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.\", \"AuthorNames-Deduped\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorNames\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718\", \"AuthorKeywords\": \"Machine learning fairness,visual analytics,intersectional bias,subgroup discovery\", \"AminerCitationCount\": 107.0, \"CitationCount_CrossRef\": 106.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 2108.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 107.0}, {\"Conference\": \"Vis\", \"Year\": 2007, \"Title\": \"Interactive Visual Analysis of Perfusion Data\", \"DOI\": \"10.1109/tvcg.2007.70569\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2007.70569\", \"FirstPage\": 1392.0, \"LastPage\": 1399.0, \"PaperType\": \"J\", \"Abstract\": \"Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorNames\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorAffiliation\": \"Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany\", \"InternalReferences\": \"10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739\", \"AuthorKeywords\": \"Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis\", \"AminerCitationCount\": 100.0, \"CitationCount_CrossRef\": 44.0, \"PubsCited_CrossRef\": 28.0, \"Downloads_Xplore\": 666.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 100.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration\", \"DOI\": \"10.1109/tvcg.2016.2598839\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598839\", \"FirstPage\": 331.0, \"LastPage\": 340.0, \"PaperType\": \"J\", \"Abstract\": \"Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.\", \"AuthorNames-Deduped\": \"Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Visual Data Exploration;Visualization by Demonstration;Visualization Tools\", \"AminerCitationCount\": 83.0, \"CitationCount_CrossRef\": 57.0, \"PubsCited_CrossRef\": 35.0, \"Downloads_Xplore\": 2781.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 83.0}, {\"Conference\": \"InfoVis\", \"Year\": 2019, \"Title\": \"Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements\", \"DOI\": \"10.1109/tvcg.2019.2934785\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934785\", \"FirstPage\": 906.0, \"LastPage\": 916.0, \"PaperType\": \"J\", \"Abstract\": \"Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Visualization for the masses,infographic,automatic visualization,presentation,and dissemination\", \"AminerCitationCount\": 79.0, \"CitationCount_CrossRef\": 71.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 2661.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 79.0}, {\"Conference\": \"InfoVis\", \"Year\": 2008, \"Title\": \"Multi-Focused Geospatial Analysis Using Probes\", \"DOI\": \"10.1109/tvcg.2008.149\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2008.149\", \"FirstPage\": 1165.0, \"LastPage\": 1172.0, \"PaperType\": \"J\", \"Abstract\": \"Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.\", \"AuthorNames-Deduped\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorNames\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorAffiliation\": \"UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center\", \"InternalReferences\": \"10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102\", \"AuthorKeywords\": \"Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes\", \"AminerCitationCount\": 73.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 20.0, \"Downloads_Xplore\": 648.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 73.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Calliope: Automatic Visual Data Story Generation from a Spreadsheet\", \"DOI\": \"10.1109/tvcg.2020.3030403\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030403\", \"FirstPage\": 453.0, \"LastPage\": 463.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.\", \"AuthorNames-Deduped\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\", \"AuthorNames\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Information Visualization,Visual Storytelling,Data Story\", \"AminerCitationCount\": 56.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 57.0, \"Downloads_Xplore\": 3724.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 56.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Podium: Ranking Data Using Mixed-Initiative Visual Analytics\", \"DOI\": \"10.1109/tvcg.2017.2745078\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2017.2745078\", \"FirstPage\": 288.0, \"LastPage\": 297.0, \"PaperType\": \"J\", \"Abstract\": \"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.\", \"AuthorNames-Deduped\": \"Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Mixed-initiative visual analytics,multi-attribute ranking,user interaction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 52.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1535.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 52.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution\", \"DOI\": \"10.1109/tvcg.2018.2864769\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864769\", \"FirstPage\": 374.0, \"LastPage\": 384.0, \"PaperType\": \"J\", \"Abstract\": \"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001\", \"AuthorNames\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins\", \"AuthorAffiliation\": \"Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning\", \"AminerCitationCount\": 47.0, \"CitationCount_CrossRef\": 40.0, \"PubsCited_CrossRef\": 69.0, \"Downloads_Xplore\": 1217.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 47.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods\", \"DOI\": \"10.1109/tvcg.2016.2598544\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598544\", \"FirstPage\": 271.0, \"LastPage\": 280.0, \"PaperType\": \"J\", \"Abstract\": \"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.\", \"AuthorNames-Deduped\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne\", \"AuthorNames\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory\", \"InternalReferences\": \"10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591\", \"AuthorKeywords\": \"trust;transparency;familiarity;uncertainty;biological data analysis\", \"AminerCitationCount\": 41.0, \"CitationCount_CrossRef\": 41.0, \"PubsCited_CrossRef\": 41.0, \"Downloads_Xplore\": 1844.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 41.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations\", \"DOI\": \"10.1109/tvcg.2016.2598543\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598543\", \"FirstPage\": 261.0, \"LastPage\": 270.0, \"PaperType\": \"J\", \"Abstract\": \"User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.\", \"AuthorNames-Deduped\": \"Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorNames\": \"Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorAffiliation\": \"Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research\", \"InternalReferences\": \"10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878\", \"AuthorKeywords\": \"Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization\", \"AminerCitationCount\": 39.0, \"CitationCount_CrossRef\": 33.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 2188.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 39.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Mixed-initiative visual analytics using task-driven recommendations\", \"DOI\": \"10.1109/vast.2015.7347625\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347625\", \"FirstPage\": 9.0, \"LastPage\": 16.0, \"PaperType\": \"C\", \"Abstract\": \"Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.\", \"AuthorNames-Deduped\": \"Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorNames\": \"Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"mixed-initiative visual analytics, task modeling, recommender systems, sensemaking\", \"AminerCitationCount\": 36.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 36.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 36.0}], \"data-54c313becad736b6a64fe20de391da14\": [{\"Conference\": \"InfoVis\", \"Year\": 1995, \"Title\": \"Towards a generative theory of diagram design\", \"DOI\": \"10.1109/infvis.1995.528681\", \"Link\": \"http://dx.doi.org/10.1109/INFVIS.1995.528681\", \"FirstPage\": 11.0, \"LastPage\": 18.0, \"PaperType\": \"C\", \"Abstract\": \"We describe the theoretical background for AVE, an automatic visualization engine for semantic networks. We have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information visualization. This implies that the diagrams should communicate the characteristics of the data as effectively as possible. In this generative theory of diagram design, we include data characterization, systematic use of graphical means of expression and the combination of graphical means of expression. After giving a brief introduction and an application scenario we discuss these aspects in detail. Finally, a process model of an automatic visualization process is sketched and directions for further research are outlined.\", \"AuthorNames-Deduped\": \"Klaus Reichenberger;Thomas Kamps;Gene Golovchinsky\", \"AuthorNames\": \"K. Reichenberger;T. Kamps;G. Golovchinsky\", \"AuthorAffiliation\": \"Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Information Systems, GMD-Institute for Integrated Publication, Darmstadt, Germany;Department of Industrial Engiheering, University of Toronto, Toronto, ONT, Canada\", \"InternalReferences\": \"10.1109/visual.1995.480815;10.1109/visual.1995.480815\", \"AuthorKeywords\": null, \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 18.0, \"Downloads_Xplore\": 133.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"Vis\", \"Year\": 1995, \"Title\": \"Subverting structure: data-driven diagram generation\", \"DOI\": \"10.1109/visual.1995.480815\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.1995.480815\", \"FirstPage\": 217.0, \"LastPage\": null, \"PaperType\": \"C\", \"Abstract\": \"Diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction. We have implemented a prototype called AVE (Automatic Visualization Environment) that generates diagrams automatically based on a generative theory of diagram design. According to this theory, diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams. This approach can be applied to knowledge represented by semantic networks. We give a brief introduction to the underlying theory, then describe the implementation and finally discuss strategies for extending the algorithm.\", \"AuthorNames-Deduped\": \"Gene Golovchinsky;Klaus Reichenberger;Thomas Kamps\", \"AuthorNames\": \"G. Golovchinsky;T. Kamps;K. Reichenberger\", \"AuthorAffiliation\": \"Department of Industrial Engineering, University of Toronto, Toronto, ONT, Canada;PaVE Department, GMD, Darmstadt, Germany;PaVE Department, GMD, Darmstadt, Germany\", \"InternalReferences\": \"10.1109/infvis.1995.528681;10.1109/infvis.1995.528681\", \"AuthorKeywords\": null, \"AminerCitationCount\": 21.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 11.0, \"Downloads_Xplore\": 66.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 21.0}, {\"Conference\": \"Vis\", \"Year\": 2004, \"Title\": \"Non-linear model fitting to parameterize diseased blood vessels\", \"DOI\": \"10.1109/visual.2004.72\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.2004.72\", \"FirstPage\": 393.0, \"LastPage\": 400.0, \"PaperType\": \"C\", \"Abstract\": \"Accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood vessels. The objective of this research is to estimate the dimensions of lower extremity arteries, imaged by computed tomography (CT). These parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization technique such as curved planar reformation (CPR). The vessel is modeled using an elliptical or cylindrical structure with specific dimensions, orientation and blood vessel mean density. The model separates two homogeneous regions: its inner side represents a region of density for vessels, and its outer side a region for background. Taking into account the point spread function (PSF) of a CT scanner, a function is modeled with a Gaussian kernel, in order to smooth the vessel boundary in the model. A new strategy for vessel parameter estimation is presented. It stems from vessel model and model parameter optimization by a nonlinear optimization procedure, i.e., the Levenberg-Marquardt technique. The method provides center location, diameter and orientation of the vessel as well as blood and background mean density values. The method is tested on synthetic data and real patient data with encouraging results.\", \"AuthorNames-Deduped\": \"Alexandra La Cruz;Mat\\u00fas Straka;Arnold K\\u00f6chl;Milos Sr\\u00e1mek;M. Eduard Gr\\u00f6ller;Dominik Fleischmann\", \"AuthorNames\": \"A. La Cruz;M. Straka;A. Kochl;M. Sramek;E. Groller;D. Fleischmann\", \"AuthorAffiliation\": \"University of Technology, Vienna, Austria;Austrian Academy of Sciences, Austria;Vienna University of Medicine, Austria;Austrian Academy of Sciences, Austria;University of Technology, Vienna, Austria;Stanford University Medical Center, USA\", \"InternalReferences\": \"10.1109/visual.2001.964555\", \"AuthorKeywords\": \"Visualization, Segmentation, Blood Vessel Detection\", \"AminerCitationCount\": 29.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 11.0, \"Downloads_Xplore\": 141.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 29.0}, {\"Conference\": \"Vis\", \"Year\": 2004, \"Title\": \"Context-Adaptive Mobile Visualization and Information Management\", \"DOI\": \"10.1109/visual.2004.19\", \"Link\": \"http://dx.doi.org/10.1109/VISUAL.2004.19\", \"FirstPage\": 8.0, \"LastPage\": 8.0, \"PaperType\": \"M\", \"Abstract\": \"This poster abstract presents a scalable information visualization system for mobile devices and desktop systems. It is designed to support the operation and the workflow of wastewater systems. The regarded information data includes general information about buildings and units, process data, occupational safety regulations, work directions and first aid instructions in case of an accident. Technically, the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like PDAs (Personal Digital Assistants) or Tablet PCs. The implementation is based on but not limited to SQL, JSP, HTML and VRML.\", \"AuthorNames-Deduped\": \"Jochen Ehret;Achim Ebert;Lars Schuchardt;Heidrun Steinmetz;Hans Hagen\", \"AuthorNames\": \"J. Ehret;A. Ebert;L. Schuchardt;H. Steinmetz;H. Hagen\", \"AuthorAffiliation\": \"Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany;Institute of Environmental Engineering, Technical University of Kaiserslautern, Germany;Center for Innovative WasteWater Technology (tectraa), Technical University of Kaiserslautern, Germany;Intelligent Visualization and Simulation, German Research Center for Artificial Intelligence, Kaiserslautern, Germany\", \"InternalReferences\": null, \"AuthorKeywords\": null, \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 4.0, \"Downloads_Xplore\": 172.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"VAST\", \"Year\": 2006, \"Title\": \"Collaborative Visual Analytics: Inferring from the Spatial Organization and Collaborative Use of Information\", \"DOI\": \"10.1109/vast.2006.261415\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2006.261415\", \"FirstPage\": 137.0, \"LastPage\": 144.0, \"PaperType\": \"C\", \"Abstract\": \"We introduce a visual analytics environment for the support of remote-collaborative sense-making activities. Team members use their individual graphical interfaces to collect, organize and comprehend task-relevant information relative to their areas of expertise. A system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information. The computational agents support the exchange of information among team members to converge their individual contributions. Our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbal communication\", \"AuthorNames-Deduped\": \"Paul E. Keel\", \"AuthorNames\": \"Paul E. Keel\", \"AuthorAffiliation\": \"Computer Science and Artifificial Intelligence Laboratory, Massachusetts Institute of Technology, UK\", \"InternalReferences\": null, \"AuthorKeywords\": \"Visual analytics, Spatial information organization,Indirect human computer interaction,Indirect collaboration, Agents,Sense-making\", \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 24.0, \"PubsCited_CrossRef\": 23.0, \"Downloads_Xplore\": 472.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"Vis\", \"Year\": 2007, \"Title\": \"Interactive Visual Analysis of Perfusion Data\", \"DOI\": \"10.1109/tvcg.2007.70569\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2007.70569\", \"FirstPage\": 1392.0, \"LastPage\": 1399.0, \"PaperType\": \"J\", \"Abstract\": \"Perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue. These data bear a great potential in medical diagnosis, since diseases can be better distinguished and detected at an earlier stage compared to static image data. The wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods. For each voxel, a time-intensity curve characterizes the enhancement of a contrast agent. Parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis. The diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity. For the visual analysis of such datasets, feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas. We present an interactive visual analysis approach for the evaluation of perfusion data. For this purpose, we integrate statistical methods and interactive feature specification. Correlation analysis and Principal Component Analysis (PCA) are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The specification result is linked to all views establishing a focus+context style of visualization in 3D. We discuss our approach with respect to clinical datasets from the three major application areas: ischemic stroke diagnosis, breast tumor diagnosis, as well as the diagnosis of the coronary heart disease (CHD). It turns out that the significance of perfusion parameters strongly depends on the individual patient, scanning parameters, and data pre-processing.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorNames\": \"Steffen Oeltze;Helmut Doleisch;Helwig Hauser;Philipp Muigg;Bernhard Preim\", \"AuthorAffiliation\": \"Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany;VRVis Research Center, Vienna, Austria;Department of Informatics, University of Bergen, Bergen, Norway;VRVis Research Center, Vienna, Austria;Department of Simulation and Graphics, University of Magdeburg, Magdeburg, Germany\", \"InternalReferences\": \"10.1109/visual.2000.885739;10.1109/visual.2005.1532847;10.1109/visual.2000.885739\", \"AuthorKeywords\": \"Multi-field Visualization, Visual Data Mining, Time-varying Volume Data, Integrating InfoVis/SciVis\", \"AminerCitationCount\": 100.0, \"CitationCount_CrossRef\": 44.0, \"PubsCited_CrossRef\": 28.0, \"Downloads_Xplore\": 666.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 100.0}, {\"Conference\": \"InfoVis\", \"Year\": 2008, \"Title\": \"Multi-Focused Geospatial Analysis Using Probes\", \"DOI\": \"10.1109/tvcg.2008.149\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2008.149\", \"FirstPage\": 1165.0, \"LastPage\": 1172.0, \"PaperType\": \"J\", \"Abstract\": \"Traditional geospatial information visualizations often present views that restrict the user to a single perspective. When zoomed out, local trends and anomalies become suppressed and lost; when zoomed in for local inspection, spatial awareness and comparison between regions become limited. In our model, coordinated visualizations are integrated within individual probe interfaces, which depict the local data in user-defined regions-of-interest. Our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe, coordinate, and compare data across multiple local regions. It is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole. We illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization systems: an agent-based social simulation, a census data exploration tool, and an 3D GIS environment for analyzing urban change over time. In each case, the probe-based interaction enhances spatial awareness, improves inspection and comparison capabilities, expands the range of scopes, and facilitates collaboration among multiple users.\", \"AuthorNames-Deduped\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorNames\": \"Thomas Butkiewicz;Wenwen Dou;Zachary Wartell;William Ribarsky;Remco Chang\", \"AuthorAffiliation\": \"UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center;UNC Charlotte, The Charlotte Visualization Center\", \"InternalReferences\": \"10.1109/infvis.2000.885102;10.1109/tvcg.2007.70574;10.1109/infvis.2000.885102\", \"AuthorKeywords\": \"Multiple-view techniques, geospatial visualization, geospatial analysis, focus + context, probes\", \"AminerCitationCount\": 73.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 20.0, \"Downloads_Xplore\": 648.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 73.0}, {\"Conference\": \"VAST\", \"Year\": 2009, \"Title\": \"Articulate: a conversational interface for visual analytics\", \"DOI\": \"10.1109/vast.2009.5333099\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2009.5333099\", \"FirstPage\": 233.0, \"LastPage\": 234.0, \"PaperType\": \"M\", \"Abstract\": \"While many visualization tools exist that offer sophisticated functions for charting complex data, they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization. This poster presents Articulate, an attempt at a semi-automated visual analytic model that is guided by a conversational user interface. The goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface, by instead being able to verbally articulate what the user wants to see, and then using natural language processing and heuristics to semi-automatically create a suitable visualization.\", \"AuthorNames-Deduped\": \"Yiwen Sun;Jason Leigh;Andrew E. Johnson 0001;Dennis Chau\", \"AuthorNames\": \"Yiwen Sun;Jason Leigh;Andrew Johnson;Dennis Chau\", \"AuthorAffiliation\": \"Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA;Electronic Visualization Laboratory, University of Illinois, Chicago, USA\", \"InternalReferences\": \"0.1109/tvcg.2007.70594;10.1109/tvcg.2006.148\", \"AuthorKeywords\": null, \"AminerCitationCount\": 3.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 9.0, \"Downloads_Xplore\": 267.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 3.0}, {\"Conference\": \"VAST\", \"Year\": 2010, \"Title\": \"ALIDA: Using machine learning for intent discernment in visual analytics interfaces\", \"DOI\": \"10.1109/vast.2010.5650854\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2010.5650854\", \"FirstPage\": 223.0, \"LastPage\": 224.0, \"PaperType\": \"M\", \"Abstract\": \"In this paper, we introduce ALIDA, an Active Learning Intent Discerning Agent for visual analytics interfaces. As users interact with and explore data in a visual analytics environment they are each developing their own unique analytic process. The goal of ALIDA is to observe and record the human-computer interactions and utilize these observations as a means of supporting user exploration; ALIDA does this by using interaction to make decision about user interest. As such, ALIDA is designed to track the decision history (interactions) of a user. This history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states, as well as providing suggestions of other search states that may be of interest based on past exploration modalities. The agent passes these suggestions (or decisions) back to an interactive visualization prototype, and these suggestions are used to guide the user, either by suggesting searches or changes to the visualization view. Current work has tested ALIDA under the exploration of homonyms for users wishing to explore word linkages within a dictionary. Ongoing work includes using ALIDA to guide users in transfer function design for volume rendering within scientific gateways.\", \"AuthorNames-Deduped\": \"Tera Marie Green;Ross Maciejewski;Steve DiPaola\", \"AuthorNames\": \"Tera Marie Green;Ross Maciejewski;Steve DiPaola\", \"AuthorAffiliation\": \"School of Interactive Arts Technology, Simon Fraser University, Canada;Purdue Visual Analytics Center, Purdue University, USA;School of Interactive Arts Technology, Simon Fraser University, Canada\", \"InternalReferences\": null, \"AuthorKeywords\": \"artificial intelligence, cognition, intent discernment, volume rendering\", \"AminerCitationCount\": 4.0, \"CitationCount_CrossRef\": 3.0, \"PubsCited_CrossRef\": 6.0, \"Downloads_Xplore\": 391.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 4.0}, {\"Conference\": \"Vis\", \"Year\": 2011, \"Title\": \"Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics\", \"DOI\": \"10.1109/tvcg.2011.217\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2011.217\", \"FirstPage\": 1882.0, \"LastPage\": 1891.0, \"PaperType\": \"J\", \"Abstract\": \"In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.\", \"AuthorNames-Deduped\": \"Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert\", \"AuthorNames\": \"Steffen Oeltze;Wolfgang Freiler;Reyk Hillert;Helmut Doleisch;Bernhard Preim;Walter Schubert\", \"AuthorAffiliation\": \"University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;SimVis GmbH, Vienna, Austria;University of Magdeburg, Germany;University of Magdeburg, Germany\", \"InternalReferences\": \"10.1109/vast.2009.5333911;10.1109/tvcg.2006.195;10.1109/tvcg.2006.147;10.1109/tvcg.2007.70569;10.1109/tvcg.2009.167;10.1109/vast.2009.5333911\", \"AuthorKeywords\": \"Visual Analytics, Fluorescence Microscopy, Toponomics, Protein Interaction, Graph Visualization\", \"AminerCitationCount\": 22.0, \"CitationCount_CrossRef\": 9.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 780.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 22.0}, {\"Conference\": \"VAST\", \"Year\": 2011, \"Title\": \"Exploring agent-based simulations using temporal graphs\", \"DOI\": \"10.1109/vast.2011.6102469\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2011.6102469\", \"FirstPage\": 271.0, \"LastPage\": 272.0, \"PaperType\": \"M\", \"Abstract\": \"Agent-based simulation has become a key technique for modeling and simulating dynamic, complicated behaviors in social and behavioral sciences. Lacking the appropriate tools and support, it is difficult for social scientists to thoroughly analyze the results of these simulations. In this work, we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph. In collaboration with expert analysts, we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes. Finally, we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day Thailand.\", \"AuthorNames-Deduped\": \"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang\", \"AuthorNames\": \"R. Jordan Crouser;Jeremy G. Freeman;Remco Chang\", \"AuthorAffiliation\": \"Tufts University, USA;Tufts University, USA;Tufts University, USA\", \"InternalReferences\": \"0.1109/infvis.2005.1532126\", \"AuthorKeywords\": null, \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 8.0, \"Downloads_Xplore\": 163.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"SciVis\", \"Year\": 2012, \"Title\": \"Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization\", \"DOI\": \"10.1109/tvcg.2012.203\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2012.203\", \"FirstPage\": 2345.0, \"LastPage\": 2354.0, \"PaperType\": \"J\", \"Abstract\": \"Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.\", \"AuthorNames-Deduped\": \"Gunnar L\\u00e4th\\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga\", \"AuthorNames\": \"Gunnar L\\u00e4th\\u00e9n;Stefan Lindholm;Reiner Lenz;Anders Persson;Magnus Borga\", \"AuthorAffiliation\": \"Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Science and Technology, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Medical and Health Sciences, Link\\u00f6ping University, Sweden;Center for Medical Image Science and Visualization (CMIV), Department of Biomedical Engineering, Link\\u00f6ping University, Sweden\", \"InternalReferences\": \"10.1109/visual.2003.1250414;10.1109/tvcg.2009.120;10.1109/visual.2001.964516;10.1109/visual.1996.568113;10.1109/tvcg.2008.162;10.1109/tvcg.2010.195;10.1109/tvcg.2008.123\", \"AuthorKeywords\": \"Direct volume rendering, transfer functions, vessel visualization\", \"AminerCitationCount\": 29.0, \"CitationCount_CrossRef\": 14.0, \"PubsCited_CrossRef\": 34.0, \"Downloads_Xplore\": 513.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 29.0}, {\"Conference\": \"InfoVis\", \"Year\": 2013, \"Title\": \"A Design Space of Visualization Tasks\", \"DOI\": \"10.1109/tvcg.2013.120\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2013.120\", \"FirstPage\": 2366.0, \"LastPage\": 2375.0, \"PaperType\": \"J\", \"Abstract\": \"Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.\", \"AuthorNames-Deduped\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorNames\": \"Hans-J\\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\", \"AuthorAffiliation\": \"University of Rostock, Germany;Potsdam Institute for Climate Impact Research, USA;Potsdam Institute for Climate Impact Research, USA;University of Rostock, Germany\", \"InternalReferences\": \"10.1109/infvis.1996.559213;10.1109/infvis.2005.1532136;10.1109/tvcg.2007.70515;10.1109/visual.1990.146372;10.1109/tvcg.2012.205;10.1109/visual.1992.235203;10.1109/infvis.2004.59;10.1109/vast.2008.4677365;10.1109/infvis.1996.559211;10.1109/infvis.2004.10;10.1109/infvis.1997.636792;10.1109/infvis.2000.885093;10.1109/infvis.2000.885092;10.1109/visual.1990.146375;10.1109/visual.2004.10;10.1109/infvis.1996.559213\", \"AuthorKeywords\": \"Task taxonomy, design space, climate impact research, visualization recommendation\", \"AminerCitationCount\": 217.0, \"CitationCount_CrossRef\": 144.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 4884.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 217.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Finding Waldo: Learning about Users from their Interactions\", \"DOI\": \"10.1109/tvcg.2014.2346575\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346575\", \"FirstPage\": 1663.0, \"LastPage\": 1672.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems.\", \"AuthorNames-Deduped\": \"Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorNames\": \"Eli T Brown;Alvitta Ottley;Helen Zhao;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\", \"AuthorAffiliation\": \"Tufts U;Tufts U;Purdue U. and Tufts U;Tufts U;U.N.C. Charlotte;Pacific Northwest National Lab;Tufts U\", \"InternalReferences\": \"10.1109/tvcg.2012.204;10.1109/vast.2010.5653587;10.1109/vast.2009.5333020;10.1109/vast.2012.6400486;10.1109/visual.2005.1532788;10.1109/tvcg.2012.276;10.1109/vast.2006.261436;10.1109/vast.2008.4677352;10.1109/tvcg.2012.204\", \"AuthorKeywords\": \"User Interactions, Analytic Provenance, Visualization, Applied Machine Learning\", \"AminerCitationCount\": 145.0, \"CitationCount_CrossRef\": 95.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 2226.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 145.0}, {\"Conference\": \"InfoVis\", \"Year\": 2014, \"Title\": \"Learning Perceptual Kernels for Visualization Design\", \"DOI\": \"10.1109/tvcg.2014.2346978\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2014.2346978\", \"FirstPage\": 1933.0, \"LastPage\": 1942.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd-sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types-including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement-and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.\", \"AuthorNames-Deduped\": \"\\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorNames\": \"\\u00c7a\\u011fatay Demiralp;Michael S. Bernstein;Jeffrey Heer\", \"AuthorAffiliation\": \"Stanford University;Stanford University;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2010.186;10.1109/tvcg.2006.163;10.1109/tvcg.2007.70594;10.1109/tvcg.2011.167;10.1109/tvcg.2007.70583;10.1109/tvcg.2008.125;10.1109/tvcg.2010.130;10.1109/tvcg.2007.70539;10.1109/tvcg.2010.186\", \"AuthorKeywords\": \"Visualization, design, encoding, perception, model, crowdsourcing, automated visualization, visual embedding\", \"AminerCitationCount\": 129.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 1247.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 129.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"Visual Analysis of Patterns in Multiple Amino Acid Mutation Graphs\", \"DOI\": \"10.1109/vast.2014.7042485\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2014.7042485\", \"FirstPage\": 93.0, \"LastPage\": 102.0, \"PaperType\": \"C\", \"Abstract\": \"Proteins are essential parts in all living organisms. They consist of sequences of amino acids. An interaction with reactive agent can stimulate a mutation at a specific position in the sequence. This mutation may set off a chain reaction, which effects other amino acids in the protein. Chain reactions need to be analyzed, as they may invoke unwanted side effects in drug treatment. A mutation chain is represented by a directed acyclic graph, where amino acids are connected by their mutation dependencies. As each amino acid may mutate individually, many mutation graphs exist. To determine important impacts of mutations, experts need to analyze and compare common patterns in these mutations graphs. Experts, however, lack suitable tools for this purpose. We present a new system for the search and the exploration of frequent patterns (i.e., motifs) in mutation graphs. We present a fast pattern search algorithm specifically developed for finding biologically relevant patterns in many mutation graphs (i.e., many labeled acyclic directed graphs). Our visualization system allows an interactive exploration and comparison of the found patterns. It enables locating the found patterns in the mutation graphs and in the 3D protein structures. In this way, potentially interesting patterns can be discovered. These patterns serve as starting point for a further biological analysis. In cooperation with biologists, we use our approach for analyzing a real world data set based on multiple HIV protease sequences.\", \"AuthorNames-Deduped\": \"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger\", \"AuthorNames\": \"Olav Lenz;Frank Keul;Sebastian Bremm;Kay Hamacher;Tatiana von Landesberger\", \"AuthorAffiliation\": \"GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt;Computational Biology, TU, Darmstadt;GRIS, TU, Darmstadt\", \"InternalReferences\": \"10.1109/tvcg.2013.225;10.1109/vast.2011.6102439;10.1109/vast.2009.5333893;10.1109/tvcg.2009.167;10.1109/tvcg.2007.70521;10.1109/tvcg.2009.122;10.1109/tvcg.2007.70529;10.1109/tvcg.2012.208;10.1109/tvcg.2013.225\", \"AuthorKeywords\": \"Biologic Visualization, Graph Visualization, Motif Search, Motif Visualization, Biology, Mutations, Pattern Visualization\", \"AminerCitationCount\": 14.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 51.0, \"Downloads_Xplore\": 331.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"VAST\", \"Year\": 2014, \"Title\": \"An Integrated Visual Analysis System for Fusing MR Spectroscopy and Multi-Modal Radiology Imaging\", \"DOI\": \"10.1109/vast.2014.7042481\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2014.7042481\", \"FirstPage\": 53.0, \"LastPage\": 62.0, \"PaperType\": \"C\", \"Abstract\": \"For cancers such as glioblastoma multiforme, there is an increasing interest in defining \\\"biological target volumes\\\" (BTV), high tumour-burden regions which may be targeted with dose boosts in radiotherapy. The definition of a BTV requires insight into tumour characteristics going beyond conventionally defined radiological abnormalities and anatomical features. Molecular and biochemical imaging techniques, like positron emission tomography, the use of Magnetic Resonance (MR) Imaging contrast agents or MR Spectroscopy deliver this information and support BTV delineation. MR Spectroscopy Imaging (MRSI) is the only non-invasive technique in this list. Studies with MRSI have shown that voxels with certain metabolic signatures are more susceptible to predict the site of relapse. Nevertheless, the discovery of complex relationships between a high number of different metabolites, anatomical, molecular and functional features is an ongoing topic of research - still lacking appropriate tools supporting a smooth workflow by providing data integration and fusion of MRSI data with other imaging modalities. We present a solution bridging this gap which gives fast and flexible access to all data at once. By integrating a customized visualization of the multi-modal and multi-variate image data with a highly flexible visual analytics (VA) framework, it is for the first time possible to interactively fuse, visualize and explore user defined metabolite relations derived from MRSI in combination with markers delivered by other imaging modalities. Real-world medical cases demonstrate the utility of our solution. By making MRSI data available both in a VA tool and in a multi-modal visualization renderer we can combine insights from each side to arrive at a superior BTV delineation. We also report feedback from domain experts indicating significant positive impact in how this work can improve the understanding of MRSI data and its integration into radiotherapy planning.\", \"AuthorNames-Deduped\": \"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\\u00e9akh\\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\\u00fchler\", \"AuthorNames\": \"Miguel Nunes;Benjamin Rowland;Matthias Schlachter;Sol\\u00e9akh\\u00e9na Ken;Kresimir Matkovic;Anne Laprie;Katja B\\u00fchler\", \"AuthorAffiliation\": \"VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria;Institut Claudius Regaud, Toulouse, France;VRVis Research Center, Vienna, Austria\", \"InternalReferences\": \"10.1109/tvcg.2007.70569;10.1109/tvcg.2013.180;10.1109/tvcg.2010.176;10.1109/tvcg.2007.70569\", \"AuthorKeywords\": \"MR spectroscopy, cancer, brain, visualization, multi-modality data, radiotherapy planning, medical decision support systems\", \"AminerCitationCount\": 17.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 29.0, \"Downloads_Xplore\": 300.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 17.0}, {\"Conference\": \"InfoVis\", \"Year\": 2015, \"Title\": \"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations\", \"DOI\": \"10.1109/tvcg.2015.2467191\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2015.2467191\", \"FirstPage\": 649.0, \"LastPage\": 658.0, \"PaperType\": \"J\", \"Abstract\": \"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.\", \"AuthorNames-Deduped\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock Mackinlay;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;Tableau Research;Tableau Research;Tableau Research;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/tvcg.2014.2346297;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70594;10.1109/tvcg.2014.2346291;10.1109/infvis.2000.885086;10.1109/tvcg.2014.2346297\", \"AuthorKeywords\": \"User interfaces, information visualization, exploratory analysis, visualization recommendation, mixed-initiative systems\", \"AminerCitationCount\": 487.0, \"CitationCount_CrossRef\": 292.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 4307.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 487.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Mixed-initiative visual analytics using task-driven recommendations\", \"DOI\": \"10.1109/vast.2015.7347625\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347625\", \"FirstPage\": 9.0, \"LastPage\": 16.0, \"PaperType\": \"C\", \"Abstract\": \"Visual data analysis is composed of a collection of cognitive actions and tasks to decompose, internalize, and recombine data to produce knowledge and insight. Visual analytic tools provide interactive visual interfaces to data to support discovery and sensemaking tasks, including forming hypotheses, asking questions, and evaluating and organizing evidence. Myriad analytic models can be incorporated into visual analytic systems at the cost of increasing complexity in the analytic discourse between user and system. Techniques exist to increase the usability of interacting with analytic models, such as inferring data models from user interactions to steer the underlying models of the system via semantic interaction, shielding users from having to do so explicitly. Such approaches are often also referred to as mixed-initiative systems. Sensemaking researchers have called for development of tools that facilitate analytic sensemaking through a combination of human and automated activities. However, design guidelines do not exist for mixed-initiative visual analytic systems to support iterative sensemaking. In this paper, we present candidate design guidelines and introduce the Active Data Environment (ADE) prototype, a spatial workspace supporting the analytic process via task recommendations invoked by inferences about user interactions within the workspace. ADE recommends data and relationships based on a task model, enabling users to co-reason with the system about their data in a single, spatial workspace. This paper provides an illustrative use case, a technical description of ADE, and a discussion of the strengths and limitations of the approach.\", \"AuthorNames-Deduped\": \"Kristin A. Cook;Nick Cramer;David J. Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorNames\": \"Kristin Cook;Nick Cramer;David Israel;Michael Wolverton;Joe Bruce;Russ Burtner;Alex Endert\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;SRI International;SRI International;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/vast.2011.6102438;10.1109/vast.2012.6400559;10.1109/tvcg.2014.2346573;10.1109/vast.2014.7042492;10.1109/tvcg.2008.174;10.1109/tvcg.2013.225;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"mixed-initiative visual analytics, task modeling, recommender systems, sensemaking\", \"AminerCitationCount\": 36.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 36.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 36.0}, {\"Conference\": \"VAST\", \"Year\": 2015, \"Title\": \"Collaborative visual analysis with RCloud\", \"DOI\": \"10.1109/vast.2015.7347627\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2015.7347627\", \"FirstPage\": 25.0, \"LastPage\": 32.0, \"PaperType\": \"C\", \"Abstract\": \"Consider the emerging role of data science teams embedded in larger organizations. Individual analysts work on loosely related problems, and must share their findings with each other and the organization at large, moving results from exploratory data analyses (EDA) into automated visualizations, diagnostics and reports deployed for wider consumption. There are two problems with the current practice. First, there are gaps in this workflow: EDA is performed with one set of tools, and automated reports and deployments with another. Second, these environments often assume a single-developer perspective, while data scientist teams could get much benefit from easier sharing of scripts and data feeds, experiments, annotations, and automated recommendations, which are well beyond what traditional version control systems provide. We contribute and justify the following three requirements for systems built to support current data science teams and users: discoverability, technology transfer, and coexistence. In addition, we contribute the design and implementation of RCloud, a system that supports the requirements of collaborative data analysis, visualization and web deployment. About 100 people used RCloud for two years. We report on interviews with some of these users, and discuss design decisions, tradeoffs and limitations in comparison to other approaches.\", \"AuthorNames-Deduped\": \"Stephen C. North;Carlos Eduardo Scheidegger;Simon Urbanek;Gordon Woodhull\", \"AuthorNames\": \"Stephen North;Carlos Scheidegger;Simon Urbanek;Gordon Woodhull\", \"AuthorAffiliation\": \"Infovisible;University of Arizona;AT&T Labs;AT&T Labs\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/vast.2007.4389011;10.1109/tvcg.2012.219;10.1109/tvcg.2009.195;10.1109/tvcg.2007.70577;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"visual analytics process, provenance, collaboration, visualization, computer-supported cooperative work\", \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 40.0, \"Downloads_Xplore\": 404.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"SciVis\", \"Year\": 2015, \"Title\": \"Automated visualization workflow for simulation experiments\", \"DOI\": \"10.1109/scivis.2015.7429509\", \"Link\": \"http://dx.doi.org/10.1109/SciVis.2015.7429509\", \"FirstPage\": 153.0, \"LastPage\": 154.0, \"PaperType\": \"M\", \"Abstract\": \"Modeling and simulation is often used to predict future events and plan accordingly. Experiments in this domain often produce thousands of results from individual simulations, based on slightly varying input parameters. Geo-spatial visualizations can be a powerful tool to help health researchers and decision-makers to take measures during catastrophic and epidemic events such as Ebola outbreaks. The work produced a web-based geo-visualization tool to visualize and compare the spread of Ebola in the West African countries Ivory Coast and Senegal based on multiple simulation results. The visualization is not Ebola specific and may visualize any time-varying frequencies for given geo-locations.\", \"AuthorNames-Deduped\": \"Jonathan P. Leidig;Santhosh Dharmapuri\", \"AuthorNames\": \"Jonathan P. Leidig;Santhosh Dharmapuri\", \"AuthorAffiliation\": \"School of Computing and Information Systems, Grand Valley State University;School of Computing and Information Systems, Grand Valley State University\", \"InternalReferences\": null, \"AuthorKeywords\": null, \"AminerCitationCount\": 1.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 12.0, \"Downloads_Xplore\": 137.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Data-Driven Guides: Supporting Expressive Design for Information Graphics\", \"DOI\": \"10.1109/tvcg.2016.2598620\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598620\", \"FirstPage\": 491.0, \"LastPage\": 500.0, \"PaperType\": \"J\", \"Abstract\": \"In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.\", \"AuthorNames-Deduped\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorNames\": \"Nam Wook Kim;Eston Schweickart;Zhicheng Liu;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\", \"AuthorAffiliation\": \"John A. Paulson School of Engineering and Applied Sciences, Harvard University;Computer Science department, Cornell University;Adobe Research;Adobe Research;Adobe Research;Adobe Research;John A. Paulson School of Engineering and Applied Sciences, Harvard University\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/infvis.1996.559212;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598609;10.1109/tvcg.2013.234;10.1109/infvis.2004.64;10.1109/tvcg.2012.197;10.1109/infvis.2000.885086;10.1109/infvis.2000.885093;10.1109/tvcg.2014.2346979;10.1109/tvcg.2014.2346320;10.1109/tvcg.2014.2346291;10.1109/tvcg.2015.2467732;10.1109/infvis.2004.12;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2010.144;10.1109/tvcg.2011.185;10.1109/tvcg.2007.70577;10.1109/tvcg.2013.134;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Information graphics;visualization;design tools;2D graphics\", \"AminerCitationCount\": 114.0, \"CitationCount_CrossRef\": 92.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2245.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 114.0}, {\"Conference\": \"InfoVis\", \"Year\": 2016, \"Title\": \"Visualization by Demonstration: An Interaction Paradigm for Visual Data Exploration\", \"DOI\": \"10.1109/tvcg.2016.2598839\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598839\", \"FirstPage\": 331.0, \"LastPage\": 340.0, \"PaperType\": \"J\", \"Abstract\": \"Although data visualization tools continue to improve, during the data exploration process many of them require users to manually specify visualization techniques, mappings, and parameters. In response, we present the Visualization by Demonstration paradigm, a novel interaction method for visual data exploration. A system which adopts this paradigm allows users to provide visual demonstrations of incremental changes to the visual representation. The system then recommends potential transformations (Visual Representation, Data Mapping, Axes, and View Specification transformations) from the given demonstrations. The user and the system continue to collaborate, incrementally producing more demonstrations and refining the transformations, until the most effective possible visualization is created. As a proof of concept, we present VisExemplar, a mixed-initiative prototype that allows users to explore their data by recommending appropriate transformations in response to the given demonstrations.\", \"AuthorNames-Deduped\": \"Bahador Saket;Hannah Kim 0001;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Bahador Saket;Hannah Kim;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;DePaul University;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2014.2346292;10.1109/tvcg.2015.2467191;10.1109/tvcg.2007.70594;10.1109/vast.2011.6102449;10.1109/tvcg.2007.70515;10.1109/tvcg.2014.2346250;10.1109/tvcg.2012.275;10.1109/tvcg.2015.2467153;10.1109/tvcg.2013.191;10.1109/tvcg.2011.251;10.1109/tvcg.2011.185;10.1109/tvcg.2014.2346291;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346292\", \"AuthorKeywords\": \"Visual Data Exploration;Visualization by Demonstration;Visualization Tools\", \"AminerCitationCount\": 83.0, \"CitationCount_CrossRef\": 57.0, \"PubsCited_CrossRef\": 35.0, \"Downloads_Xplore\": 2781.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 83.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Familiarity Vs Trust: A Comparative Study of Domain Scientists' Trust in Visual Analytics and Conventional Analysis Methods\", \"DOI\": \"10.1109/tvcg.2016.2598544\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598544\", \"FirstPage\": 271.0, \"LastPage\": 280.0, \"PaperType\": \"J\", \"Abstract\": \"Combining interactive visualization with automated analytical methods like statistics and data mining facilitates data-driven discovery. These visual analytic methods are beginning to be instantiated within mixed-initiative systems, where humans and machines collaboratively influence evidence-gathering and decision-making. But an open research question is that, when domain experts analyze their data, can they completely trust the outputs and operations on the machine-side? Visualization potentially leads to a transparent analysis process, but do domain experts always trust what they see? To address these questions, we present results from the design and evaluation of a mixed-initiative, visual analytics system for biologists, focusing on analyzing the relationships between familiarity of an analysis medium and domain experts' trust. We propose a trust-augmented design of the visual analytics system, that explicitly takes into account domain-specific tasks, conventions, and preferences. For evaluating the system, we present the results of a controlled user study with 34 biologists where we compare the variation of the level of trust across conventional and visual analytic mediums and explore the influence of familiarity and task complexity on trust. We find that despite being unfamiliar with a visual analytic medium, scientists seem to have an average level of trust that is comparable with the same in conventional analysis medium. In fact, for complex sense-making tasks, we find that the visual analytic system is able to inspire greater trust than other mediums. We summarize the implications of our findings with directions for future research on trustworthiness of visual analytic systems.\", \"AuthorNames-Deduped\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin A. Cook;Samuel H. Payne\", \"AuthorNames\": \"Aritra Dasgupta;Joon-Yong Lee;Ryan Wilson;Robert A. Lafrance;Nick Cramer;Kristin Cook;Samuel Payne\", \"AuthorAffiliation\": \"Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory\", \"InternalReferences\": \"10.1109/tvcg.2015.2467591;10.1109/vast.2015.7347625;10.1109/tvcg.2012.224;10.1109/infvis.2005.1532136;10.1109/vast.2006.261416;10.1109/tvcg.2013.124;10.1109/tvcg.2013.120;10.1109/tvcg.2015.2467591\", \"AuthorKeywords\": \"trust;transparency;familiarity;uncertainty;biological data analysis\", \"AminerCitationCount\": 41.0, \"CitationCount_CrossRef\": 41.0, \"PubsCited_CrossRef\": 41.0, \"Downloads_Xplore\": 1844.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 41.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations\", \"DOI\": \"10.1109/tvcg.2016.2598543\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598543\", \"FirstPage\": 261.0, \"LastPage\": 270.0, \"PaperType\": \"J\", \"Abstract\": \"User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.\", \"AuthorNames-Deduped\": \"Jian Zhao 0010;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorNames\": \"Jian Zhao;Michael Glueck;Simon Breslav;Fanny Chevalier;Azam Khan\", \"AuthorAffiliation\": \"Autodesk Research;Autodesk Research;Autodesk Research;INRIA;Autodesk Research\", \"InternalReferences\": \"10.1109/vast.2009.5333878;10.1109/tvcg.2015.2467871;10.1109/vast.2009.5333023;10.1109/vast.2011.6102447;10.1109/tvcg.2008.137;10.1109/tvcg.2014.2346573;10.1109/vast.2008.4677365;10.1109/tvcg.2013.124;10.1109/tvcg.2007.70577;10.1109/vast.2010.5652879;10.1109/vast.2009.5333878\", \"AuthorKeywords\": \"Externalization user-authored annotation;exploratory sequential data analysis;graph-based visualization\", \"AminerCitationCount\": 39.0, \"CitationCount_CrossRef\": 33.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 2188.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 39.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"Toward Theoretical Techniques for Measuring the Use of Human Effort in Visual Analytic Systems\", \"DOI\": \"10.1109/tvcg.2016.2598460\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2598460\", \"FirstPage\": 121.0, \"LastPage\": 130.0, \"PaperType\": \"J\", \"Abstract\": \"Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.\", \"AuthorNames-Deduped\": \"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kristin A. Cook\", \"AuthorNames\": \"R. Jordan Crouser;Lyndsey Franklin;Alex Endert;Kris Cook\", \"AuthorAffiliation\": \"Smith College;Smith College;Smith College;Smith College\", \"InternalReferences\": \"10.1109/vast.2011.6102467;10.1109/vast.2010.5652910;10.1109/vast.2011.6102438;10.1109/tvcg.2012.195;10.1109/vast.2015.7347625;10.1109/vast.2007.4389009;10.1109/vast.2011.6102449;10.1109/vast.2012.6400486;10.1109/vast.2011.6102467\", \"AuthorKeywords\": \"Theoretical models;human oracle;visual analytics;mixed initiative systems;semantic interaction;sensemaking\", \"AminerCitationCount\": 20.0, \"CitationCount_CrossRef\": 16.0, \"PubsCited_CrossRef\": 87.0, \"Downloads_Xplore\": 978.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 20.0}, {\"Conference\": \"VAST\", \"Year\": 2016, \"Title\": \"VisMatchmaker: Cooperation of the User and the Computer in Centralized Matching Adjustment\", \"DOI\": \"10.1109/tvcg.2016.2599378\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2016.2599378\", \"FirstPage\": 231.0, \"LastPage\": 240.0, \"PaperType\": \"J\", \"Abstract\": \"Centralized matching is a ubiquitous resource allocation problem. In a centralized matching problem, each agent has a preference list ranking the other agents and a central planner is responsible for matching the agents manually or with an algorithm. While algorithms can find a matching which optimizes some performance metrics, they are used as a black box and preclude the central planner from applying his domain knowledge to find a matching which aligns better with the user tasks. Furthermore, the existing matching visualization techniques (i.e. bipartite graph and adjacency matrix) fail in helping the central planner understand the differences between matchings. In this paper, we present VisMatchmaker, a visualization system which allows the central planner to explore alternatives to an algorithm-generated matching. We identified three common tasks in the process of matching adjustment: problem detection, matching recommendation and matching evaluation. We classified matching comparison into three levels and designed visualization techniques for them, including the number line view and the stacked graph view. Two types of algorithmic support, namely direct assignment and range search, and their interactive operations are also provided to enable the user to apply his domain knowledge in matching adjustment.\", \"AuthorNames-Deduped\": \"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu\", \"AuthorNames\": \"Po-Ming Law;Wenchao Wu;Yixian Zheng;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology;Hong Kong University of Science and Technology\", \"InternalReferences\": \"10.1109/infvis.2004.1;10.1109/tvcg.2006.122;10.1109/tvcg.2014.2346249;10.1109/tvcg.2014.2346441;10.1109/vast.2011.6102453;10.1109/infvis.2004.1\", \"AuthorKeywords\": \"Centralized matching;matching visualization;interaction techniques;visual analytics\", \"AminerCitationCount\": 7.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 32.0, \"Downloads_Xplore\": 557.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Warning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics\", \"DOI\": \"10.1109/vast.2017.8585669\", \"Link\": \"http://dx.doi.org/10.1109/VAST.2017.8585669\", \"FirstPage\": 104.0, \"LastPage\": 115.0, \"PaperType\": \"C\", \"Abstract\": \"Visual analytic tools combine the complementary strengths of humans and machines in human-in-the-loop systems. Humans provide invaluable domain expertise and sensemaking capabilities to this discourse with analytic models; however, little consideration has yet been given to the ways inherent human biases might shape the visual analytic process. In this paper, we establish a conceptual framework for considering bias assessment through human-in-the-loop systems and lay the theoretical foundations for bias measurement. We propose six preliminary metrics to systematically detect and quantify bias from user interactions and demonstrate how the metrics might be implemented in an existing visual analytic system, InterAxis. We discuss how our proposed metrics could be used by visual analytic systems to mitigate the negative effects of cognitive biases by making users aware of biased processes throughout their analyses.\", \"AuthorNames-Deduped\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorNames\": \"Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\", \"AuthorAffiliation\": \"Georgia Tech;Pacific Northwest National Laboratory;Pacific Northwest National Laboratory;Georgia Tech\", \"InternalReferences\": \"10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2016.2599058;10.1109/vast.2008.4677365;10.1109/vast.2008.4677361;10.1109/visual.2000.885678;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2012.273;10.1109/tvcg.2015.2467551;10.1109/tvcg.2015.2467591;10.1109/tvcg.2014.2346481;10.1109/tvcg.2016.2598466;10.1109/tvcg.2017.2745078;10.1109/tvcg.2007.70589;10.1109/tvcg.2007.70515;10.1109/vast.2012.6400486\", \"AuthorKeywords\": \"cognitive bias,visual analytics,human-in-the-loop,mixed initiative,user interaction,H.5.0 [Information Systems]: Human-Computer Interaction-General\", \"AminerCitationCount\": 115.0, \"CitationCount_CrossRef\": 70.0, \"PubsCited_CrossRef\": 80.0, \"Downloads_Xplore\": 1801.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 115.0}, {\"Conference\": \"VAST\", \"Year\": 2017, \"Title\": \"Podium: Ranking Data Using Mixed-Initiative Visual Analytics\", \"DOI\": \"10.1109/tvcg.2017.2745078\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2017.2745078\", \"FirstPage\": 288.0, \"LastPage\": 297.0, \"PaperType\": \"J\", \"Abstract\": \"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.\", \"AuthorNames-Deduped\": \"Emily Wall;Subhajit Das 0002;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorNames\": \"Emily Wall;Subhajit Das;Ravish Chawla;Bharath Kalidindi;Eli T. Brown;Alex Endert\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;Georgia Institute of Technology, Atlanta, GA, USA;DePaul University, Chicago, IL, USA;Georgia Institute of Technology, Atlanta, GA, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/vast.2012.6400486;10.1109/tvcg.2014.2346575;10.1109/vast.2015.7347625;10.1109/tvcg.2016.2598594;10.1109/vast.2011.6102449;10.1109/tvcg.2013.173;10.1109/tvcg.2015.2467615;10.1109/tvcg.2016.2598446;10.1109/tvcg.2015.2467551;10.1109/tvcg.2016.2598839;10.1109/tvcg.2012.253;10.1109/vast.2017.8585669;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Mixed-initiative visual analytics,multi-attribute ranking,user interaction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 52.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1535.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 52.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco\", \"DOI\": \"10.1109/tvcg.2018.2865240\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865240\", \"FirstPage\": 438.0, \"LastPage\": 448.0, \"PaperType\": \"J\", \"Abstract\": \"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.\", \"AuthorNames-Deduped\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\", \"AuthorNames\": \"Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith;Bill Howe;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington;University of Washington;University of Washington;University of Washington;University of California Santa Cruz;University of Washington;University of Washington\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2014.2346984;10.1109/tvcg.2013.183;10.1109/tvcg.2014.2346979;10.1109/tvcg.2007.70594;10.1109/tvcg.2017.2744320;10.1109/tvcg.2017.2744198;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2744359;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Automated Visualization Design,Perceptual Effectiveness,Constraints,Knowledge Bases,Answer Set Programming\", \"AminerCitationCount\": 225.0, \"CitationCount_CrossRef\": 177.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 3238.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 225.0}, {\"Conference\": \"InfoVis\", \"Year\": 2018, \"Title\": \"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication\", \"DOI\": \"10.1109/tvcg.2018.2865145\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2865145\", \"FirstPage\": 672.0, \"LastPage\": 681.0, \"PaperType\": \"J\", \"Abstract\": \"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.\", \"AuthorNames-Deduped\": \"Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\", \"AuthorNames\": \"Arjun Srinivasan;Steven M. Drucker;Alex Endert;John Stasko\", \"AuthorAffiliation\": \"Georgia Institute of Technology, Atlanta, GA, US;Microsoft Research, Redmond, WA, US;Georgia Institute of Technology, Atlanta, GA, US;Georgia Institute of Technology, Atlanta, GA, US\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2010.164;10.1109/tvcg.2013.119;10.1109/tvcg.2012.229;10.1109/tvcg.2007.70594;10.1109/visual.1992.235203;10.1109/tvcg.2017.2744843;10.1109/tvcg.2017.2745219;10.1109/visual.1990.146375;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Natural Language Generation,Mixed-initiative Interaction,Visualization Recommendation,Data-driven Communication\", \"AminerCitationCount\": 120.0, \"CitationCount_CrossRef\": 121.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 2942.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 120.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks\", \"DOI\": \"10.1109/tvcg.2018.2864504\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864504\", \"FirstPage\": 288.0, \"LastPage\": 298.0, \"PaperType\": \"J\", \"Abstract\": \"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.\", \"AuthorNames-Deduped\": \"Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\", \"AuthorNames\": \"Junpeng Wang;Liang Gou;Han-Wei Shen;Hao Yang\", \"AuthorAffiliation\": \"The Ohio State University;Visa Research;The Ohio State University;Visa Research\", \"InternalReferences\": \"10.1109/tvcg.2017.2744683;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2017.2744718;10.1109/tvcg.2011.179;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/vast.2017.8585721;10.1109/tvcg.2013.200;10.1109/tvcg.2017.2744358;10.1109/tvcg.2017.2744158;10.1109/tvcg.2017.2744683\", \"AuthorKeywords\": \"Deep Q-Network (DQN),reinforcement learning,model interpretation,visual analytics\", \"AminerCitationCount\": 108.0, \"CitationCount_CrossRef\": 91.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 2871.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 108.0}, {\"Conference\": \"VAST\", \"Year\": 2018, \"Title\": \"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution\", \"DOI\": \"10.1109/tvcg.2018.2864769\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2018.2864769\", \"FirstPage\": 374.0, \"LastPage\": 384.0, \"PaperType\": \"J\", \"Abstract\": \"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel A. Keim;Christopher Collins 0001\", \"AuthorNames\": \"Mennatallah El-Assady;Fabian Sperrle;Oliver Deussen;Daniel Keim;Christopher Collins\", \"AuthorAffiliation\": \"Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;Universitat Konstanz, Konstanz, Baden-W\\u00c3\\u00bcrttemberg, DE;University of Ontario Institute of Technology, Oshawa, ON, CA\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2017.2744199;10.1109/tvcg.2017.2743959;10.1109/tvcg.2013.231;10.1109/tvcg.2013.212;10.1109/tvcg.2016.2598445;10.1109/tvcg.2014.2346578;10.1109/tvcg.2013.232;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"User-Steerable Topic Modeling,Speculative Execution,Mixed-Initiative Visual Analytics,Explainable Machine Learning\", \"AminerCitationCount\": 47.0, \"CitationCount_CrossRef\": 40.0, \"PubsCited_CrossRef\": 69.0, \"Downloads_Xplore\": 1217.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 47.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning\", \"DOI\": \"10.1109/vast47406.2019.8986948\", \"Link\": \"http://dx.doi.org/10.1109/VAST47406.2019.8986948\", \"FirstPage\": 46.0, \"LastPage\": 56.0, \"PaperType\": \"C\", \"Abstract\": \"The growing capability and accessibility of machine learning has led to its application to many real-world domains and data about people. Despite the benefits algorithmic systems may bring, models can reflect, inject, or exacerbate implicit and explicit societal biases into their outputs, disadvantaging certain demographic subgroups. Discovering which biases a machine learning model has introduced is a great challenge, due to the numerous definitions of fairness and the large number of potentially impacted subgroups. We present FAIRVIS, a mixed-initiative visual analytics system that integrates a novel subgroup discovery technique for users to audit the fairness of machine learning models. Through FAIRVIS, users can apply domain knowledge to generate and investigate known subgroups, and explore suggested and similar subgroups. FAIRVIS's coordinated views enable users to explore a high-level overview of subgroup performance and subsequently drill down into detailed investigation of specific subgroups. We show how FAIRVIS helps to discover biases in two real datasets used in predicting income and recidivism. As a visual analytics system devoted to discovering bias in machine learning, FAIRVIS demonstrates how interactive visualization may help data scientists and the general public understand and create more equitable algorithmic systems.\", \"AuthorNames-Deduped\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorNames\": \"\\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology;Georgia Institute of Technology\", \"InternalReferences\": \"10.1109/tvcg.2017.2744718;10.1109/vast.2017.8585720;10.1109/tvcg.2016.2598828;10.1109/tvcg.2018.2865044;10.1109/tvcg.2017.2744718\", \"AuthorKeywords\": \"Machine learning fairness,visual analytics,intersectional bias,subgroup discovery\", \"AminerCitationCount\": 107.0, \"CitationCount_CrossRef\": 106.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 2108.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 107.0}, {\"Conference\": \"InfoVis\", \"Year\": 2019, \"Title\": \"Text-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements\", \"DOI\": \"10.1109/tvcg.2019.2934785\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934785\", \"FirstPage\": 906.0, \"LastPage\": 916.0, \"PaperType\": \"J\", \"Abstract\": \"Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Xiaoyu Zhang;Yun Wang;He Huang;Bei Chen;Lei Fang;Haidong Zhang;Jian-Guan Lou;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia;ViDi Research Group, University of California, Davis;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2012.197;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.234;10.1109/tvcg.2016.2598876;10.1109/tvcg.2015.2467321;10.1109/tvcg.2016.2598620;10.1109/tvcg.2007.70594;10.1109/tvcg.2012.221;10.1109/tvcg.2018.2865240;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2010.179;10.1109/tvcg.2015.2467471;10.1109/tvcg.2018.2865145;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Visualization for the masses,infographic,automatic visualization,presentation,and dissemination\", \"AminerCitationCount\": 79.0, \"CitationCount_CrossRef\": 71.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 2661.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 79.0}, {\"Conference\": \"VAST\", \"Year\": 2019, \"Title\": \"Semantic Concept Spaces: Guided Topic Model Refinement using Word-Embedding Projections\", \"DOI\": \"10.1109/tvcg.2019.2934654\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2019.2934654\", \"FirstPage\": 1001.0, \"LastPage\": 1011.0, \"PaperType\": \"J\", \"Abstract\": \"We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users' decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.\", \"AuthorNames-Deduped\": \"Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins 0001;Daniel A. Keim;Oliver Deussen\", \"AuthorNames\": \"Mennatallah El-Assady;Rebecca Kehlbeck;Christopher Collins;Daniel Keim;Oliver Deussen\", \"AuthorAffiliation\": \"University of Konstanz, Germany and Ontario Tech University, Canada;University of Konstanz, Germany;Ontario Tech University, Canada;University of Konstanz, Germany;University of Konstanz, Germany\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/tvcg.2013.212;10.1109/vast.2011.6102461;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2017.2746018;10.1109/tvcg.2017.2744199;10.1109/tvcg.2013.126;10.1109/tvcg.2017.2744478;10.1109/tvcg.2019.2934629;10.1109/vast.2014.7042494;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"Topic Model Optimization,Word Embedding,Mixed-Initiative Refinement,Guided Visual Analytics,Semantic Mapping\", \"AminerCitationCount\": 30.0, \"CitationCount_CrossRef\": 18.0, \"PubsCited_CrossRef\": 59.0, \"Downloads_Xplore\": 1300.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 30.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Calliope: Automatic Visual Data Story Generation from a Spreadsheet\", \"DOI\": \"10.1109/tvcg.2020.3030403\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030403\", \"FirstPage\": 453.0, \"LastPage\": 463.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories shown in the form of narrative visualizations such as a poster or a data video, are frequently used in data-oriented storytelling to facilitate the understanding and memorization of the story content. Although useful, technique barriers, such as data analysis, visualization, and scripting, make the generation of a visual data story difficult. Existing authoring tools rely on users' skills and experiences, which are usually inefficient and still difficult. In this paper, we introduce a novel visual data story generating system, Calliope, which creates visual data stories from an input spreadsheet through an automatic process and facilities the easy revision of the generated story based on an online story editor. Particularly, Calliope incorporates a new logic-oriented Monte Carlo tree search algorithm that explores the data space given by the input spreadsheet to progressively generate story pieces (i.e., data facts) and organize them in a logical order. The importance of data facts is measured based on information theory, and each data fact is visualized in a chart and captioned by an automatically generated description. We evaluate the proposed technique through three example stories, two controlled experiments, and a series of interviews with 10 domain experts. Our evaluation shows that Calliope is beneficial to efficient visual data story generation.\", \"AuthorNames-Deduped\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\", \"AuthorNames\": \"Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University;Intelligent Big Data Visualization Lab, Tongji University\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934785;10.1109/tvcg.2013.119;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2019.2934281;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2018.2865232;10.1109/tvcg.2019.2934398;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Information Visualization,Visual Storytelling,Data Story\", \"AminerCitationCount\": 56.0, \"CitationCount_CrossRef\": 80.0, \"PubsCited_CrossRef\": 57.0, \"Downloads_Xplore\": 3724.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 56.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"PlotThread: Creating Expressive Storyline Visualizations using Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2020.3030467\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030467\", \"FirstPage\": 294.0, \"LastPage\": 303.0, \"PaperType\": \"J\", \"Abstract\": \"Storyline visualizations are an effective means to present the evolution of plots and reveal the scenic interactions among characters. However, the design of storyline visualizations is a difficult task as users need to balance between aesthetic goals and narrative constraints. Despite that the optimization-based methods have been improved significantly in terms of producing aesthetic and legible layouts, the existing (semi-) automatic methods are still limited regarding 1) efficient exploration of the storyline design space and 2) flexible customization of storyline layouts. In this work, we propose a reinforcement learning framework to train an AI agent that assists users in exploring the design space efficiently and generating well-optimized storylines. Based on the framework, we introduce PlotThread, an authoring tool that integrates a set of flexible interactions to support easy customization of storyline visualizations. To seamlessly integrate the AI agent into the authoring process, we employ a mixed-initiative approach where both the agent and designers work on the same canvas to boost the collaborative design of storylines. We evaluate the reinforcement learning model through qualitative and quantitative experiments and demonstrate the usage of PlotThread using a collection of use cases.\", \"AuthorNames-Deduped\": \"Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch 0001;Lingyun Yu 0001;Peiran Ren;Thomas Ertl;Yingcai Wu\", \"AuthorNames\": \"Tan Tang;Renzhong Li;Xinke Wu;Shuhan Liu;Johannes Knittel;Steffen Koch;Lingyun Yu;Peiran Ren;Thomas Ertl;Yingcai Wu\", \"AuthorAffiliation\": \"Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;VIS/VISUS, University of Stuttgart;Department of Computer Science and Software Engineering, Xi 'an Jiaotong-Liverpool University.;Alibaba Group;Zhejiang Lab and State Key Lab of CAD&CG, Zhejiang University\", \"InternalReferences\": \"10.1109/vast.2017.8585487;10.1109/tvcg.2019.2934396;10.1109/tvcg.2013.191;10.1109/tvcg.2016.2598831;10.1109/tvcg.2013.196;10.1109/tvcg.2012.212;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934798;10.1109/vast.2017.8585487\", \"AuthorKeywords\": \"Storyline visualization,reinforcement learning,mixed-initiative design\", \"AminerCitationCount\": 26.0, \"CitationCount_CrossRef\": 36.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 1931.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 26.0}, {\"Conference\": \"InfoVis\", \"Year\": 2020, \"Title\": \"Retrieve-Then-Adapt: Example-based Automatic Generation for Proportion-related Infographics\", \"DOI\": \"10.1109/tvcg.2020.3030448\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030448\", \"FirstPage\": 443.0, \"LastPage\": 452.0, \"PaperType\": \"J\", \"Abstract\": \"Infographic is a data visualization technique which combines graphic and textual descriptions in an aesthetic and effective manner. Creating infographics is a difficult and time-consuming process which often requires significant attempts and adjustments even for experienced designers, not to mention novice users with limited design expertise. Recently, a few approaches have been proposed to automate the creation process by applying predefined blueprints to user information. However, predefined blueprints are often hard to create, hence limited in volume and diversity. In contrast, good infogrpahics have been created by professionals and accumulated on the Internet rapidly. These online examples often represent a wide variety of design styles, and serve as exemplars or inspiration to people who like to create their own infographics. Based on these observations, we propose to generate infographics by automatically imitating examples. We present a two-stage approach, namely retrieve-then-adapt. In the retrieval stage, we index online examples by their visual elements. For a given user information, we transform it to a concrete query by sampling from a learned distribution about visual elements, and then find appropriate examples in our example library based on the similarity between example indexes and the query. For a retrieved example, we generate an initial drafts by replacing its content with user information. However, in many cases, user information cannot be perfectly fitted to retrieved examples. Therefore, we further introduce an adaption stage. Specifically, we propose a MCMC-like approach and leverage recursive neural networks to help adjust the initial draft and improve its visual appearance iteratively, until a satisfactory result is obtained. We implement our approach on widely-used proportion-related infographics, and demonstrate its effectiveness by sample results and expert reviews.\", \"AuthorNames-Deduped\": \"Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang 0001\", \"AuthorNames\": \"Chunyao Qian;Shizhao Sun;Weiwei Cui;Jian-Guang Lou;Haidong Zhang;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia, Peking University;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia;Microsoft Research Asia\", \"InternalReferences\": \"10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2019.2934810\", \"AuthorKeywords\": \"Infographics,automatic visualization\", \"AminerCitationCount\": 20.0, \"CitationCount_CrossRef\": 31.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 1004.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 20.0}, {\"Conference\": \"VAST\", \"Year\": 2020, \"Title\": \"VizCommender: Computing Text-Based Similarity in Visualization Repositories for Content-Based Recommendations\", \"DOI\": \"10.1109/tvcg.2020.3030387\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030387\", \"FirstPage\": 495.0, \"LastPage\": 505.0, \"PaperType\": \"J\", \"Abstract\": \"Cloud-based visualization services have made visual analytics accessible to a much wider audience than ever before. Systems such as Tableau have started to amass increasingly large repositories of analytical knowledge in the form of interactive visualization workbooks. When shared, these collections can form a visual analytic knowledge base. However, as the size of a collection increases, so does the difficulty in finding relevant information. Content-based recommendation (CBR) systems could help analysts in finding and managing workbooks relevant to their interests. Toward this goal, we focus on text-based content that is representative of the subject matter of visualizations rather than the visual encodings and style. We discuss the challenges associated with creating a CBR based on visualization specifications and explore more concretely how to implement the relevance measures required using Tableau workbook specifications as the source of content data. We also demonstrate what information can be extracted from these visualization specifications and how various natural language processing techniques can be used to compute similarity between workbooks as one way to measure relevance. We report on a crowd-sourced user study to determine if our similarity measure mimics human judgement. Finally, we choose latent Dirichl et al.ocation (LDA) as a specific model and instantiate it in a proof-of-concept recommender tool to demonstrate the basic function of our similarity measure.\", \"AuthorNames-Deduped\": \"Michael Oppermann;Robert Kincaid;Tamara Munzner\", \"AuthorNames\": \"Michael Oppermann;Robert Kincaid;Tamara Munzner\", \"AuthorAffiliation\": \"Tableau Research and the University of British Columbia;Tableau Research (retired);University of British Columbia\", \"InternalReferences\": \"10.1109/tvcg.2015.2467757;10.1109/tvcg.2014.2346978;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2007.70577;10.1109/tvcg.2015.2467757\", \"AuthorKeywords\": \"visualization recommendation,content-based filtering,recommender systems,visualization workbook repositories\", \"AminerCitationCount\": 26.0, \"CitationCount_CrossRef\": 28.0, \"PubsCited_CrossRef\": 81.0, \"Downloads_Xplore\": 1243.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 26.0}, {\"Conference\": \"VAST\", \"Year\": 2020, \"Title\": \"Integrating Prior Knowledge in Mixed-Initiative Social Network Clustering\", \"DOI\": \"10.1109/tvcg.2020.3030347\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030347\", \"FirstPage\": 1775.0, \"LastPage\": 1785.0, \"PaperType\": \"J\", \"Abstract\": \"We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.\", \"AuthorNames-Deduped\": \"Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia\", \"AuthorNames\": \"Alexis Pister;Paolo Buono;Jean-Daniel Fekete;Catherine Plaisant;Paola Valdivia\", \"AuthorAffiliation\": \"Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;University of Bari, Italy;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France and University of Maryland, USA;Universit\\u00e9 Paris-Saclay, CNRS, Inria, LRI, France\", \"InternalReferences\": \"10.1109/tvcg.2018.2864477;10.1109/vast.2015.7347625;10.1109/tvcg.2014.2346260;10.1109/tvcg.2006.147;10.1109/tvcg.2017.2745178;10.1109/tvcg.2014.2346248;10.1109/tvcg.2014.2346321;10.1109/tvcg.2017.2745078;10.1109/tvcg.2018.2864477\", \"AuthorKeywords\": \"Social network analysis,network visualization,clustering,mixed-initiative,prior knowledge,user interface\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 17.0, \"PubsCited_CrossRef\": 58.0, \"Downloads_Xplore\": 754.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 13.0}, {\"Conference\": \"SciVis\", \"Year\": 2020, \"Title\": \"Polyphorm: Structural Analysis of Cosmological Datasets via Interactive Physarum Polycephalum Visualization\", \"DOI\": \"10.1109/tvcg.2020.3030407\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030407\", \"FirstPage\": 806.0, \"LastPage\": 816.0, \"PaperType\": \"J\", \"Abstract\": \"This paper introduces Polyphorm, an interactive visualization and model fitting tool that provides a novel approach for investigating cosmological datasets. Through a fast computational simulation method inspired by the behavior of Physarum polycephalum, an unicellular slime mold organism that efficiently forages for nutrients, astrophysicists are able to extrapolate from sparse datasets, such as galaxy maps archived in the Sloan Digital Sky Survey, and then use these extrapolations to inform analyses of a wide range of other data, such as spectroscopic observations captured by the Hubble Space Telescope. Researchers can interactively update the simulation by adjusting model parameters, and then investigate the resulting visual output to form hypotheses about the data. We describe details of Polyphorm's simulation model and its interaction and visualization modalities, and we evaluate Polyphorm through three scientific use cases that demonstrate the effectiveness of our approach.\", \"AuthorNames-Deduped\": \"Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes\", \"AuthorNames\": \"Oskar Elek;Joseph N. Burchett;J. Xavier Prochaska;Angus G. Forbes\", \"AuthorAffiliation\": \"Dept. of Computational Media, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Astronomy and Astrophysics, University of California, Santa Cruz;Dept. of Computational Media, University of California, Santa Cruz\", \"InternalReferences\": \"10.1109/tvcg.2019.2934259;10.1109/tvcg.2019.2934259\", \"AuthorKeywords\": \"Astrophysics visualization,agent-based modeling,intergalactic media,Physarum polycephalum,Cosmic Web\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 10.0, \"PubsCited_CrossRef\": 79.0, \"Downloads_Xplore\": 530.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 13.0}, {\"Conference\": \"SciVis\", \"Year\": 2020, \"Title\": \"IsoTrotter: Visually Guided Empirical Modelling of Atmospheric Convection\", \"DOI\": \"10.1109/tvcg.2020.3030389\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2020.3030389\", \"FirstPage\": 775.0, \"LastPage\": 784.0, \"PaperType\": \"J\", \"Abstract\": \"Empirical models, fitted to data from observations, are often used in natural sciences to describe physical behaviour and support discoveries. However, with more complex models, the regression of parameters quickly becomes insufficient, requiring a visual parameter space analysis to understand and optimize the models. In this work, we present a design study for building a model describing atmospheric convection. We present a mixed-initiative approach to visually guided modelling, integrating an interactive visual parameter space analysis with partial automatic parameter optimization. Our approach includes a new, semi-automatic technique called IsoTrotting, where we optimize the procedure by navigating along isocontours of the model. We evaluate the model with unique observational data of atmospheric convection based on flight trajectories of paragliders.\", \"AuthorNames-Deduped\": \"Juraj P\\u00e1lenik;Thomas Spengler;Helwig Hauser\", \"AuthorNames\": \"Juraj Palenik;Thomas Spengler;Helwig Hauser\", \"AuthorAffiliation\": \"University of Bergen;University of Bergen;University of Bergen\", \"InternalReferences\": \"10.1109/tvcg.2010.190;10.1109/vast.2009.5333431;10.1109/vast.2011.6102450;10.1109/tvcg.2008.139;10.1109/tvcg.2018.2864901;10.1109/tvcg.2014.2346744;10.1109/tvcg.2013.125;10.1109/tvcg.2014.2346578;10.1109/tvcg.2014.2346321;10.1109/tvcg.2012.190;10.1109/visual.1993.398859;10.1109/tvcg.2009.170;10.1109/tvcg.2010.190\", \"AuthorKeywords\": \"visual parameter space exploration,scientific modelling,atmospheric convection\", \"AminerCitationCount\": 1.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 39.0, \"Downloads_Xplore\": 417.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"KG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation\", \"DOI\": \"10.1109/tvcg.2021.3114863\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114863\", \"FirstPage\": 195.0, \"LastPage\": 205.0, \"PaperType\": \"J\", \"Abstract\": \"Visualization recommendation or automatic visualization generation can significantly lower the barriers for general users to rapidly create effective data visualizations, especially for those users without a background in data visualizations. However, existing rule-based approaches require tedious manual specifications of visualization rules by visualization experts. Other machine learning-based approaches often work like black-box and are difficult to understand why a specific visualization is recommended, limiting the wider adoption of these approaches. This paper fills the gap by presenting KG4Vis, a knowledge graph (KG)-based approach for visualization recommendation. It does not require manual specifications of visualization rules and can also guarantee good explainability. Specifically, we propose a framework for building knowledge graphs, consisting of three types of entities (i.e., data features, data columns and visualization design choices) and the relations between them, to model the mapping rules between data and effective visualizations. A TransE-based embedding technique is employed to learn the embeddings of both entities and relations of the knowledge graph from existing dataset-visualization pairs. Such embeddings intrinsically model the desirable visualization rules. Then, given a new dataset, effective visualizations can be inferred from the knowledge graph with semantically meaningful rules. We conducted extensive evaluations to assess the proposed approach, including quantitative comparisons, case studies and expert interviews. The results demonstrate the effectiveness of our approach.\", \"AuthorNames-Deduped\": \"Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu\", \"AuthorNames\": \"Haotian Li;Yong Wang;Songheng Zhang;Yangqiu Song;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology and Singapore Management University, Hong Kong;Singapore Management University, Singapore;Singapore Management University, Singapore;Hong Kong University of Science and Technology, Hong Kong;Hong Kong University of Science and Technology, Hong Kong\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2020.3030469;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2864812;10.1109/tvcg.2018.2865240;10.1109/tvcg.2015.2467091;10.1109/tvcg.2019.2934798;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"Data visualization,Visualization recommendation,Knowledge graph\", \"AminerCitationCount\": 17.0, \"CitationCount_CrossRef\": 69.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 3452.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 17.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Accessible Visualization via Natural Language Descriptions: A Four-Level Model of Semantic Content\", \"DOI\": \"10.1109/tvcg.2021.3114770\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114770\", \"FirstPage\": 1073.0, \"LastPage\": 1083.0, \"PaperType\": \"J\", \"Abstract\": \"Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.\", \"AuthorNames-Deduped\": \"Alan Lundgard;Arvind Satyanarayan\", \"AuthorNames\": \"Alan Lundgard;Arvind Satyanarayan\", \"AuthorAffiliation\": \"MIT CSAIL, USA;MIT CSAIL, USA\", \"InternalReferences\": \"10.1109/tvcg.2020.3030375;10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2013.124;10.1109/tvcg.2011.255;10.1109/vast.2007.4389004;10.1109/tvcg.2016.2598920;10.1109/tvcg.2012.279;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2013.234;10.1109/tvcg.2020.3030375\", \"AuthorKeywords\": \"Visualization,natural language,accessibility,description,caption,semantic,model,theory,alt text,blind,disability\", \"AminerCitationCount\": 24.0, \"CitationCount_CrossRef\": 62.0, \"PubsCited_CrossRef\": 108.0, \"Downloads_Xplore\": 2594.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 24.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Augmenting Sports Videos with VisCommentator\", \"DOI\": \"10.1109/tvcg.2021.3114806\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114806\", \"FirstPage\": 824.0, \"LastPage\": 834.0, \"PaperType\": \"J\", \"Abstract\": \"Visualizing data in sports videos is gaining traction in sports analytics, given its ability to communicate insights and explicate player strategies engagingly. However, augmenting sports videos with such data visualizations is challenging, especially for sports analysts, as it requires considerable expertise in video editing. To ease the creation process, we present a design space that characterizes augmented sports videos at an element-level <i>(what the constituents are)</i> and clip-level <i>(how those constituents are organized)</i>. We do so by systematically reviewing 233 examples of augmented sports videos collected from TV channels, teams, and leagues. The design space guides selection of data insights and visualizations for various purposes. Informed by the design space and close collaboration with domain experts, we design VisCommentator, a fast prototyping tool, to eases the creation of augmented table tennis videos by leveraging machine learning-based data extractors and design space-based visualization recommendations. With VisCommentator, sports analysts can create an augmented video by <i>selecting the data</i> to visualize instead of manually <i>drawing the graphical marks</i>. Our system can be generalized to other racket sports <i>(e.g</i>., tennis, badminton) once the underlying datasets and models are available. A user study with seven domain experts shows high satisfaction with our system, confirms that the participants can reproduce augmented sports videos in a short period, and provides insightful implications into future improvements and opportunities.\", \"AuthorNames-Deduped\": \"Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu\", \"AuthorNames\": \"Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang;Huamin Qu;Yingcai Wu\", \"AuthorAffiliation\": \"Department of Cognitive Science and Design Lab, State Key Lab of CAD & CG, Zhejiang University and Hong Kong University of Science and Technology, University of California, San Diego, United States;State Key Lab of CAD & CG, Zhejiang University, China;State Key Lab of CAD & CG, Zhejiang University, China;Department of Cognitive Science and Design Lab, University of California, San Diego, United States;Department of Sport Science, Zhejiang University, China;Hong Kong University of Science and Technology, Hong Kong;State Key Lab of CAD & CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2019.2934810;10.1109/tvcg.2014.2346250;10.1109/tvcg.2018.2865240;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2017.2745181;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2017.2744218;10.1109/tvcg.2020.3028957;10.1109/tvcg.2020.3030359;10.1109/tvcg.2020.3030392;10.1109/tvcg.2019.2934656;10.1109/tvcg.2020.3030458\", \"AuthorKeywords\": \"Augmented Sports Videos,Video-based Visualization,Sports visualization,Intelligent Design Tool,Storytelling\", \"AminerCitationCount\": 19.0, \"CitationCount_CrossRef\": 42.0, \"PubsCited_CrossRef\": 62.0, \"Downloads_Xplore\": 2151.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 19.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Kori: Interactive Synthesis of Text and Charts in Data Documents\", \"DOI\": \"10.1109/tvcg.2021.3114802\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114802\", \"FirstPage\": 184.0, \"LastPage\": 194.0, \"PaperType\": \"J\", \"Abstract\": \"Charts go hand in hand with text to communicate complex data and are widely adopted in news articles, online blogs, and academic papers. They provide graphical summaries of the data, while text explains the message and context. However, synthesizing information across text and charts is difficult; it requires readers to frequently shift their attention. We investigated ways to support the tight coupling of text and charts in data documents. To understand their interplay, we analyzed the design space of chart-text references through news articles and scientific papers. Informed by the analysis, we developed a mixed-initiative interface enabling users to construct interactive references between text and charts. It leverages natural language processing to automatically suggest references as well as allows users to manually construct other references effortlessly. A user study complemented with algorithmic evaluation of the system suggests that the interface provides an effective way to compose interactive data documents.\", \"AuthorNames-Deduped\": \"Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck 0001;Nam Wook Kim\", \"AuthorNames\": \"Shahid Latif;Zheng Zhou;Yoon Kim;Fabian Beck;Nam Wook Kim\", \"AuthorAffiliation\": \"University of Duisburg-Essen, Germany;Boston College, USA;Harvard University, USA;University of Duisburg-Essen, Germany;Boston College, USA\", \"InternalReferences\": \"10.1109/tvcg.2016.2598647;10.1109/tvcg.2018.2865119;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2016.2598620;10.1109/tvcg.2018.2865022;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2011.183;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2016.2598647\", \"AuthorKeywords\": \"Data-driven storytelling,interaction design,authoring,visualization-text linking,mixed-initiative interface,interactive documents\", \"AminerCitationCount\": 11.0, \"CitationCount_CrossRef\": 34.0, \"PubsCited_CrossRef\": 67.0, \"Downloads_Xplore\": 1308.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 11.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"VizLinter: A Linter and Fixer Framework for Data Visualization\", \"DOI\": \"10.1109/tvcg.2021.3114804\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114804\", \"FirstPage\": 206.0, \"LastPage\": 216.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the rising popularity of automated visualization tools, existing systems tend to provide direct results which do not always fit the input data or meet visualization requirements. Therefore, additional specification adjustments are still required in real-world use cases. However, manual adjustments are difficult since most users do not necessarily possess adequate skills or visualization knowledge. Even experienced users might create imperfect visualizations that involve chart construction errors. We present a framework, VizLinter, to help users detect flaws and rectify already-built but defective visualizations. The framework consists of two components, (1) a visualization linter, which applies well-recognized principles to inspect the legitimacy of rendered visualizations, and (2) a visualization fixer, which automatically corrects the detected violations according to the linter. We implement the framework into an online editor prototype based on Vega-Lite specifications. To further evaluate the system, we conduct an in-lab user study. The results prove its effectiveness and efficiency in identifying and fixing errors for data visualizations.\", \"AuthorNames-Deduped\": \"Qing Chen 0001;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao 0001\", \"AuthorNames\": \"Qing Chen;Fuling Sun;Xinyue Xu;Zui Chen;Jiazhe Wang;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Intelligent Big Data Visualization Lab at Tongji University, China;Ant Group, China;Intelligent Big Data Visualization Lab at Tongji University, China\", \"InternalReferences\": \"10.1109/tvcg.2008.166;10.1109/tvcg.2006.138;10.1109/tvcg.2006.163;10.1109/tvcg.2013.126;10.1109/tvcg.2012.219;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745140;10.1109/infvis.2000.885086;10.1109/tvcg.2020.3030467;10.1109/vast.2009.5332628;10.1109/infvis.2003.1249018;10.1109/tvcg.2018.2864912;10.1109/tvcg.2017.2745919;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2013.234;10.1109/tvcg.2008.166\", \"AuthorKeywords\": \"Visualization Linting,Automated Visualization Design,Visualization Optimization\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 32.0, \"PubsCited_CrossRef\": 64.0, \"Downloads_Xplore\": 1919.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"MultiVision: Designing Analytical Dashboards with Deep Learning Based Recommendation\", \"DOI\": \"10.1109/tvcg.2021.3114826\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114826\", \"FirstPage\": 162.0, \"LastPage\": 172.0, \"PaperType\": \"J\", \"Abstract\": \"We contribute a deep-learning-based method that assists in designing analytical dashboards for analyzing a data table. Given a data table, data workers usually need to experience a tedious and time-consuming process to select meaningful combinations of data columns for creating charts. This process is further complicated by the needs of creating dashboards composed of multiple views that unveil different perspectives of data. Existing automated approaches for recommending multiple-view visualizations mainly build on manually crafted design rules, producing sub-optimal or irrelevant suggestions. To address this gap, we present a deep learning approach for selecting data columns and recommending multiple charts. More importantly, we integrate the deep learning models into a mixed-initiative system. Our model could make recommendations given optional user-input selections of data columns. The model, in turn, learns from provenance data of authoring logs in an offline manner. We compare our deep learning model with existing methods for visualization recommendation and conduct a user study to evaluate the usefulness of the system.\", \"AuthorNames-Deduped\": \"Aoyu Wu;Yun Wang 0012;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang 0001\", \"AuthorNames\": \"Aoyu Wu;Yun Wang;Mengyu Zhou;Xinyi He;Haidong Zhang;Huamin Qu;Dongmei Zhang\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology, Hong Kong and Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Microsoft Research Area, United States;Hong Kong University of Science and Technology, Hong Kong;Microsoft Research Area, United States\", \"InternalReferences\": \"10.1109/tvcg.2020.3030338;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934332;10.1109/tvcg.2018.2865138;10.1109/tvcg.2013.119;10.1109/tvcg.2016.2598620;10.1109/tvcg.2017.2744019;10.1109/tvcg.2018.2865235;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030430;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030387;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744843;10.1109/tvcg.2019.2934798;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423\", \"AuthorKeywords\": \"Visualization Recommendation,Deep Learning,Multiple-View,Dashboard,Mixed-Initiative,Visualization Provenance\", \"AminerCitationCount\": 14.0, \"CitationCount_CrossRef\": 31.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 1788.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"An Evaluation-Focused Framework for Visualization Recommendation Algorithms\", \"DOI\": \"10.1109/tvcg.2021.3114814\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114814\", \"FirstPage\": 346.0, \"LastPage\": 356.0, \"PaperType\": \"J\", \"Abstract\": \"Although we have seen a proliferation of algorithms for recommending visualizations, these algorithms are rarely compared with one another, making it difficult to ascertain which algorithm is best for a given visual analysis scenario. Though several formal frameworks have been proposed in response, we believe this issue persists because visualization recommendation algorithms are inadequately specified from an <i>evaluation</i> perspective. In this paper, we propose an evaluation-focused framework to contextualize and compare a broad range of visualization recommendation algorithms. We present the structure of our framework, where algorithms are specified using three components: (1) a graph representing the full space of possible visualization designs, (2) the method used to traverse the graph for potential candidates for recommendation, and (3) an oracle used to rank candidate designs. To demonstrate how our framework guides the formal comparison of algorithmic performance, we not only theoretically compare five existing representative recommendation algorithms, but also empirically compare four new algorithms generated based on our findings from the theoretical comparison. Our results show that these algorithms behave similarly in terms of user performance, highlighting the need for more rigorous formal comparisons of recommendation algorithms to further clarify their benefits in various analysis scenarios.\", \"AuthorNames-Deduped\": \"Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\", \"AuthorNames\": \"Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\", \"AuthorAffiliation\": \"University of Maryland, United States;University of Maryland, United States;Adobe Research, United States;Adobe Research, United States;Adobe Research, United States and KAIST, South Korea;Adobe Research, United States;Adobe Research, United States;University of Maryland, United States and University of Washington, United States\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2008.137;10.1109/tvcg.2012.219;10.1109/visual.1999.809871;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2007.70577;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191\", \"AuthorKeywords\": \"Visualization Tools,Visualization Recommendation Algorithms\", \"AminerCitationCount\": 13.0, \"CitationCount_CrossRef\": 25.0, \"PubsCited_CrossRef\": 38.0, \"Downloads_Xplore\": 1106.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 13.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Towards Visual Explainable Active Learning for Zero-Shot Classification\", \"DOI\": \"10.1109/tvcg.2021.3114793\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114793\", \"FirstPage\": 791.0, \"LastPage\": 801.0, \"PaperType\": \"J\", \"Abstract\": \"Zero-shot classification is a promising paradigm to solve an applicable problem when the training classes and test classes are disjoint. Achieving this usually needs experts to externalize their domain knowledge by manually specifying a class-attribute matrix to define which classes have which attributes. Designing a suitable class-attribute matrix is the key to the subsequent procedure, but this design process is tedious and trial-and-error with no guidance. This paper proposes a visual explainable active learning approach with its design and implementation called semantic navigator to solve the above problems. This approach promotes human-AI teaming with four actions (ask, explain, recommend, respond) in each interaction loop. The machine asks contrastive questions to guide humans in the thinking process of attributes. A novel visualization called semantic map explains the current status of the machine. Therefore analysts can better understand why the machine misclassifies objects. Moreover, the machine recommends the labels of classes for each attribute to ease the labeling burden. Finally, humans can steer the model by modifying the labels interactively, and the machine adjusts its recommendations. The visual explainable active learning approach improves humans' efficiency of building zero-shot classification models interactively, compared with the method without guidance. We justify our results with user studies using the standard benchmarks for zero-shot classification.\", \"AuthorNames-Deduped\": \"Shichao Jia;Zeyu Li 0003;Nuo Chen;Jiawan Zhang\", \"AuthorNames\": \"Shichao Jia;Zeyu Li;Nuo Chen;Jiawan Zhang\", \"AuthorAffiliation\": \"College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China;College of Intelligence and Computing, Tianjin University, China and Tianjin cultural heritage conservation and inheritance engineering technology center and Key Research Center for Surface Monitoring and Analysis of Relics, State Administration of Cultural Heritage, China\", \"InternalReferences\": \"10.1109/tvcg.2017.2744818;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865047;10.1109/tvcg.2012.260;10.1109/tvcg.2012.277;10.1109/vast.2012.6400492;10.1109/tvcg.2017.2744938;10.1109/tvcg.2016.2598831;10.1109/tvcg.2018.2864843;10.1109/tvcg.2017.2744378;10.1109/vast.2017.8585721;10.1109/tvcg.2018.2864812;10.1109/tvcg.2019.2934267;10.1109/tvcg.2017.2744805;10.1109/tvcg.2017.2744158;10.1109/tvcg.2018.2864504;10.1109/tvcg.2015.2467191;10.1109/vast47406.2019.8986943;10.1109/vast.2012.6400486;10.1109/tvcg.2017.2744818\", \"AuthorKeywords\": \"Active Learning,Explainable Artificial Intelligence,Human-AI Teaming,Mixed-Initiative Visual Analytics\", \"AminerCitationCount\": 7.0, \"CitationCount_CrossRef\": 24.0, \"PubsCited_CrossRef\": 76.0, \"Downloads_Xplore\": 1775.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy\", \"DOI\": \"10.1109/tvcg.2021.3114810\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114810\", \"FirstPage\": 151.0, \"LastPage\": 161.0, \"PaperType\": \"J\", \"Abstract\": \"Although cancer patients survive years after oncologic therapy, they are plagued with long-lasting or permanent residual symptoms, whose severity, rate of development, and resolution after treatment vary largely between survivors. The analysis and interpretation of symptoms is complicated by their partial co-occurrence, variability across populations and across time, and, in the case of cancers that use radiotherapy, by further symptom dependency on the tumor location and prescribed treatment. We describe THALIS, an environment for visual analysis and knowledge discovery from cancer therapy symptom data, developed in close collaboration with oncology experts. Our approach leverages unsupervised machine learning methodology over cohorts of patients, and, in conjunction with custom visual encodings and interactions, provides context for new patients based on patients with similar diagnostic features and symptom evolution. We evaluate this approach on data collected from a cohort of head and neck cancer patients. Feedback from our clinician collaborators indicates that THALIS supports knowledge discovery beyond the limits of machines or humans alone, and that it serves as a valuable tool in both the clinic and symptom research.\", \"AuthorNames-Deduped\": \"Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne van Dijk;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;G. Elisabeta Marai\", \"AuthorNames\": \"Carla Floricel;Nafiul Nipu;Mikayla Biggs;Andrew Wentzel;Guadalupe Canahuate;Lisanne Van Dijk;Abdallah Mohamed;C.David Fuller;G.Elisabeta Marai\", \"AuthorAffiliation\": \"University of Illinois, Chicago, USA;University of Illinois, Chicago, USA;University of Iowa, USA;University of Illinois, Chicago, USA;University of Iowa, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;MD Anderson Cancer Center at the University of Texas, USA;University of Illinois, Chicago, USA\", \"InternalReferences\": \"10.1109/tvcg.2020.3030437;10.1109/tvcg.2011.185;10.1109/tvcg.2018.2864477;10.1109/tvcg.2018.2865043;10.1109/vast.2016.7883512;10.1109/tvcg.2017.2745280;10.1109/tvcg.2014.2346682;10.1109/infvis.1997.636793;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/visual.2005.1532781;10.1109/tvcg.2008.155;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2865027;10.1109/tvcg.2013.161;10.1109/tvcg.2015.2467325;10.1109/tvcg.2020.3030437\", \"AuthorKeywords\": \"Temporal Data,Application Motivated Visualization,Life Sciences,Mixed Initiative Human-Machine Analysis\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 21.0, \"PubsCited_CrossRef\": 105.0, \"Downloads_Xplore\": 815.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"GlyphCreator: Towards Example-based Automatic Generation of Circular Glyphs\", \"DOI\": \"10.1109/tvcg.2021.3114877\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114877\", \"FirstPage\": 400.0, \"LastPage\": 410.0, \"PaperType\": \"J\", \"Abstract\": \"Circular glyphs are used across disparate fields to represent multidimensional data. However, although these glyphs are extremely effective, creating them is often laborious, even for those with professional design skills. This paper presents GlyphCreator, an interactive tool for the example-based generation of circular glyphs. Given an example circular glyph and multidimensional input data, GlyphCreator promptly generates a list of design candidates, any of which can be edited to satisfy the requirements of a particular representation. To develop GlyphCreator, we first derive a design space of circular glyphs by summarizing relationships between different visual elements. With this design space, we build a circular glyph dataset and develop a deep learning model for glyph parsing. The model can deconstruct a circular glyph bitmap into a series of visual elements. Next, we introduce an interface that helps users bind the input data attributes to visual elements and customize visual styles. We evaluate the parsing model through a quantitative experiment, demonstrate the use of GlyphCreator through two use scenarios, and validate its effectiveness through user interviews.\", \"AuthorNames-Deduped\": \"Lu Ying;Tan Tang;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu 0001;Yingcai Wu\", \"AuthorNames\": \"Lu Ying;Tan Tangl;Yuzhe Luo;Lvkeshen Shen;Xiao Xie;Lingyun Yu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China;Department of Sport Science, Zhejiang University, Hangrhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD & CG, Zhejiang University, Hangzhou, China\", \"InternalReferences\": \"10.1109/tvcg.2011.185;10.1109/tvcg.2015.2467196;10.1109/vast.2016.7883517;10.1109/tvcg.2019.2934810;10.1109/infvis.2005.1532140;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934670;10.1109/tvcg.2012.271;10.1109/tvcg.2016.2599378;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2009.191;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.213;10.1109/tvcg.2020.3030403;10.1109/vast.2014.7042494;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030359;10.1109/tvcg.2018.2864825;10.1109/tvcg.2020.3030392;10.1109/tvcg.2020.3030367;10.1109/tvcg.2020.3030458;10.1109/tvcg.2013.234;10.1109/tvcg.2011.185\", \"AuthorKeywords\": \"Glyph-based visualization,machine learning,automatic visualization\", \"AminerCitationCount\": 10.0, \"CitationCount_CrossRef\": 19.0, \"PubsCited_CrossRef\": 73.0, \"Downloads_Xplore\": 1101.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 10.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks\", \"DOI\": \"10.1109/tvcg.2021.3114858\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114858\", \"FirstPage\": 813.0, \"LastPage\": 823.0, \"PaperType\": \"J\", \"Abstract\": \"Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting \\u201cdog faces\\u201d of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting \\u201cdog face\\u201d and \\u201cdog tail\\u201d are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.\", \"AuthorNames-Deduped\": \"Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng (Polo) Chau\", \"AuthorNames\": \"Haekyu Park;Nilaksh Das;Rahul Duggal;Austin P. Wright;Omar Shaikh;Fred Hohman;Duen Horng Polo Chau\", \"AuthorAffiliation\": \"Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Georgia Institute of Technology, United States;Apple, United States;Georgia Institute of Technology, United States\", \"InternalReferences\": \"10.1109/tvcg.2019.2934659;10.1109/tvcg.2019.2934659;10.1109/tvcg.2020.3030461;10.1109/vast.2018.8802509\", \"AuthorKeywords\": \"Deep learning interpretability,visual analytics,scalable summarization,neuron clustering,neuron embedding\", \"AminerCitationCount\": 8.0, \"CitationCount_CrossRef\": 15.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 830.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"VITALITY: Promoting Serendipitous Discovery of Academic Literature with Transformers &amp; Visual Analytics\", \"DOI\": \"10.1109/tvcg.2021.3114820\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114820\", \"FirstPage\": 486.0, \"LastPage\": 496.0, \"PaperType\": \"J\", \"Abstract\": \"There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.\", \"AuthorNames-Deduped\": \"Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall\", \"AuthorNames\": \"Arpit Narechania;Alireza Karduni;Ryan Wesslen;Emily Wall\", \"AuthorAffiliation\": \"Georgia Tech., United States;UNC-Charlotte, United States;UNC-Charlotte, United States;Emory University, United States and Northwestern University, United States\", \"InternalReferences\": \"10.1109/vast.2014.7042493;10.1109/tvcg.2015.2467757;10.1109/tvcg.2018.2865233;10.1109/tvcg.2016.2598594;10.1109/tvcg.2013.162;10.1109/tvcg.2017.2745080;10.1109/vast.2011.6102449;10.1109/tvcg.2017.2746018;10.1109/tvcg.2015.2467621;10.1109/tvcg.2015.2467452;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.175;10.1109/tvcg.2016.2598827;10.1109/tvcg.2021.3114827;10.1109/tvcg.2017.2744478;10.1109/tvcg.2017.2744138;10.1109/vast.2017.8585669;10.1109/tvcg.2021.3114862;10.1109/vast.2014.7042493\", \"AuthorKeywords\": \"transformers,word embeddings,literature review,web scraper,dataset,visual analytics\", \"AminerCitationCount\": 9.0, \"CitationCount_CrossRef\": 15.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 1087.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 9.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"A Mixed-Initiative Approach to Reusing Infographic Charts\", \"DOI\": \"10.1109/tvcg.2021.3114856\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114856\", \"FirstPage\": 173.0, \"LastPage\": 183.0, \"PaperType\": \"J\", \"Abstract\": \"Infographic bar charts have been widely adopted for communicating numerical information because of their attractiveness and memorability. However, these infographics are often created manually with general tools, such as PowerPoint and Adobe Illustrator, and merely composed of primitive visual elements, such as text blocks and shapes. With the absence of chart models, updating or reusing these infographics requires tedious and error-prone manual edits. In this paper, we propose a mixed-initiative approach to mitigate this pain point. On one hand, machines are adopted to perform precise and trivial operations, such as mapping numerical values to shape attributes and aligning shapes. On the other hand, we rely on humans to perform subjective and creative tasks, such as changing embellishments or approving the edits made by machines. We encapsulate our technique in a PowerPoint add-in prototype and demonstrate the effectiveness by applying our technique on a diverse set of infographic bar chart examples.\", \"AuthorNames-Deduped\": \"Weiwei Cui;Jinpeng Wang 0001;He Huang;Yun Wang 0012;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang 0001\", \"AuthorNames\": \"Weiwei Cui;Jinpeng Wang;He Huang;Yun Wang;Chin-Yew Lin;Haidong Zhang;Dongmei Zhang\", \"AuthorAffiliation\": \"Microsoft Research Asia, China;Meituan, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China;Microsoft Research Asia, China\", \"InternalReferences\": \"10.1109/tvcg.2015.2467732;10.1109/tvcg.2019.2934810;10.1109/tvcg.2019.2934785;10.1109/tvcg.2019.2934431;10.1109/tvcg.2016.2598620;10.1109/tvcg.2020.3030360;10.1109/tvcg.2012.229;10.1109/tvcg.2017.2744320;10.1109/tvcg.2020.3030448;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2020.3030403;10.1109/tvcg.2019.2934398;10.1109/tvcg.2020.3030423;10.1109/tvcg.2015.2467732\", \"AuthorKeywords\": \"Infographics,Reusable templates,Graphic design,Automatic visualization\", \"AminerCitationCount\": 4.0, \"CitationCount_CrossRef\": 13.0, \"PubsCited_CrossRef\": 48.0, \"Downloads_Xplore\": 1211.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 4.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"An Automated Approach to Reasoning About Task-Oriented Insights in Responsive Visualization\", \"DOI\": \"10.1109/tvcg.2021.3114782\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114782\", \"FirstPage\": 129.0, \"LastPage\": 139.0, \"PaperType\": \"J\", \"Abstract\": \"Authors often transform a large screen visualization for smaller displays through rescaling, aggregation and other techniques when creating visualizations for both desktop and mobile devices (i.e., responsive visualization). However, transformations can alter relationships or patterns implied by the large screen view, requiring authors to reason carefully about what information to preserve while adjusting their design for the smaller display. We propose an automated approach to approximating the loss of support for task-oriented visualization insights (identification, comparison, and trend) in responsive transformation of a source visualization. We operationalize identification, comparison, and trend loss as objective functions calculated by comparing properties of the rendered source visualization to each realized target (small screen) visualization. To evaluate the utility of our approach, we train machine learning models on human ranked small screen alternative visualizations across a set of source visualizations. We find that our approach achieves an accuracy of 84% (random forest model) in ranking visualizations. We demonstrate this approach in a prototype responsive visualization recommender that enumerates responsive transformations using Answer Set Programming and evaluates the preservation of task-oriented insights using our loss measures. We discuss implications of our approach for the development of automated and semi-automated responsive visualization recommendation.\", \"AuthorNames-Deduped\": \"Hyeok Kim;Ryan A. Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman\", \"AuthorNames\": \"Hyeok Kim;Ryan Rossi;Abhraneel Sarma;Dominik Moritz;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Carnegie Mellon University, USA;Northwestern University, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2018.2865142;10.1109/tvcg.2019.2934397;10.1109/tvcg.2013.124;10.1109/tvcg.2006.161;10.1109/tvcg.2014.2346978;10.1109/tvcg.2011.255;10.1109/tvcg.2013.119;10.1109/tvcg.2013.163;10.1109/tvcg.2014.2346325;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2010.179;10.1109/tvcg.2018.2865145;10.1109/tvcg.2017.2744359;10.1109/tvcg.2019.2934432;10.1109/infvis.2003.1249005;10.1109/tvcg.2020.3030423;10.1109/tvcg.2009.153;10.1109/infvis.2005.1532136\", \"AuthorKeywords\": \"Task-oriented insight preservation,responsive visualization\", \"AminerCitationCount\": 6.0, \"CitationCount_CrossRef\": 9.0, \"PubsCited_CrossRef\": 77.0, \"Downloads_Xplore\": 751.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 6.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Semantic Snapping for Guided Multi-View Visualization Design\", \"DOI\": \"10.1109/tvcg.2021.3114860\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114860\", \"FirstPage\": 43.0, \"LastPage\": 53.0, \"PaperType\": \"J\", \"Abstract\": \"Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is \\u201caligned\\u201d with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.\", \"AuthorNames-Deduped\": \"Yngve Sekse Kristiansen;Laura A. Garrison;Stefan Bruckner\", \"AuthorNames\": \"Yngve S. Kristiansen;Laura Garrison;Stefan Bruckner\", \"AuthorAffiliation\": \"Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway;Department of Informatics, University of Bergen, Norway\", \"InternalReferences\": \"10.1109/tvcg.2020.3030338;10.1109/tvcg.2018.2864907;10.1109/tvcg.2020.3030424;10.1109/tvcg.2010.164;10.1109/tvcg.2016.2598620;10.1109/tvcg.2014.2346325;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2017.2744198;10.1109/tvcg.2014.2346291;10.1109/tvcg.2018.2865158;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.220;10.1109/infvis.2000.885086;10.1109/tvcg.2015.2467191;10.1109/tvcg.2014.2346293;10.1109/tvcg.2020.3030338\", \"AuthorKeywords\": \"Tabular data,guidelines,mixed initiative human-machine analysis,coordinated and multiple views\", \"AminerCitationCount\": 5.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 50.0, \"Downloads_Xplore\": 896.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 5.0}, {\"Conference\": \"Vis\", \"Year\": 2021, \"Title\": \"Visualization Equilibrium\", \"DOI\": \"10.1109/tvcg.2021.3114842\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2021.3114842\", \"FirstPage\": 465.0, \"LastPage\": 474.0, \"PaperType\": \"J\", \"Abstract\": \"In many real-world strategic settings, people use information displays to make decisions. In these settings, an information provider chooses which information to provide to strategic agents and how to present it, and agents formulate a best response based on the information and their anticipation of how others will behave. We contribute the results of a controlled online experiment to examine how the provision and presentation of information impacts people's decisions in a congestion game. Our experiment compares how different visualization approaches for displaying this information, including bar charts and hypothetical outcome plots, and different information conditions, including where the visualized information is private versus public (i.e., available to all agents), affect decision making and welfare. We characterize the effects of visualization anticipation, referring to changes to behavior when an agent goes from alone having access to a visualization to knowing that others also have access to the visualization to guide their decisions. We also empirically identify the visualization equilibrium, i.e., the visualization for which the visualized outcome of agents' decisions matches the realized decisions of the agents who view it. We reflect on the implications of visualization equilibria and visualization anticipation for designing information displays for real-world strategic settings.\", \"AuthorNames-Deduped\": \"Paula Kayongo;Glenn Sun;Jason D. Hartline;Jessica Hullman\", \"AuthorNames\": \"Paula Kayongo;Glenn Sun;Jason Hartline;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;University of California, Los Angeles, USA;Northwestern University, USA;Northwestern University, USA\", \"InternalReferences\": \"10.1109/tvcg.2018.2864907;10.1109/tvcg.2019.2934287;10.1109/tvcg.2011.255;10.1109/tvcg.2020.3030335;10.1109/tvcg.2014.2346325;10.1109/tvcg.2014.2346419;10.1109/infvis.2005.1532122;10.1109/tvcg.2007.70589;10.1109/tvcg.2018.2864907\", \"AuthorKeywords\": \"Visualization equilibrium,Uncertainty visualization,Strategic communication,Nash equilibrium\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 40.0, \"Downloads_Xplore\": 647.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"MetaGlyph: Automatic Generation of Metaphoric Glyph-based Visualization\", \"DOI\": \"10.1109/tvcg.2022.3209447\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209447\", \"FirstPage\": 331.0, \"LastPage\": 341.0, \"PaperType\": \"J\", \"Abstract\": \"Glyph-based visualization achieves an impressive graphic design when associated with comprehensive visual metaphors, which help audiences effectively grasp the conveyed information through revealing data semantics. However, creating such metaphoric glyph-based visualization (MGV) is not an easy task, as it requires not only a deep understanding of data but also professional design skills. This paper proposes MetaGlyph, an automatic system for generating MGVs from a spreadsheet. To develop MetaGlyph, we first conduct a qualitative analysis to understand the design of current MGVs from the perspectives of metaphor embodiment and glyph design. Based on the results, we introduce a novel framework for generating MGVs by metaphoric image selection and an MGV construction. Specifically, MetaGlyph automatically selects metaphors with corresponding images from online resources based on the input data semantics. We then integrate a Monte Carlo tree search algorithm that explores the design of an MGV by associating visual elements with data dimensions given the data importance, semantic relevance, and glyph non-overlap. The system also provides editing feedback that allows users to customize the MGVs according to their design preferences. We demonstrate the use of MetaGlyph through a set of examples, one usage scenario, and validate its effectiveness through a series of expert interviews.\", \"AuthorNames-Deduped\": \"Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu 0001;Yingcai Wu\", \"AuthorNames\": \"Lu Ying;Xinhuan Shu;Dazhen Deng;Yuchen Yang;Tan Tang;Lingyun Yu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China;School of Art and Archaeology, Zhejiang University, Hangzhou, China;Department of Computing, Xi'an Jiaotong-Liverpool University, Suzhou, China;State Key Lab of CAD&CG, Zhejiang University, Hangzhou, China\", \"InternalReferences\": \"10.1109/tvcg.2012.254;10.1109/tvcg.2021.3114792;10.1109/tvcg.2021.3114875;10.1109/tvcg.2022.3209468;10.1109/tvcg.2018.2864769;10.1109/tvcg.2015.2468292;10.1109/tvcg.2016.2598620;10.1109/tvcg.2016.2598432;10.1109/tvcg.2015.2467554;10.1109/tvcg.2014.2346445;10.1109/tvcg.2018.2865158;10.1109/tvcg.2013.206;10.1109/tvcg.2017.2745258;10.1109/tvcg.2020.3030359;10.1109/tvcg.2021.3114877;10.1109/vast50239.2020.00014;10.1109/tvcg.2022.3209360;10.1109/tvcg.2019.2934613;10.1109/tvcg.2014.2346922;10.1109/tvcg.2012.254\", \"AuthorKeywords\": \"Glyph-based visualization,metaphor,machine learning,automatic visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 18.0, \"PubsCited_CrossRef\": 68.0, \"Downloads_Xplore\": 1095.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 18.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"DashBot: Insight-Driven Dashboard Generation Based on Deep Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2022.3209468\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209468\", \"FirstPage\": 690.0, \"LastPage\": 700.0, \"PaperType\": \"J\", \"Abstract\": \"Analytical dashboards are popular in business intelligence to facilitate insight discovery with multiple charts. However, creating an effective dashboard is highly demanding, which requires users to have adequate data analysis background and be familiar with professional tools, such as Power BI. To create a dashboard, users have to configure charts by selecting data columns and exploring different chart combinations to optimize the communication of insights, which is trial-and-error. Recent research has started to use deep learning methods for dashboard generation to lower the burden of visualization creation. However, such efforts are greatly hindered by the lack of large-scale and high-quality datasets of dashboards. In this work, we propose using deep reinforcement learning to generate analytical dashboards that can use well-established visualization knowledge and the estimation capacity of reinforcement learning. Specifically, we use visualization knowledge to construct a training environment and rewards for agents to explore and imitate human exploration behavior with a well-designed agent network. The usefulness of the deep reinforcement learning model is demonstrated through ablation studies and user studies. In conclusion, our work opens up new opportunities to develop effective ML-based visualization recommenders without beforehand training datasets.\", \"AuthorNames-Deduped\": \"Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu\", \"AuthorNames\": \"Dazhen Deng;Aoyu Wu;Huamin Qu;Yingcai Wu\", \"AuthorAffiliation\": \"State Key Lab of CAD&CG, Zhejiang University, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;State Key Lab of CAD&CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2013.234;10.1109/tvcg.2021.3114804;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030376;10.1109/tvcg.2020.3030462;10.1109/tvcg.2021.3114863;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2016.2599030;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2020.3030396;10.1109/tvcg.2018.2865145;10.1109/tvcg.2020.3030467;10.1109/tvcg.2018.2864899;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114877;10.1109/tvcg.2022.3209447;10.1109/tvcg.2016.2598497;10.1109/tvcg.2021.3114814;10.1109/tvcg.2022.3209360;10.1109/tvcg.2022.3209448;10.1109/tvcg.2013.234\", \"AuthorKeywords\": \"Reinforcement Learning,Visualization Recommendation,Multiple-View Visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 14.0, \"PubsCited_CrossRef\": 83.0, \"Downloads_Xplore\": 1671.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 14.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"Diverse Interaction Recommendation for Public Users Exploring Multi-view Visualization using Deep Learning\", \"DOI\": \"10.1109/tvcg.2022.3209461\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209461\", \"FirstPage\": 95.0, \"LastPage\": 105.0, \"PaperType\": \"J\", \"Abstract\": \"Interaction is an important channel to offer users insights in interactive visualization systems. However, which interaction to operate and which part of data to explore are hard questions for public users facing a multi-view visualization for the first time. Making these decisions largely relies on professional experience and analytic abilities, which is a huge challenge for non-professionals. To solve the problem, we propose a method aiming to provide diverse, insightful, and real-time interaction recommendations for novice users. Building on the Long-Short Term Memory Model (LSTM) structure, our model captures users' interactions and visual states and encodes them in numerical vectors to make further recommendations. Through an illustrative example of a visualization system about Chinese poets in the museum scenario, the model is proven to be workable in systems with multi-views and multiple interaction types. A further user study demonstrates the method's capability to help public users conduct more insightful and diverse interactive explorations and gain more accurate data insights.\", \"AuthorNames-Deduped\": \"Yixuan Li;Yusheng Qi;Yang Shi 0007;Qing Chen 0001;Nan Cao 0001;Siming Chen 0001\", \"AuthorNames\": \"Yixuan Li;Yusheng Qi;Yang Shi;Qing Chen;Nan Cao;Siming Chen\", \"AuthorAffiliation\": \"School of Data Science, Fudan University, China;School of Data Science, Fudan University, China;Tongji University, China;Tongji University, China;Tongji University, China;School of Data Science, Fudan University, China\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467871;10.1109/tvcg.2015.2467201;10.1109/tvcg.2014.2346575;10.1109/tvcg.2016.2598468;10.1109/infvis.1996.559213;10.1109/tvcg.2016.2598471;10.1109/tvcg.2019.2934283;10.1109/vast.2008.4677365;10.1109/tvcg.2015.2467613;10.1109/tvcg.2008.127;10.1109/tvcg.2012.244;10.1109/tvcg.2016.2599030;10.1109/tvcg.2015.2467091;10.1109/tvcg.2007.70589;10.1109/tvcg.2021.3114826;10.1109/tvcg.2007.70515;10.1109/tvcg.2016.2598543\", \"AuthorKeywords\": \"Interaction Recommendation,Visualization for public education,Mixed-initiative Exploration\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 1276.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"MEDLEY: Intent-based Recommendations to Support Dashboard Composition\", \"DOI\": \"10.1109/tvcg.2022.3209421\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209421\", \"FirstPage\": 1135.0, \"LastPage\": 1145.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the ever-growing popularity of dashboards across a wide range of domains, their authoring still remains a tedious and complex process. Current tools offer considerable support for creating individual visualizations but provide limited support for discovering groups of visualizations that can be collectively useful for composing analytic dashboards. To address this problem, we present Medley, a mixed-initiative interface that assists in dashboard composition by recommending dashboard collections (i.e., a logically grouped set of views and filtering widgets) that map to specific analytical intents. Users can specify dashboard intents (namely, measure analysis, change analysis, category analysis, or distribution analysis) explicitly through an input panel in the interface or implicitly by selecting data attributes and views of interest. The system recommends collections based on these analytic intents, and views and widgets can be selected to compose a variety of dashboards. Medley also provides a lightweight direct manipulation interface to configure interactions between views in a dashboard. Based on a study with 13 participants performing both targeted and open-ended tasks, we discuss how Medley's recommendations guide dashboard composition and facilitate different user workflows. Observations from the study identify potential directions for future work, including combining manual view specification with dashboard recommendations and designing natural language interfaces for dashboard authoring.\", \"AuthorNames-Deduped\": \"Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\", \"AuthorNames\": \"Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\", \"AuthorAffiliation\": \"Northeastern University, USA;Tableau Research, Germany;Tableau Research, Germany\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2013.124;10.1109/tvcg.2020.3030338;10.1109/tvcg.2020.3030424;10.1109/tvcg.2021.3114860;10.1109/tvcg.2021.3114848;10.1109/tvcg.2007.70594;10.1109/tvcg.2020.3030378;10.1109/tvcg.2017.2744198;10.1109/tvcg.2018.2864903;10.1109/tvcg.2017.2744184;10.1109/tvcg.2016.2599030;10.1109/tvcg.2013.120;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/tvcg.2021.3114826\", \"AuthorKeywords\": \"Dashboards,intent,recommendations,direct manipulation,multi-view coordination\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 8.0, \"PubsCited_CrossRef\": 55.0, \"Downloads_Xplore\": 1537.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 8.0}, {\"Conference\": \"Vis\", \"Year\": 2022, \"Title\": \"GenoREC: A Recommendation System for Interactive Genomics Data Visualization\", \"DOI\": \"10.1109/tvcg.2022.3209407\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2022.3209407\", \"FirstPage\": 570.0, \"LastPage\": 580.0, \"PaperType\": \"J\", \"Abstract\": \"Interpretation of genomics data is critically reliant on the application of a wide range of visualization tools. A large number of visualization techniques for genomics data and different analysis tasks pose a significant challenge for analysts: which visualization technique is most likely to help them generate insights into their data? Since genomics analysts typically have limited training in data visualization, their choices are often based on trial and error or guided by technical details, such as data formats that a specific tool can load. This approach prevents them from making effective visualization choices for the many combinations of data types and analysis questions they encounter in their work. Visualization recommendation systems assist non-experts in creating data visualization by recommending appropriate visualizations based on the data and task characteristics. However, existing visualization recommendation systems are not designed to handle domain-specific problems. To address these challenges, we designed GenoREC, a novel visualization recommendation system for genomics. GenoREC enables genomics analysts to select effective visualizations based on a description of their data and analysis tasks. Here, we present the recommendation model that uses a knowledge-based method for choosing appropriate visualizations and a web application that enables analysts to input their requirements, explore recommended visualizations, and export them for their usage. Furthermore, we present the results of two user studies demonstrating that GenoREC recommends visualizations that are both accepted by domain experts and suited to address the given genomics analysis problem. All supplemental materials are available at https://osf.io/y73pt/.\", \"AuthorNames-Deduped\": \"Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg\", \"AuthorNames\": \"Aditeya Pandey;Sehi L'Yi;Qianwen Wang;Michelle A. Borkin;Nils Gehlenborg\", \"AuthorAffiliation\": \"Northeastern University, MA, US;Harvard Medical School, MA, US;Harvard Medical School, MA, US;Northeastern University, MA, US;Harvard Medical School, MA, US\", \"InternalReferences\": \"10.1109/tvcg.2013.234;10.1109/tvcg.2013.124;10.1109/tvcg.2021.3114860;10.1109/tvcg.2022.3209398;10.1109/tvcg.2020.3030419;10.1109/tvcg.2021.3114876;10.1109/tvcg.2007.70594;10.1109/tvcg.2009.167;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2865240;10.1109/tvcg.2017.2744198;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423;10.1109/tvcg.2021.3114814;10.1109/tvcg.2013.234\", \"AuthorKeywords\": \"genomics,visualization,recommendation systems,data,tasks\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 62.0, \"Downloads_Xplore\": 2485.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation of User Feedback\", \"DOI\": \"10.1109/tvcg.2023.3327363\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327363\", \"FirstPage\": 131.0, \"LastPage\": 141.0, \"PaperType\": \"J\", \"Abstract\": \"Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.\", \"AuthorNames-Deduped\": \"Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh\", \"AuthorNames\": \"Guande Wu;Shunan Guo;Jane Hoffswell;Gromit Yeuk-Yin Chan;Ryan A. Rossi;Eunyee Koh\", \"AuthorAffiliation\": \"New York University, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA;Adobe Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2016.2598647;10.1109/tvcg.2015.2467732;10.1109/tvcg.2011.185;10.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114806;10.1109/vast.2015.7347625;10.1109/tvcg.2019.2934785;10.1109/tvcg.2012.260;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2022.3209421;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209428;10.1109/tvcg.2020.3030467;10.1109/tvcg.2017.2745078;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774\", \"AuthorKeywords\": \"Narrative visualization,visual storytelling,conversational agent\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 7.0, \"PubsCited_CrossRef\": 79.0, \"Downloads_Xplore\": 816.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 7.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"InkSight: Leveraging Sketch Interaction for Documenting Chart Findings in Computational Notebooks\", \"DOI\": \"10.1109/tvcg.2023.3327170\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327170\", \"FirstPage\": 944.0, \"LastPage\": 954.0, \"PaperType\": \"J\", \"Abstract\": \"Computational notebooks have become increasingly popular for exploratory data analysis due to their ability to support data exploration and explanation within a single document. Effective documentation for explaining chart findings during the exploration process is essential as it helps recall and share data analysis. However, documenting chart findings remains a challenge due to its time-consuming and tedious nature. While existing automatic methods alleviate some of the burden on users, they often fail to cater to users' specific interests. In response to these limitations, we present InkSight, a mixed-initiative computational notebook plugin that generates finding documentation based on the user's intent. InkSight allows users to express their intent in specific data subsets through sketching atop visualizations intuitively. To facilitate this, we designed two types of sketches, i.e., open-path and closed-path sketch. Upon receiving a user's sketch, InkSight identifies the sketch type and corresponding selected data items. Subsequently, it filters data fact types based on the sketch and selected data items before employing existing automatic data fact recommendation algorithms to infer data facts. Using large language models (GPT-3.5), InkSight converts data facts into effective natural language documentation. Users can conveniently fine-tune the generated documentation within InkSight. A user study with 12 participants demonstrated the usability and effectiveness of InkSight in expressing user intent and facilitating chart finding documentation.\", \"AuthorNames-Deduped\": \"Yanna Lin;Haotian Li 0001;Leni Yang;Aoyu Wu;Huamin Qu\", \"AuthorNames\": \"Yanna Lin;Haotian Li;Leni Yang;Aoyu Wu;Huamin Qu\", \"AuthorAffiliation\": \"Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Hong Kong University of Science and Technology, China;Harvard University, USA;Hong Kong University of Science and Technology, China\", \"InternalReferences\": \"0.1109/tvcg.2019.2934785;10.1109/tvcg.2021.3114802;10.1109/tvcg.2013.191;10.1109/tvcg.2020.3030378;10.1109/tvcg.2022.3209421;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2012.275;10.1109/tvcg.2022.3209357;10.1109/tvcg.2019.2934398;10.1109/tvcg.2021.3114826;10.1109/tvcg.2021.3114774;10.1109/tvcg.2019.2934668\", \"AuthorKeywords\": \"Computational Notebook,Sketch-based Interaction,Documentation,Visualization,Exploratory Data Analysis\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 5.0, \"PubsCited_CrossRef\": 58.0, \"Downloads_Xplore\": 665.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 5.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Mystique: Deconstructing SVG Charts for Layout Reuse\", \"DOI\": \"10.1109/tvcg.2023.3327354\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327354\", \"FirstPage\": 447.0, \"LastPage\": 457.0, \"PaperType\": \"J\", \"Abstract\": \"To facilitate the reuse of existing charts, previous research has examined how to obtain a semantic understanding of a chart by deconstructing its visual representation into reusable components, such as encodings. However, existing deconstruction approaches primarily focus on chart styles, handling only basic layouts. In this paper, we investigate how to deconstruct chart layouts, focusing on rectangle-based ones, as they cover not only 17 chart types but also advanced layouts (e.g., small multiples, nested layouts). We develop an interactive tool, called Mystique, adopting a mixed-initiative approach to extract the axes and legend, and deconstruct a chart's layout into four semantic components: mark groups, spatial relationships, data encodings, and graphical constraints. Mystique employs a wizard interface that guides chart authors through a series of steps to specify how the deconstructed components map to their own data. On 150 rectangle-based SVG charts, Mystique achieves above 85% accuracy for axis and legend extraction and 96% accuracy for layout deconstruction. In a chart reproduction study, participants could easily reuse existing charts on new datasets. We discuss the current limitations of Mystique and future research directions.\", \"AuthorNames-Deduped\": \"Chen Chen 0080;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu 0001\", \"AuthorNames\": \"Chen Chen;Bongshin Lee;Yunhai Wang;Yunjeong Chang;Zhicheng Liu\", \"AuthorAffiliation\": \"University of Maryland, College Park, Maryland, United States;Microsoft Research, Redmond, Washington, United States;Shandong University, Qingdao, China;University of Maryland, College Park, Maryland, United States;University of Maryland, College Park, Maryland, United States\", \"InternalReferences\": \"0.1109/tvcg.2022.3209490;10.1109/tvcg.2011.185;10.1109/tvcg.2019.2934810;10.1109/tvcg.2021.3114856;10.1109/tvcg.2017.2744320;10.1109/tvcg.2018.2865158;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/infvis.2001.963283;10.1109/tvcg.2019.2934538;10.1109/tvcg.2008.165;10.1109/tvcg.2021.3114877\", \"AuthorKeywords\": \"Chart layout,Reuse,Reverse-engineering,Deconstruction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 47.0, \"Downloads_Xplore\": 481.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Supporting Guided Exploratory Visual Analysis on Time Series Data with Reinforcement Learning\", \"DOI\": \"10.1109/tvcg.2023.3327200\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3327200\", \"FirstPage\": 1172.0, \"LastPage\": 1182.0, \"PaperType\": \"J\", \"Abstract\": \"The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.\", \"AuthorNames-Deduped\": \"Yang Shi 0007;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao 0001\", \"AuthorNames\": \"Yang Shi;Bingchang Chen;Ying Chen;Zhuochen Jin;Ke Xu;Xiaohan Jiao;Tian Gao;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Huawei Cloud Computing Technologies Co., Ltd., China;Huawei Cloud Computing Technologies Co., Ltd., China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China\", \"InternalReferences\": \"0.1109/tvcg.2018.2865040;10.1109/vast.2014.7042480;10.1109/tvcg.2016.2598876;10.1109/tvcg.2016.2598468;10.1109/tvcg.2022.3209468;10.1109/tvcg.2021.3114875;10.1109/tvcg.2020.3028889;10.1109/tvcg.2018.2865077;10.1109/tvcg.2012.229;10.1109/tvcg.2018.2864526;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030403;10.1109/tvcg.2022.3209409;10.1109/tvcg.2022.3209486;10.1109/tvcg.2012.191;10.1109/tvcg.2018.2865145;10.1109/tvcg.2015.2467751;10.1109/tvcg.2019.2934398;10.1109/tvcg.2015.2467191;10.1109/vast.2009.5332595;10.1109/tvcg.2021.3114826;10.1109/tvcg.2023.3326913;10.1109/tvcg.2021.3114774;10.1109/tvcg.2011.195;10.1109/tvcg.2021.3114865\", \"AuthorKeywords\": \"Time Series Data,Exploratory Visual Analysis,Reinforcement Learning\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 77.0, \"Downloads_Xplore\": 1050.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Roses Have Thorns: Understanding the Downside of Oncological Care Delivery Through Visual Analytics and Sequential Rule Mining\", \"DOI\": \"10.1109/tvcg.2023.3326939\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326939\", \"FirstPage\": 1227.0, \"LastPage\": 1237.0, \"PaperType\": \"J\", \"Abstract\": \"Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.\", \"AuthorNames-Deduped\": \"Carla Floricel;Andrew Wentzel;Abdallah Sherif Radwan Mohamed;Clifton David Fuller;Guadalupe Canahuate;G. Elisabeta Marai\", \"AuthorNames\": \"Carla Floricel;Andrew Wentzel;Abdallah Mohamed;C.David Fuller;Guadalupe Canahuate;G.Elisabeta Marai\", \"AuthorAffiliation\": \"University of Illinois Chicago, USA;University of Illinois Chicago, USA;M.D. Anderson Cancer Center at the University of Texas, USA;M.D. Anderson Cancer Center at the University of Texas, USA;University of Iowa, USA;University of Illinois Chicago, USA\", \"InternalReferences\": \"0.1109/tvcg.2020.3030437;10.1109/tvcg.2017.2745278;10.1109/tvcg.2020.3030442;10.1109/vast.2016.7883512;10.1109/tvcg.2021.3114810;10.1109/tvcg.2014.2346682;10.1109/tvcg.2017.2745320;10.1109/tvcg.2014.2346591;10.1109/tvcg.2018.2864849;10.1109/tvcg.2017.2744459;10.1109/tvcg.2013.161;10.1109/tvcg.2018.2864812;10.1109/tvcg.2013.200;10.1109/tvcg.2021.3114840;10.1109/tvcg.2009.187;10.1109/tvcg.2019.2934546;10.1109/tvcg.2018.2864475\", \"AuthorKeywords\": \"Temporal Data,Life Sciences,Mixed Initiative Human-Machine Analysis,Data Clustering and Aggregation\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 82.0, \"Downloads_Xplore\": 361.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Too Many Cooks: Exploring How Graphical Perception Studies Influence Visualization Recommendations in Draco\", \"DOI\": \"10.1109/tvcg.2023.3326527\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326527\", \"FirstPage\": 1063.0, \"LastPage\": 1073.0, \"PaperType\": \"J\", \"Abstract\": \"Findings from graphical perception can guide visualization recommendation algorithms in identifying effective visualization designs. However, existing algorithms use knowledge from, at best, a few studies, limiting our understanding of how complementary (or contradictory) graphical perception results influence generated recommendations. In this paper, we present a pipeline of applying a large body of graphical perception results to develop new visualization recommendation algorithms and conduct an exploratory study to investigate how results from graphical perception can alter the behavior of downstream algorithms. Specifically, we model graphical perception results from 30 papers in Draco\\u2014a framework to model visualization knowledge\\u2014to develop new recommendation algorithms. By analyzing Draco-generated algorithms, we showcase the feasibility of our method to (1) identify gaps in existing graphical perception literature informing recommendation algorithms, (2) cluster papers by their preferred design rules and constraints, and (3) investigate why certain studies can dominate Draco's recommendations, whereas others may have little influence. Given our findings, we discuss the potential for mutually reinforcing advancements in graphical perception and visualization recommendation research.\", \"AuthorNames-Deduped\": \"Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle\", \"AuthorNames\": \"Zehua Zeng;Junran Yang;Dominik Moritz;Jeffrey Heer;Leilani Battle\", \"AuthorAffiliation\": \"University of Maryland, College Park, USA;University of Washington, Seattle, USA;Carnegie Mellon University, United States;University of Washington, Seattle, USA;University of Washington, Seattle, USA\", \"InternalReferences\": \"0.1109/tvcg.2017.2745086;10.1109/tvcg.2018.2865077;10.1109/tvcg.2019.2934786;10.1109/tvcg.2021.3114863;10.1109/tvcg.2007.70594;10.1109/tvcg.2021.3114684;10.1109/tvcg.2018.2865240;10.1109/tvcg.2018.2864884;10.1109/tvcg.2019.2934807;10.1109/tvcg.2018.2865264;10.1109/tvcg.2016.2599030;10.1109/tvcg.2014.2346320;10.1109/tvcg.2019.2934784;10.1109/tvcg.2015.2467191;10.1109/tvcg.2019.2934400;10.1109/tvcg.2021.3114814\", \"AuthorKeywords\": \"Graphical Perception Studies,Visualization Recommendation Algorithms\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 51.0, \"Downloads_Xplore\": 371.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"The Rational Agent Benchmark for Data Visualization\", \"DOI\": \"10.1109/tvcg.2023.3326513\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326513\", \"FirstPage\": 338.0, \"LastPage\": 347.0, \"PaperType\": \"J\", \"Abstract\": \"Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.\", \"AuthorNames-Deduped\": \"Yifan Wu 0005;Ziyang Guo;Michalis Mamakos;Jason D. Hartline;Jessica Hullman\", \"AuthorNames\": \"Yifan Wu;Ziyang Guo;Michalis Mamakos;Jason Hartline;Jessica Hullman\", \"AuthorAffiliation\": \"Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA;Northwestern University, USA\", \"InternalReferences\": \"0.1109/tvcg.2021.3114813;10.1109/tvcg.2020.3030395;10.1109/tvcg.2019.2934287;10.1109/tvcg.2018.2864889;10.1109/tvcg.2013.126;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3030335;10.1109/tvcg.2021.3114824;10.1109/tvcg.2020.3028984;10.1109/tvcg.2009.111;10.1109/visual.2005.1532781\", \"AuthorKeywords\": \"Evaluation,decision-making,rational agent,scoring rule\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 2.0, \"PubsCited_CrossRef\": 33.0, \"Downloads_Xplore\": 434.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 2.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Calliope-Net: Automatic Generation of Graph Data Facts via Annotated Node-Link Diagrams\", \"DOI\": \"10.1109/tvcg.2023.3326925\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326925\", \"FirstPage\": 562.0, \"LastPage\": 572.0, \"PaperType\": \"J\", \"Abstract\": \"Graph or network data are widely studied in both data mining and visualization communities to review the relationship among different entities and groups. The data facts derived from graph visual analysis are important to help understand the social structures of complex data, especially for data journalism. However, it is challenging for data journalists to discover graph data facts and manually organize correlated facts around a meaningful topic due to the complexity of graph data and the difficulty to interpret graph narratives. Therefore, we present an automatic graph facts generation system, Calliope-Net, which consists of a fact discovery module, a fact organization module, and a visualization module. It creates annotated node-link diagrams with facts automatically discovered and organized from network data. A novel layout algorithm is designed to present meaningful and visually appealing annotated graphs. We evaluate the proposed system with two case studies and an in-lab user study. The results show that Calliope-Net can benefit users in discovering and understanding graph data facts with visually pleasing annotated visualizations.\", \"AuthorNames-Deduped\": \"Qing Chen 0001;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu 0007;Hanghang Tong;Nan Cao 0001\", \"AuthorNames\": \"Qing Chen;Nan Chen;Wei Shuai;Guande Wu;Zhe Xu;Hanghang Tong;Nan Cao\", \"AuthorAffiliation\": \"Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;Intelligent Big Data Visualization Lab, Tongji University, China;New York University, USA;University of Illinois at Urbana-Champaign, USA;University of Illinois at Urbana-Champaign, USA;Intelligent Big Data Visualization Lab, Tongji University, China\", \"InternalReferences\": \"0.1109/tvcg.2016.2598876;10.1109/tvcg.2019.2934810;10.1109/tvcg.2013.119;10.1109/tvcg.2021.3114802;10.1109/tvcg.2017.2743858;10.1109/tvcg.2010.179;10.1109/tvcg.2020.3030403;10.1109/tvcg.2018.2865145;10.1109/tvcg.2019.2934398;10.1109/tvcg.2017.2745919;10.1109/tvcg.2020.3030428\", \"AuthorKeywords\": \"Graph Data,Application Motivated Visualization,Automatic Visualization,Narrative Visualization,Authoring Tools\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 78.0, \"Downloads_Xplore\": 662.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Dupo: A Mixed-Initiative Authoring Tool for Responsive Visualization\", \"DOI\": \"10.1109/tvcg.2023.3326583\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326583\", \"FirstPage\": 934.0, \"LastPage\": 943.0, \"PaperType\": \"J\", \"Abstract\": \"Designing responsive visualizations for various screen types can be tedious as authors must manage multiple chart versions across design iterations. Automated approaches for responsive visualization must take into account the user's need for agency in exploring possible design ideas and applying customizations based on their own goals. We design and implement Dupo, a mixedinitiative approach to creating responsive visualizations that combines the agency afforded by a manual interface with automation provided by a recommender system. Given an initial design, users can browse automated design suggestions for a different screen type and make edits to a chosen design, thereby supporting quick prototyping and customizability. Dupo employs a two-step recommender pipeline that first suggests significant design changes (Exploration) followed by more subtle changes (Alteration). We evaluated Dupo with six expert responsive visualization authors. While creating responsive versions of a source design in Dupo, participants could reason about different design suggestions without having to manually prototype them, and thus avoid prematurely fixating on a particular design. This process led participants to create designs that they were satisfied with but which they had previously overlooked.\", \"AuthorNames-Deduped\": \"Hyeok Kim;Ryan A. Rossi;Jessica Hullman;Jane Hoffswell\", \"AuthorNames\": \"Hyeok Kim;Ryan Rossi;Jessica Hullman;Jane Hoffswell\", \"AuthorAffiliation\": \"Northwestern University, USA;Adobe Research, USA;Northwestern University, USA;Adobe Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2011.185;10.1109/vast.2015.7347625;10.1109/tvcg.2021.3114856;10.1109/tvcg.2006.138;10.1109/tvcg.2021.3114782;10.1109/tvcg.2017.2744198;10.1109/tvcg.2016.2599030;10.1109/tvcg.2017.2745078;10.1109/tvcg.2015.2467191;10.1109/tvcg.2020.3030423\", \"AuthorKeywords\": \"Visualization,responsive visualization,mixed-initiative authoring\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 45.0, \"Downloads_Xplore\": 330.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Visual Analytics for Understanding Draco's Knowledge Base\", \"DOI\": \"10.1109/tvcg.2023.3326912\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326912\", \"FirstPage\": 392.0, \"LastPage\": 402.0, \"PaperType\": \"J\", \"Abstract\": \"Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.\", \"AuthorNames-Deduped\": \"Johanna Schmidt;Bernhard Pointner;Silvia Miksch\", \"AuthorNames\": \"Johanna Schmidt;Bernhard Pointner;Silvia Miksch\", \"AuthorAffiliation\": \"VRVis Zentrum f\\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;VRVis Zentrum f\\u00fcr Virtual Reality und visualisierung Forschungs-GmbH, Austria;Centre for Visual Analytics Science and Technology (CVAST), TU Wien, Austria\", \"InternalReferences\": \"0.1109/tvcg.2013.184;10.1109/tvcg.2007.70582;10.1109/tvcg.2007.70594;10.1109/tvcg.2018.2865240;10.1109/tvcg.2009.111;10.1109/tvcg.2016.2599030;10.1109/infvis.2000.885091;10.1109/tvcg.2018.2865146\", \"AuthorKeywords\": \"Visual Analytics,Hypergraph visualization,Rule-based recommendation systems\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 53.0, \"Downloads_Xplore\": 365.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Data Formulator: AI-Powered Concept-Driven Visualization Authoring\", \"DOI\": \"10.1109/tvcg.2023.3326585\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326585\", \"FirstPage\": 1128.0, \"LastPage\": 1138.0, \"PaperType\": \"J\", \"Abstract\": \"With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.\", \"AuthorNames-Deduped\": \"Chenglong Wang;John Thompson 0002;Bongshin Lee\", \"AuthorNames\": \"Chenglong Wang;John Thompson;Bongshin Lee\", \"AuthorAffiliation\": \"Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA\", \"InternalReferences\": \"0.1109/tvcg.2021.3114830;10.1109/tvcg.2009.174;10.1109/tvcg.2011.185;10.1109/tvcg.2021.3114848;10.1109/tvcg.2018.2865240;10.1109/tvcg.2020.3030378;10.1109/tvcg.2018.2865158;10.1109/tvcg.2016.2598839;10.1109/tvcg.2019.2934281;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030476;10.1109/tvcg.2015.2467191;10.1109/tvcg.2022.3209470;10.1109/tvcg.2020.3030367;10.1109/tvcg.2022.3209369\", \"AuthorKeywords\": \"AI,visualization authoring,data transformation,programming by example,natural language,large language model\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 63.0, \"Downloads_Xplore\": 893.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2023, \"Title\": \"Guided Visual Analytics for Image Selection in Time and Space\", \"DOI\": \"10.1109/tvcg.2023.3326572\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2023.3326572\", \"FirstPage\": 66.0, \"LastPage\": 75.0, \"PaperType\": \"J\", \"Abstract\": \"Unexploded Ordnance (UXO) detection, the identification of remnant active bombs buried underground from archival aerial images, implies a complex workflow involving decision-making at each stage. An essential phase in UXO detection is the task of image selection, where a small subset of images must be chosen from archives to reconstruct an area of interest (AOI) and identify craters. The selected image set must comply with good spatial and temporal coverage over the AOI, particularly in the temporal vicinity of recorded aerial attacks, and do so with minimal images for resource optimization. This paper presents a guidance-enhanced visual analytics prototype to select images for UXO detection. In close collaboration with domain experts, our design process involved analyzing user tasks, eliciting expert knowledge, modeling quality metrics, and choosing appropriate guidance. We report on a user study with two real-world scenarios of image selection performed with and without guidance. Our solution was well-received and deemed highly usable. Through the lens of our task-based design and developed quality measures, we observed guidance-driven changes in user behavior and improved quality of analysis results. An expert evaluation of the study allowed us to improve our guidance-enhanced prototype further and discuss new possibilities for user-adaptive guidance.\", \"AuthorNames-Deduped\": \"Ignacio P\\u00e9rez-Messina;Davide Ceneda;Silvia Miksch\", \"AuthorNames\": \"Ignacio P\\u00e9rez-Messina;Davide Ceneda;Silvia Miksch\", \"AuthorAffiliation\": \"TU Wien, Austria;TU Wien, Austria;TU Wien, Austria\", \"InternalReferences\": \"0.1109/tvcg.2013.124;10.1109/tvcg.2016.2598468;10.1109/tvcg.2021.3114813;10.1109/tvcg.2018.2864769;10.1109/vast.2017.8585498;10.1109/tvcg.2011.231;10.1109/tvcg.2017.2744418;10.1109/tvcg.2020.3030364;10.1109/tvcg.2014.2346481;10.1109/tvcg.2014.2346321;10.1109/tvcg.2022.3209393;10.1109/vast47406.2019.8986917;10.1109/tvcg.2019.2934658;10.1109/tvcg.2018.2865146\", \"AuthorKeywords\": \"Application Motivated Visualization,Geospatial Data,Mixed Initiative Human-Machine Analysis,Process/Workflow Design,Task Abstractions & Application Domains,Temporal Data\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 37.0, \"Downloads_Xplore\": 1208.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts\", \"DOI\": \"10.1109/tvcg.2024.3456398\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456398\", \"FirstPage\": 481.0, \"LastPage\": 491.0, \"PaperType\": \"J\", \"Abstract\": \"Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\", \"AuthorNames-Deduped\": \"Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma\", \"AuthorNames\": \"Sam Yu-Te Lee;Aryaman Bahukhandi;Dongyu Liu;Kwan-Liu Ma\", \"AuthorAffiliation\": \"University of California, USA;University of California, USA;University of California, USA;University of California, USA\", \"InternalReferences\": \"10.1109/tvcg.2017.2743858;10.1109/tvcg.2017.2744938;10.1109/tvcg.2017.2744358;10.1109/tvcg.2015.2467112;10.1109/tvcg.2017.2744158;10.1109/tvcg.2023.3326585;10.1109/tvcg.2017.2744878\", \"AuthorKeywords\": \"Visual analytics,prompt engineering,,,text summarization,human-computer interaction,dimensionality reduction\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 65.0, \"Downloads_Xplore\": 386.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"KNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration\", \"DOI\": \"10.1109/tvcg.2024.3456364\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456364\", \"FirstPage\": 547.0, \"LastPage\": 557.0, \"PaperType\": \"J\", \"Abstract\": \"The increasing reliance on Large Language Models (LLMs) for health information seeking can pose severe risks due to the potential for misinformation and the complexity of these topics. This paper introduces KnowNet a visualization system that integrates LLMs with Knowledge Graphs (KG) to provide enhanced accuracy and structured exploration. Specifically, for enhanced accuracy, KnowNet extracts triples (e.g., entities and their relations) from LLM outputs and maps them into the validated information and supported evidence in external KGs. For structured exploration, KnowNet provides next-step recommendations based on the neighborhood of the currently explored entities in KGs, aiming to guide a comprehensive understanding without overlooking critical aspects. To enable reasoning with both the structured data in KGs and the unstructured outputs from LLMs, KnowNet conceptualizes the understanding of a subject as the gradual construction of graph visualization. A progressive graph visualization is introduced to monitor past inquiries, and bridge the current query with the exploration history and next-step recommendations. We demonstrate the effectiveness of our system via use cases and expert interviews.\", \"AuthorNames-Deduped\": \"Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\", \"AuthorNames\": \"Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\", \"AuthorAffiliation\": \"Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Medical School, University of Minnesota, Twin Cities, MN, USA;Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN, USA\", \"InternalReferences\": \"10.1109/tvcg.2022.3209408;10.1109/tvcg.2023.3327168;10.1109/tvcg.2013.154;10.1109/tvcg.2021.3114876;10.1109/tvcg.2022.3209435;10.1109/tvcg.2018.2865232;10.1109/tvcg.2021.3114840;10.1109/tvcg.2020.3030471;10.1109/tvcg.2019.2934798\", \"AuthorKeywords\": \"Human-AI interactions,knowledge graph,,,conversational agent,large language model,progressive visualization\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 60.0, \"Downloads_Xplore\": 632.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"VisEval: A Benchmark for Data Visualization in the Era of Large Language Models\", \"DOI\": \"10.1109/tvcg.2024.3456320\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456320\", \"FirstPage\": 1301.0, \"LastPage\": 1311.0, \"PaperType\": \"J\", \"Abstract\": \"Translating natural language to visualization (NL2VIS) has shown great promise for visual data analysis, but it remains a challenging task that requires multiple low-level implementations, such as natural language processing and visualization design. Recent advancements in pre-trained large language models (LLMs) are opening new avenues for generating visualizations from natural language. However, the lack of a comprehensive and reliable benchmark hinders our understanding of LLMs' capabilities in visualization generation. In this paper, we address this gap by proposing a new NL2VIS benchmark called VisEval. Firstly, we introduce a high-quality and large-scale dataset. This dataset includes 2,524 representative queries covering 146 databases, paired with accurately labeled ground truths. Secondly, we advocate for a comprehensive automated evaluation methodology covering multiple dimensions, including validity, legality, and readability. By systematically scanning for potential issues with a number of heterogeneous checkers, VisEval provides reliable and trustworthy evaluation outcomes. We run VisEval on a series of state-of-the-art LLMs. Our evaluation reveals prevalent challenges and delivers essential insights for future advancements.\", \"AuthorNames-Deduped\": \"Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001\", \"AuthorNames\": \"Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang\", \"AuthorAffiliation\": \"Microsoft Research, USA;Microsoft Research, USA;Microsoft Research, USA;ShanghaiTech University and MoE Key Laboratory of Intelligent Perception and Human Machine Collaboration, China;Microsoft Research, USA\", \"InternalReferences\": \"10.1109/infvis.2005.1532136;10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114804;10.1109/tvcg.2021.3114848;10.1109/tvcg.2020.3030378;10.1109/tvcg.2016.2599030;10.1109/tvcg.2020.3030423;10.1109/tvcg.2019.2934668\", \"AuthorKeywords\": \"Visualization evaluation,automatic visualization,,,large language models,benchmark\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 1.0, \"PubsCited_CrossRef\": 75.0, \"Downloads_Xplore\": 625.0, \"Award\": \"BP\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 1.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"When Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis with Touch and Speech\", \"DOI\": \"10.1109/tvcg.2024.3456358\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456358\", \"FirstPage\": 864.0, \"LastPage\": 874.0, \"PaperType\": \"J\", \"Abstract\": \"Despite the recent surge of research efforts to make data visualizations accessible to people who are blind or have low vision (BLV), how to support BLV people's data analysis remains an important and challenging question. As refreshable tactile displays (RTDs) become cheaper and conversational agents continue to improve, their combination provides a promising approach to support BLV people's interactive data exploration and analysis. To understand how BLV people would use and react to a system combining an RTD with a conversational agent, we conducted a Wizard-of-Oz study with 11 BLV participants, where they interacted with line charts, bar charts, and isarithmic maps. Our analysis of participants' interactions led to the identification of nine distinct patterns. We also learned that the choice of modalities depended on the type of task and prior experience with tactile graphics, and that participants strongly preferred the combination of RTD and speech to a single modality. In addition, participants with more tactile experience described how tactile images facilitated a deeper engagement with the data and supported independent interpretation. Our findings will inform the design of interfaces for such interactive mixed-modality systems.\", \"AuthorNames-Deduped\": \"Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\", \"AuthorNames\": \"Samuel Reinders;Matthew Butler;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\", \"AuthorAffiliation\": \"Monash University, Australia;Monash University, Australia;Monash University, Australia;Yonsei University, South Korea;Monash University, Australia;Monash University, Australia\", \"InternalReferences\": \"10.1109/tvcg.2023.3327393;10.1109/tvcg.2021.3114846;10.1109/tvcg.2012.275\", \"AuthorKeywords\": \"Accessible data visualization,refreshable tactile displays,,,conversational agents,interactive data exploration,Wizard of Oz study,people who are blind or have low vision\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 184.0, \"Award\": \"HM\", \"GraphicsReplicabilityStamp\": null, \"award_flag\": true, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"DracoGPT: Extracting Visualization Design Preferences from Large Language Models\", \"DOI\": \"10.1109/tvcg.2024.3456350\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456350\", \"FirstPage\": 710.0, \"LastPage\": 720.0, \"PaperType\": \"J\", \"Abstract\": \"Trained on vast corpora, Large Language Models (LLMs) have the potential to encode visualization design knowledge and best practices. However, if they fail to do so, they might provide unreliable visualization recommendations. What visualization design preferences, then, have LLMs learned? We contribute DracoGPT, a method for extracting, modeling, and assessing visualization design preferences from LLMs. To assess varied tasks, we develop two pipelines\\u2014DracoGPT-Rank and DracoGPT-Recommend\\u2014to model LLMs prompted to either rank or recommend visual encoding specifications. We use Draco as a shared knowledge base in which to represent LLM design preferences and compare them to best practices from empirical research. We demonstrate that DracoGPT can accurately model the preferences expressed by LLMs, enabling analysis in terms of Draco design constraints. Across a suite of backing LLMs, we find that DracoGPT-Rank and DracoGPT-Recommend moderately agree with each other, but both substantially diverge from guidelines drawn from human subjects experiments. Future work can build on our approach to expand Draco's knowledge base to model a richer set of preferences and to provide a robust and cost-effective stand-in for LLMs.\", \"AuthorNames-Deduped\": \"Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer\", \"AuthorNames\": \"Huichen Will Wang;Mitchell Gordon;Leilani Battle;Jeffrey Heer\", \"AuthorAffiliation\": \"University of Washington, USA;University of Washington, USA;University of Washington, USA;University of Washington, USA\", \"InternalReferences\": \"10.1109/tvcg.2015.2467732;10.1109/tvcg.2021.3114863;10.1109/tvcg.2018.2865240;10.1109/tvcg.2016.2599030;10.1109/tvcg.2023.3327172;10.1109/tvcg.2023.3326527\", \"AuthorKeywords\": \"Visualization,Large Language Models,,,Visualization Recommendation,Graphical Perception\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 45.0, \"Downloads_Xplore\": 378.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Smartboard: Visual Exploration of Team Tactics with LLM Agent\", \"DOI\": \"10.1109/tvcg.2024.3456200\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456200\", \"FirstPage\": 23.0, \"LastPage\": 33.0, \"PaperType\": \"J\", \"Abstract\": \"Tactics play an important role in team sports by guiding how players interact on the field. Both sports fans and experts have a demand for analyzing sports tactics. Existing approaches allow users to visually perceive the multivariate tactical effects. However, these approaches require users to experience a complex reasoning process to connect the multiple interactions within each tactic to the final tactical effect. In this work, we collaborate with basketball experts and propose a progressive approach to help users gain a deeper understanding of how each tactic works and customize tactics on demand. Users can progressively sketch on a tactic board, and a coach agent will simulate the possible actions in each step and present the simulation to users with facet visualizations. We develop an extensible framework that integrates large language models (LLMs) and visualizations to help users communicate with the coach agent with multimodal inputs. Based on the framework, we design and develop Smartboard, an agent-based interactive visualization system for fine-grained tactical analysis, especially for play design. Smartboard provides users with a structured process of setup, simulation, and evolution, allowing for iterative exploration of tactics based on specific personalized scenarios. We conduct case studies based on real-world basketball datasets to demonstrate the effectiveness and usefulness of our system.\", \"AuthorNames-Deduped\": \"Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu 0003;Liqi Cheng;Hui Zhang 0051;Yingcai Wu\", \"AuthorNames\": \"Ziao Liu;Xiao Xie;Moqi He;Wenshuo Zhao;Yihong Wu;Liqi Cheng;Hui Zhang;Yingcai Wu\", \"AuthorAffiliation\": \"Department of Sports Science, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China;Department of Sports Science, Zhejiang University, China;State Key Lab of CAD&CG, Zhejiang University, China\", \"InternalReferences\": \"10.1109/tvcg.2023.3326524;10.1109/vast.2014.7042478;10.1109/tvcg.2023.3326910;10.1109/tvcg.2024.3456145;10.1109/tvcg.2023.3327353;10.1109/tvcg.2023.3327161;10.1109/tvcg.2022.3209353;10.1109/tvcg.2013.192;10.1109/tvcg.2012.263;10.1109/tvcg.2019.2934243;10.1109/tvcg.2014.2346445;10.1109/tvcg.2023.3326940;10.1109/tvcg.2022.3209352;10.1109/tvcg.2023.3327153;10.1109/tvcg.2022.3209452;10.1109/tvcg.2021.3114832;10.1109/tvcg.2022.3209373;10.1109/tvcg.2017.2744218;10.1109/tvcg.2018.2865041;10.1109/tvcg.2023.3326913;10.1109/tvcg.2020.3030359;10.1109/tvcg.2022.3209497;10.1109/tvcg.2021.3114806\", \"AuthorKeywords\": \"Sports visualization,tactic board,,,tactical analysis\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 74.0, \"Downloads_Xplore\": 813.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}, {\"Conference\": \"Vis\", \"Year\": 2024, \"Title\": \"Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations\", \"DOI\": \"10.1109/tvcg.2024.3456182\", \"Link\": \"http://dx.doi.org/10.1109/TVCG.2024.3456182\", \"FirstPage\": 754.0, \"LastPage\": 764.0, \"PaperType\": \"J\", \"Abstract\": \"People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6\", \"AuthorNames-Deduped\": \"Ratanond Koonchanok;Michael E. Papka;Khairi Reda\", \"AuthorNames\": \"Ratanond Koonchanok;Michael E. Papka;Khairi Reda\", \"AuthorAffiliation\": \"Indiana University Indianapolis, USA;Argonne National Laboratory, University of Illinois Chicago, USA;Indiana University Indianapolis, USA\", \"InternalReferences\": \"10.1109/tvcg.2016.2598862;10.1109/vast.2017.8585665;10.1109/tvcg.2014.2346979;10.1109/tvcg.2023.3326516;10.1109/tvcg.2020.3029412;10.1109/tvcg.2020.3028984;10.1109/tvcg.2012.199;10.1109/tvcg.2015.2467758;10.1109/vast.2017.8585669;10.1109/tvcg.2010.161;10.1109/tvcg.2023.3326513\", \"AuthorKeywords\": \"Visual inference,statistical rationality,,,human-machine collaboration\", \"AminerCitationCount\": 0.0, \"CitationCount_CrossRef\": 0.0, \"PubsCited_CrossRef\": 71.0, \"Downloads_Xplore\": 138.0, \"Award\": null, \"GraphicsReplicabilityStamp\": null, \"award_flag\": false, \"citation_pref\": 0.0}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_bcb7f009\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nDataset path used: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nPapers considered (AutoVis-identified or full if none matched): 80\nAward-flagged papers in this set: 10\n### End of facts\n### Begin of facts\nCitation metric coverage and distribution (AutoVis subset):\nAminerCitationCount \u2014 nonzero: 53/80 (66.25%), mean=34.8, median=9.5, p90=109, p95=130, p99=280, max=487\nCitationCount_CrossRef \u2014 nonzero: 72/80 (90.00%), mean=30.8, median=14.0, p90=81, p95=107, p99=201, max=292\n### End of facts\n### Begin of facts\nCorrelation between Aminer and CrossRef citation counts (Pearson): 0.946\nCorrelation between CitationCount_CrossRef and Downloads_Xplore: 0.814\nInterpretation: higher correlation indicates metrics track each other; low CrossRef nonzero fraction suggests missingness for older/aggregate data.\n### End of facts\n### Begin of facts\nRecommendation for selection rule:\nInclude all Award-flagged papers regardless of citation counts PLUS top papers by a primary citation metric. Choose the metric with better coverage; here we choose: CitationCount_CrossRef\nSuggested numeric thresholds (based on distribution): top 1% >= 201 CitationCount_CrossRef, top 5% >= 106 CitationCount_CrossRef, top 10% >= 81 CitationCount_CrossRef.\nFor a compact milestone table of ~20 papers: include all awards (10) + fill remaining slots with top papers by CitationCount_CrossRef.\nAlternative: use combined score (normalized citations + normalized downloads) to boost recent high-download works.\n### End of facts\n### Begin of facts\nCandidate milestone list (top 20):\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 CitationCount_CrossRef=292, CrossRef=292, Downloads=4307, Award=NO; Authors=Kanit Wongsuphasawat;Dominik Moritz;Anushka Anand;Jock D. Mackinlay;Bill Howe;Jeffrey Heer\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 CitationCount_CrossRef=177, CrossRef=177, Downloads=3238, Award=YES; Authors=Dominik Moritz;Chenglong Wang;Greg L. Nelson;Halden Lin;Adam M. Smith 0001;Bill Howe;Jeffrey Heer\nA Design Space of Visualization Tasks (2013) \u2014 CitationCount_CrossRef=144, CrossRef=144, Downloads=4884, Award=NO; Authors=Hans-J\u00f6rg Schulz;Thomas Nocke;Magnus Heitzler;Heidrun Schumann\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 CitationCount_CrossRef=121, CrossRef=121, Downloads=2942, Award=NO; Authors=Arjun Srinivasan;Steven Mark Drucker;Alex Endert;John T. Stasko\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 CitationCount_CrossRef=106, CrossRef=106, Downloads=2108, Award=NO; Authors=\u00c1ngel Alexander Cabrera;Will Epperson;Fred Hohman;Minsuk Kahng;Jamie Morgenstern;Duen Horng Chau\nFinding Waldo: Learning about Users from their Interactions (2014) \u2014 CitationCount_CrossRef=95, CrossRef=95, Downloads=2226, Award=NO; Authors=Eli T. Brown;Alvitta Ottley;Helen Zhao 0001;Quan Lin;Richard Souvenir;Alex Endert;Remco Chang\nData-Driven Guides: Supporting Expressive Design for Information Graphics (2016) \u2014 CitationCount_CrossRef=92, CrossRef=92, Downloads=2245, Award=NO; Authors=Nam Wook Kim;Eston Schweickart;Zhicheng Liu 0001;Mira Dontcheva;Wilmot Li;Jovan Popovic;Hanspeter Pfister\nDQNViz: A Visual Analytics Approach to Understand Deep Q-Networks (2018) \u2014 CitationCount_CrossRef=91, CrossRef=91, Downloads=2871, Award=YES; Authors=Junpeng Wang 0001;Liang Gou;Han-Wei Shen;Hao Yang 0007\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 CitationCount_CrossRef=80, CrossRef=80, Downloads=3724, Award=NO; Authors=Danqing Shi;Xinyue Xu;Fuling Sun;Yang Shi 0007;Nan Cao 0001\nLearning Perceptual Kernels for Visualization Design (2014) \u2014 CitationCount_CrossRef=80, CrossRef=80, Downloads=1247, Award=NO; Authors=\u00c7agatay Demiralp;Michael S. Bernstein;Jeffrey Heer\nText-to-Viz: Automatic Generation of Infographics from Proportion-Related Natural Language Statements (2019) \u2014 CitationCount_CrossRef=71, CrossRef=71, Downloads=2661, Award=NO; Authors=Weiwei Cui;Xiaoyu Zhang 0014;Yun Wang 0012;He Huang;Bei Chen;Lei Fang 0004;Haidong Zhang;Jian-Guang Lou;Dongmei Zhang 0001\nWarning, Bias May Occur: A Proposed Approach to Detecting Cognitive Bias in Interactive Visual Analytics (2017) \u2014 CitationCount_CrossRef=70, CrossRef=70, Downloads=1801, Award=NO; Authors=Emily Wall;Leslie M. Blaha;Lyndsey Franklin;Alex Endert\nKG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation (2021) \u2014 CitationCount_CrossRef=69, CrossRef=69, Downloads=3452, Award=YES; Authors=Haotian Li 0001;Yong Wang 0021;Songheng Zhang;Yangqiu Song;Huamin Qu\nAugmenting Sports Videos with VisCommentator (2021) \u2014 CitationCount_CrossRef=42, CrossRef=42, Downloads=2151, Award=YES; Authors=Zhutian Chen;Shuainan Ye;Xiangtong Chu;Haijun Xia;Hui Zhang 0051;Huamin Qu;Yingcai Wu\nAn Evaluation-Focused Framework for Visualization Recommendation Algorithms (2021) \u2014 CitationCount_CrossRef=25, CrossRef=25, Downloads=1106, Award=YES; Authors=Zehua Zeng;Phoebe Moh;Fan Du;Jane Hoffswell;Tak Yeon Lee;Sana Malik;Eunyee Koh;Leilani Battle\nMEDLEY: Intent-based Recommendations to Support Dashboard Composition (2022) \u2014 CitationCount_CrossRef=8, CrossRef=8, Downloads=1537, Award=YES; Authors=Aditeya Pandey;Arjun Srinivasan;Vidya Setlur\nVisEval: A Benchmark for Data Visualization in the Era of Large Language Models (2024) \u2014 CitationCount_CrossRef=1, CrossRef=1, Downloads=625, Award=YES; Authors=Nan Chen;Yuge Zhang;Jiahang Xu;Kan Ren;Yuqing Yang 0001\nKNowNEt:Guided Health Information Seeking from LLMs via Knowledge Graph Integration (2024) \u2014 CitationCount_CrossRef=1, CrossRef=1, Downloads=632, Award=YES; Authors=Youfu Yan;Yu Hou;Yongkang Xiao;Rui Zhang;Qianwen Wang\nWhen Refreshable Tactile Displays Meet Conversational Agents: Investigating Accessible Data Presentation and Analysis wi (2024) \u2014 CitationCount_CrossRef=0, CrossRef=0, Downloads=184, Award=YES; Authors=Samuel Reinders;Matthew Butler 0002;Ingrid Zukerman;Bongshin Lee;Lizhen Qu;Kim Marriott\nData Formulator: AI-Powered Concept-Driven Visualization Authoring (2023) \u2014 CitationCount_CrossRef=0, CrossRef=0, Downloads=893, Award=YES; Authors=Chenglong Wang;John Thompson 0002;Bongshin Lee\n### End of facts\n### Begin of facts\nTop 10 papers by CitationCount_CrossRef:\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 CitationCount_CrossRef=292, Award=NO\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 CitationCount_CrossRef=177, Award=YES\nA Design Space of Visualization Tasks (2013) \u2014 CitationCount_CrossRef=144, Award=NO\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 CitationCount_CrossRef=121, Award=NO\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 CitationCount_CrossRef=106, Award=NO\nFinding Waldo: Learning about Users from their Interactions (2014) \u2014 CitationCount_CrossRef=95, Award=NO\nData-Driven Guides: Supporting Expressive Design for Information Graphics (2016) \u2014 CitationCount_CrossRef=92, Award=NO\nDQNViz: A Visual Analytics Approach to Understand Deep Q-Networks (2018) \u2014 CitationCount_CrossRef=91, Award=YES\nLearning Perceptual Kernels for Visualization Design (2014) \u2014 CitationCount_CrossRef=80, Award=NO\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 CitationCount_CrossRef=80, Award=NO\n### End of facts\n",
                    "text": "The milestone-paper view (citation bars and citation-source comparison) surfaces highly influential works used as temporal anchors: Voyager (2015) is the single most-cited AutoVis paper (CrossRef=292, Aminer=487) and also tops downloads, marking a practical, high-impact recommender system milestone. Draco (2018) formalizing design knowledge is another anchor (CrossRef=177, award-flag present) that shifted attention toward constraint-based, explainable recommendation. Other recurring milestones are task taxonomies (2013), perceptual\u2011kernel work (2014), and tools for automated story and data-fact generation (Calliope, Voder). For milestone selection we recommend including all award-flagged papers plus the top CrossRef-cited papers (or a combined normalized metric of citations and downloads) to represent both long-term influence and current practical uptake."
                }
            ]
        },
        {
            "section_number": 4,
            "section_name": "What were researchers building? Topics, methods, and tools",
            "section_size": "medium",
            "section_description": "Thematic analysis of AutoVis research: common keywords, recurring methods (recommendation, mixed-initiative, generation, agents), input/output types (datasets, visualization grammars, web-based tools), and evaluation strategies. Planned visuals: keyword frequency bar chart, topic clusters or embedding projection (UMAP/t-SNE) colored by era, and example pipelines illustrating typical system architectures.",
            "analyses": [
                {
                    "analysis_schema": {
                        "action": "present",
                        "information_needed": {
                            "question_text": "Which author keywords (and high-frequency topical terms) are most common in the AutoVis corpus, and how do the top keywords change across eras?",
                            "primary_attributes": [
                                "keyword (tokenized from AuthorKeywords and high-frequency terms from Title/Abstract)",
                                "count"
                            ],
                            "secondary_attributes": [
                                "era (derived from Year, e.g., pre-2010 / 2010\u20132015 / 2016\u20132020 / 2021+)"
                            ],
                            "transformation": [
                                "tokenize AuthorKeywords into one row per keyword; normalize to lowercase and trim",
                                "extract high-frequency terms from Title/Abstract (optional: use simple regex/tokenization and stopword removal)",
                                "derive era bins from Year"
                            ],
                            "expected_insight_types": [
                                "top (most frequent keywords)",
                                "trend (how prominence of top keywords shifts across eras)",
                                "distribution (long tail of keywords)"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_99ed8c9d\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-dcf39b2aee410b9bb3198b16211309a7\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"era\", \"title\": \"Era\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"kw\", \"type\": \"nominal\"}, {\"field\": \"era\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"kw\", \"sort\": null, \"title\": \"Keyword\", \"type\": \"nominal\"}}, \"height\": 420, \"title\": \"Top author keywords (by era) in the AutoVis corpus\", \"width\": 760, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-dcf39b2aee410b9bb3198b16211309a7\": [{\"kw\": \"application motivated visualization\", \"era\": \"2021+\", \"count\": 3}, {\"kw\": \"automatic visualization\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"automatic visualization\", \"era\": \"2021+\", \"count\": 5}, {\"kw\": \"conversational agent\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"glyph-based visualization\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"graph visualization\", \"era\": \"2010-2015\", \"count\": 2}, {\"kw\": \"infographics\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"infographics\", \"era\": \"2021+\", \"count\": 1}, {\"kw\": \"mixed initiative human-machine analysis\", \"era\": \"2021+\", \"count\": 4}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2021+\", \"count\": 1}, {\"kw\": \"recommender systems\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"recommender systems\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"reinforcement learning\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"reinforcement learning\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"sports visualization\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"temporal data\", \"era\": \"2021+\", \"count\": 3}, {\"kw\": \"visual analytics\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"visual analytics\", \"era\": \"2016-2020\", \"count\": 5}, {\"kw\": \"visual analytics\", \"era\": \"2021+\", \"count\": 4}, {\"kw\": \"visual analytics\", \"era\": \"pre-2010\", \"count\": 1}, {\"kw\": \"visualization\", \"era\": \"2010-2015\", \"count\": 4}, {\"kw\": \"visualization\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"visualization\", \"era\": \"2021+\", \"count\": 5}, {\"kw\": \"visualization\", \"era\": \"pre-2010\", \"count\": 1}, {\"kw\": \"visualization recommendation\", \"era\": \"2010-2015\", \"count\": 2}, {\"kw\": \"visualization recommendation\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"visualization recommendation\", \"era\": \"2021+\", \"count\": 4}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_99ed8c9d\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nTop 10 author keywords overall (keyword: count):\nvisualization: 11\nvisual analytics: 11\nvisualization recommendation: 8\nautomatic visualization: 7\nmixed-initiative visual analytics: 4\nreinforcement learning: 4\nmixed initiative human-machine analysis: 4\ntemporal data: 3\napplication motivated visualization: 3\nmachine learning: 2\n### End of facts\n### Begin of facts\nTop keywords by era (showing up to 10 each):\npre-2010: visualization(1), visual data mining(1), focus + context(1), geospatial analysis(1), geospatial visualization(1), multiple-view techniques(1), scivis(1), integrating infovis(1), time-varying volume data(1), multi-field visualization(1)\n2010-2015: visualization(4), graph visualization(2), visualization recommendation(2), artificial intelligence(1), medical decision support systems(1), motif visualization(1), biology(1), mutations(1), pattern visualization(1), mr spectroscopy(1)\n2016-2020: visual analytics(5), reinforcement learning(2), visualization recommendation(2), user interaction(2), automatic visualization(2), mixed-initiative visual analytics(2), topic model optimization(1), information visualization(1), semantic mapping(1), guided visual analytics(1)\n2021+: visualization(5), automatic visualization(5), visual analytics(4), mixed initiative human-machine analysis(4), visualization recommendation(4), application motivated visualization(3), temporal data(3), glyph-based visualization(2), large language model(2), machine learning(2)\n### End of facts\n### Begin of facts\nHigh-frequency terms from Title+Abstract (top 10):\ndesign: 104\nour: 100\ncan: 74\nvisualizations: 68\nmodel: 65\nanalysis: 65\napproach: 61\nwhich: 56\nthese: 53\nknowledge: 49\n### End of facts\n### Begin of facts\nOverlap between top author keywords and top title/abstract terms:\nNone\n### End of facts\n### Begin of facts\nKeyword distribution: total occurrences=337, unique keywords=268, top 10 coverage=16.9%\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                },
                {
                    "analysis_schema": {
                        "action": "explore",
                        "information_needed": {
                            "question_text": "How should we generate and parameterize a low-dimensional embedding and clustering of AutoVis papers (for UMAP/t-SNE projection) so we can visualize topic clusters and color them by era?",
                            "key_uncertainty": "Which textual fields and embedding method produce coherent topic clusters (AuthorKeywords vs Abstract+Title vs both); which dimensionality reduction (UMAP vs t-SNE) and parameter settings (n_neighbors, min_dist or perplexity) work best; and how many clusters (k) should we use?",
                            "expected_outputs": [
                                "Recommended text input strategy (e.g., concatenate Title + Abstract + AuthorKeywords, or use keywords only) and preprocessing steps (stopword removal, stemming/lemmatization, n-grams)",
                                "Recommended embedding approach (TF-IDF + truncated SVD for speed, or SBERT/sentence embeddings for semantic fidelity) and concrete parameters to try",
                                "Recommended dimensionality reduction and clustering pipeline with parameter suggestions (e.g., UMAP with n_neighbors=15, min_dist=0.1, then HDBSCAN or k-means with suggested k and method to choose k like silhouette scores) and era binning strategy"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n                "
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nAutoVis papers detected: 80 / 83 (96.4% of dataset) using keyword search in Title+Abstract+AuthorKeywords\n### End of facts\n### Begin of facts\nKeyword location: AuthorKeywords only: 21, Title/Abstract only: 40, Both: 19.\n### End of facts\n### Begin of facts\nAutoVis by era (counts): {'<=2009': np.int64(7), '2010-2014': np.int64(7), '2015-2019': np.int64(19), '2020+': np.int64(47)}\n### End of facts\n### Begin of facts\nTop AutoVis papers by CitationCount_CrossRef:\n2015: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (292 crossref cites)\n2018: Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (177 crossref cites)\n2013: A Design Space of Visualization Tasks (144 crossref cites)\n2018: Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (121 crossref cites)\n2019: FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (106 crossref cites)\n### End of facts\n### Begin of facts\nTop AutoVis papers by AminerCitationCount:\n2015: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (487 Aminer cites)\n2018: Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (225 Aminer cites)\n2013: A Design Space of Visualization Tasks (217 Aminer cites)\n2014: Finding Waldo: Learning about Users from their Interactions (145 Aminer cites)\n2014: Learning Perceptual Kernels for Visualization Design (129 Aminer cites)\n### End of facts\n### Begin of facts\nTop authors in AutoVis papers (count): [('Alex Endert', 7), ('Huamin Qu', 6), ('Yingcai Wu', 6), ('Jeffrey Heer', 5), ('Nan Cao', 5), ('Dominik Moritz', 4)]\n### End of facts\n### Begin of facts\nRecommendations:\n- Text input: concatenate Title + Abstract + AuthorKeywords; preprocess: lowercase, remove punctuation, remove stopwords, lemmatize, keep unigrams+bigrams.\n- Embedding: for speed use TF-IDF (max_features=20000) + TruncatedSVD (n_components=100). For semantic quality use SBERT (e.g., 'all-MiniLM-L6-v2') to get dense vectors.\n- Projection: prefer UMAP (n_neighbors=15, min_dist=0.1, metric='cosine'); try n_neighbors in [5,15,50], min_dist in [0.0,0.1,0.5]. If using t-SNE try perplexity in [5,30,50].\n- Clustering: HDBSCAN (min_cluster_size=10) to find variable-size clusters; fallback k-means with k in [3..8] and select by silhouette.\n- Era coloring: use bins like <=2009,2010-2014,2015-2019,2020+ or a continuous year colormap.\n- Practical pipeline: clean text -> SBERT (or TF-IDF+SVD) -> UMAP (n_neighbors=15,min_dist=0.1) -> HDBSCAN; tune based on cluster sizes and silhouette.\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                }
            ],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "Thematic analysis shows AutoVis as a multi-modal, mixed-initiative research area: common aims are automatic generation or recommendation of visualizations, augmenting visualizations with natural-language facts, and using machine learning to steer design or interaction. Frequent methods include recommender systems (heuristic and learned), constraint-based optimization (Draco-style), mixed-initiative interfaces and semantic interaction, reinforcement learning for dashboard/layout, and language models for NL2VIS and authoring. Inputs range from spreadsheets, structured tabular data, and visualization grammars to embeddings and language prompts; outputs include charts, small-multiple dashboards, annotated story segments, and exportable templates. Evaluation strategies are diverse \u2014 controlled user studies, task-based lab evaluations, and empirical comparisons \u2014 but are not yet standardized, and only a minority of papers release datasets or code artifacts for replication."
                },
                {
                    "id": 1,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_99ed8c9d\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-dcf39b2aee410b9bb3198b16211309a7\"}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"era\", \"title\": \"Era\", \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"kw\", \"type\": \"nominal\"}, {\"field\": \"era\", \"type\": \"nominal\"}, {\"field\": \"count\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"kw\", \"sort\": null, \"title\": \"Keyword\", \"type\": \"nominal\"}}, \"height\": 420, \"title\": \"Top author keywords (by era) in the AutoVis corpus\", \"width\": 760, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-dcf39b2aee410b9bb3198b16211309a7\": [{\"kw\": \"application motivated visualization\", \"era\": \"2021+\", \"count\": 3}, {\"kw\": \"automatic visualization\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"automatic visualization\", \"era\": \"2021+\", \"count\": 5}, {\"kw\": \"conversational agent\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"glyph-based visualization\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"graph visualization\", \"era\": \"2010-2015\", \"count\": 2}, {\"kw\": \"infographics\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"infographics\", \"era\": \"2021+\", \"count\": 1}, {\"kw\": \"mixed initiative human-machine analysis\", \"era\": \"2021+\", \"count\": 4}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"mixed-initiative visual analytics\", \"era\": \"2021+\", \"count\": 1}, {\"kw\": \"recommender systems\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"recommender systems\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"reinforcement learning\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"reinforcement learning\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"sports visualization\", \"era\": \"2021+\", \"count\": 2}, {\"kw\": \"temporal data\", \"era\": \"2021+\", \"count\": 3}, {\"kw\": \"visual analytics\", \"era\": \"2010-2015\", \"count\": 1}, {\"kw\": \"visual analytics\", \"era\": \"2016-2020\", \"count\": 5}, {\"kw\": \"visual analytics\", \"era\": \"2021+\", \"count\": 4}, {\"kw\": \"visual analytics\", \"era\": \"pre-2010\", \"count\": 1}, {\"kw\": \"visualization\", \"era\": \"2010-2015\", \"count\": 4}, {\"kw\": \"visualization\", \"era\": \"2016-2020\", \"count\": 1}, {\"kw\": \"visualization\", \"era\": \"2021+\", \"count\": 5}, {\"kw\": \"visualization\", \"era\": \"pre-2010\", \"count\": 1}, {\"kw\": \"visualization recommendation\", \"era\": \"2010-2015\", \"count\": 2}, {\"kw\": \"visualization recommendation\", \"era\": \"2016-2020\", \"count\": 2}, {\"kw\": \"visualization recommendation\", \"era\": \"2021+\", \"count\": 4}]}};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_99ed8c9d\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nTop 10 author keywords overall (keyword: count):\nvisualization: 11\nvisual analytics: 11\nvisualization recommendation: 8\nautomatic visualization: 7\nmixed-initiative visual analytics: 4\nreinforcement learning: 4\nmixed initiative human-machine analysis: 4\ntemporal data: 3\napplication motivated visualization: 3\nmachine learning: 2\n### End of facts\n### Begin of facts\nTop keywords by era (showing up to 10 each):\npre-2010: visualization(1), visual data mining(1), focus + context(1), geospatial analysis(1), geospatial visualization(1), multiple-view techniques(1), scivis(1), integrating infovis(1), time-varying volume data(1), multi-field visualization(1)\n2010-2015: visualization(4), graph visualization(2), visualization recommendation(2), artificial intelligence(1), medical decision support systems(1), motif visualization(1), biology(1), mutations(1), pattern visualization(1), mr spectroscopy(1)\n2016-2020: visual analytics(5), reinforcement learning(2), visualization recommendation(2), user interaction(2), automatic visualization(2), mixed-initiative visual analytics(2), topic model optimization(1), information visualization(1), semantic mapping(1), guided visual analytics(1)\n2021+: visualization(5), automatic visualization(5), visual analytics(4), mixed initiative human-machine analysis(4), visualization recommendation(4), application motivated visualization(3), temporal data(3), glyph-based visualization(2), large language model(2), machine learning(2)\n### End of facts\n### Begin of facts\nHigh-frequency terms from Title+Abstract (top 10):\ndesign: 104\nour: 100\ncan: 74\nvisualizations: 68\nmodel: 65\nanalysis: 65\napproach: 61\nwhich: 56\nthese: 53\nknowledge: 49\n### End of facts\n### Begin of facts\nOverlap between top author keywords and top title/abstract terms:\nNone\n### End of facts\n### Begin of facts\nKeyword distribution: total occurrences=337, unique keywords=268, top 10 coverage=16.9%\n### End of facts\n",
                    "text": "The keyword-era chart makes the thematic shift visible: early work (pre\u20112010) centers on SciVis and geospatial/multi-field visualization, while the 2010\u20132015 window introduces graph visualization and early recommendation ideas. From 2016\u20132020 \u201cvisual analytics\u201d, \u201cautomatic visualization\u201d and \u201cmixed-initiative\u201d rise alongside emerging ML methods and reinforcement learning for layout/interaction. The 2021+ era shows a broader, more application-driven vocabulary \u2014 \u201cautomatic visualization\u201d, \u201cvisualization recommendation\u201d, \u201cmixed initiative human\u2011machine analysis\u201d, \u201ctemporal data\u201d and new LLM- and NLG-related terms \u2014 indicating an infusion of natural-language interfaces, larger-scale recommender models, and domain-specific authoring tools. This progression reflects both methodological maturation and adoption of new enabling technologies (web tooling, ML, and LLMs)."
                },
                {
                    "id": 2,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n                "
                    },
                    "facts": "### Begin of facts\nAutoVis papers detected: 80 / 83 (96.4% of dataset) using keyword search in Title+Abstract+AuthorKeywords\n### End of facts\n### Begin of facts\nKeyword location: AuthorKeywords only: 21, Title/Abstract only: 40, Both: 19.\n### End of facts\n### Begin of facts\nAutoVis by era (counts): {'<=2009': np.int64(7), '2010-2014': np.int64(7), '2015-2019': np.int64(19), '2020+': np.int64(47)}\n### End of facts\n### Begin of facts\nTop AutoVis papers by CitationCount_CrossRef:\n2015: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (292 crossref cites)\n2018: Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (177 crossref cites)\n2013: A Design Space of Visualization Tasks (144 crossref cites)\n2018: Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (121 crossref cites)\n2019: FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (106 crossref cites)\n### End of facts\n### Begin of facts\nTop AutoVis papers by AminerCitationCount:\n2015: Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (487 Aminer cites)\n2018: Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (225 Aminer cites)\n2013: A Design Space of Visualization Tasks (217 Aminer cites)\n2014: Finding Waldo: Learning about Users from their Interactions (145 Aminer cites)\n2014: Learning Perceptual Kernels for Visualization Design (129 Aminer cites)\n### End of facts\n### Begin of facts\nTop authors in AutoVis papers (count): [('Alex Endert', 7), ('Huamin Qu', 6), ('Yingcai Wu', 6), ('Jeffrey Heer', 5), ('Nan Cao', 5), ('Dominik Moritz', 4)]\n### End of facts\n### Begin of facts\nRecommendations:\n- Text input: concatenate Title + Abstract + AuthorKeywords; preprocess: lowercase, remove punctuation, remove stopwords, lemmatize, keep unigrams+bigrams.\n- Embedding: for speed use TF-IDF (max_features=20000) + TruncatedSVD (n_components=100). For semantic quality use SBERT (e.g., 'all-MiniLM-L6-v2') to get dense vectors.\n- Projection: prefer UMAP (n_neighbors=15, min_dist=0.1, metric='cosine'); try n_neighbors in [5,15,50], min_dist in [0.0,0.1,0.5]. If using t-SNE try perplexity in [5,30,50].\n- Clustering: HDBSCAN (min_cluster_size=10) to find variable-size clusters; fallback k-means with k in [3..8] and select by silhouette.\n- Era coloring: use bins like <=2009,2010-2014,2015-2019,2020+ or a continuous year colormap.\n- Practical pipeline: clean text -> SBERT (or TF-IDF+SVD) -> UMAP (n_neighbors=15,min_dist=0.1) -> HDBSCAN; tune based on cluster sizes and silhouette.\n### End of facts\n",
                    "text": "Auxiliary analyses show that top unigrams in titles/abstracts emphasize \u201cdesign\u201d, \u201cmodel\u201d, \u201canalysis\u201d, and \u201cvisualizations\u201d while author keywords enumerate higher-level categories (visual analytics, visualization recommendation, automatic visualization). The weak overlap between the two lists implies different emphases: authors summarize contributions in keywords (method or application labels) while abstracts emphasize design and modeling language; for future topic modeling we recommend combining both sources, using TF-IDF or SBERT embeddings, and projecting with UMAP plus HDBSCAN to recover era-colored clusters that reflect pipelines (input \u2192 model \u2192 interface \u2192 evaluation)."
                }
            ]
        },
        {
            "section_number": 5,
            "section_name": "Who shaped the field? People, collaborations, and influence",
            "section_size": "medium",
            "section_description": "Network and impact analysis of authors and institutions working on AutoVis. Planned analyses/visuals: co-authorship graph highlighting core clusters and bridges, institution contributions map, citation influence (citations per AutoVis paper), top authors/papers/institutions, and timeline of newcomers vs long-term contributors. Also inspect awards and highly-downloaded papers to surface influential work.",
            "analyses": [
                {
                    "analysis_schema": {
                        "action": "explore",
                        "information_needed": {
                            "question_text": "What is the structure of the co-authorship network among AutoVis authors \u2014 which clusters (research groups), bridging authors, and central nodes emerge when we build a co-authorship graph from the dataset?",
                            "key_uncertainty": "Author name quality (deduplication) and appropriate thresholds for including edges (e.g., minimum co-authored papers) are uncertain; also unclear which community-detection and centrality metrics best reveal meaningful structure for this dataset.",
                            "expected_outputs": [
                                "A cleaned list of author nodes using AuthorNames-Deduped (and examples of ambiguous names needing manual review).",
                                "A co-authorship edge list with coauthorship counts and a recommended minimum edge threshold (e.g., >= 1, >=2, >=3) to prune noise.",
                                "Computed node-level metrics for each author: degree, weighted degree, betweenness centrality (or approximation), and publication count; and a recommended community assignment (clusters) for visualization.",
                                "Suggested parameters for visualization (min publications per author, min coauthorship weight) and a shortlist of top candidate authors to highlight as bridges/cores."
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "antv",
                        "specification": "\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset=\"UTF-8\">\n        </head>\n        <body>\n            \n    <div id=\"network_a4d136b6-c3e1-40cb-9898-8d49a89297a5\" style=\"width: 100%px; height: 100%px;\"></div>\n    <script src=\"https://unpkg.com/@antv/g6@5/dist/g6.min.js\"></script>\n    <script>\nconst data_13d6b349 = {\"nodes\": [{\"id\": \"Daniel A. Keim\"}, {\"id\": \"Sebastian Bremm\"}, {\"id\": \"William Ribarsky\"}, {\"id\": \"Weiwei Cui\"}, {\"id\": \"Nick Cramer\"}, {\"id\": \"Han-Wei Shen\"}, {\"id\": \"Nuo Chen\"}, {\"id\": \"Jonathan P. Leidig\"}, {\"id\": \"Mira Dontcheva\"}, {\"id\": \"Jane Hoffswell\"}, {\"id\": \"Hans Hagen\"}, {\"id\": \"Richard Souvenir\"}, {\"id\": \"Tatiana von Landesberger\"}, {\"id\": \"Bahador Saket\"}, {\"id\": \"Dazhen Deng\"}, {\"id\": \"Jian-Guang Lou\"}, {\"id\": \"Anushka Anand\"}, {\"id\": \"Zheng Zhou\"}, {\"id\": \"Nam Wook Kim\"}, {\"id\": \"Jochen Ehret\"}, {\"id\": \"Kan Ren\"}, {\"id\": \"Emily Wall\"}, {\"id\": \"G. Elisabeta Marai\"}, {\"id\": \"Joon-Yong Lee\"}, {\"id\": \"Nafiul Nipu\"}, {\"id\": \"Walter Schubert\"}, {\"id\": \"Yuchen Yang\"}, {\"id\": \"Eli T. Brown\"}, {\"id\": \"Xiangtong Chu\"}, {\"id\": \"Haidong Zhang\"}, {\"id\": \"Arvind Satyanarayan\"}, {\"id\": \"Jason Leigh\"}, {\"id\": \"Jamie Morgenstern\"}, {\"id\": \"Andrew Wentzel\"}, {\"id\": \"Jiahang Xu\"}, {\"id\": \"Aoyu Wu\"}, {\"id\": \"Bernhard Pointner\"}, {\"id\": \"Danqing Shi\"}, {\"id\": \"Abdallah Sherif Radwan Mohamed\"}, {\"id\": \"Bernhard Preim\"}, {\"id\": \"Stephen C. North\"}, {\"id\": \"Jean-Daniel Fekete\"}, {\"id\": \"Peiran Ren\"}, {\"id\": \"Yingcai Wu\"}, {\"id\": \"Ravish Chawla\"}, {\"id\": \"Kanit Wongsuphasawat\"}, {\"id\": \"Yuqing Yang 0001\"}, {\"id\": \"Junpeng Wang 0001\"}, {\"id\": \"Yang Shi 0007\"}, {\"id\": \"Michalis Mamakos\"}, {\"id\": \"Lars Schuchardt\"}, {\"id\": \"Xinke Wu\"}, {\"id\": \"Huamin Qu\"}, {\"id\": \"Eunyee Koh\"}, {\"id\": \"Kristin A. Cook\"}, {\"id\": \"Shuainan Ye\"}, {\"id\": \"Ziyang Guo\"}, {\"id\": \"Zachary Wartell\"}, {\"id\": \"Chenglong Wang\"}, {\"id\": \"Remco Chang\"}, {\"id\": \"Lvkeshen Shen\"}, {\"id\": \"Alvitta Ottley\"}, {\"id\": \"\\u00c1ngel Alexander Cabrera\"}, {\"id\": \"Jock D. Mackinlay\"}, {\"id\": \"Chen Chen 0080\"}, {\"id\": \"Paolo Buono\"}, {\"id\": \"Khairi Reda\"}, {\"id\": \"Helen Zhao 0001\"}, {\"id\": \"John T. Stasko\"}, {\"id\": \"Yuge Zhang\"}, {\"id\": \"Tamara Munzner\"}, {\"id\": \"Samuel H. Payne\"}, {\"id\": \"Leslie M. Blaha\"}, {\"id\": \"Liang Gou\"}, {\"id\": \"Moqi He\"}, {\"id\": \"Zui Chen\"}, {\"id\": \"Adam M. Smith 0001\"}, {\"id\": \"Junran Yang\"}, {\"id\": \"Zhe Xu 0007\"}, {\"id\": \"Milos Sr\\u00e1mek\"}, {\"id\": \"Mengyu Zhou\"}, {\"id\": \"Qianwen Wang\"}, {\"id\": \"Robert Kincaid\"}, {\"id\": \"Olav Lenz\"}, {\"id\": \"Haotian Li 0001\"}, {\"id\": \"Carlos Eduardo Scheidegger\"}, {\"id\": \"Kwan-Liu Ma\"}, {\"id\": \"Xiaoyu Zhang 0014\"}, {\"id\": \"Wenwen Dou\"}, {\"id\": \"Ignacio P\\u00e9rez-Messina\"}, {\"id\": \"Jian Zhao 0010\"}, {\"id\": \"Haijun Xia\"}, {\"id\": \"Oskar Elek\"}, {\"id\": \"Oliver Deussen\"}, {\"id\": \"Ryan A. Rossi\"}, {\"id\": \"Steve DiPaola\"}, {\"id\": \"Yuzhe Luo\"}, {\"id\": \"Fabian Beck 0001\"}, {\"id\": \"Stefan Lindholm\"}, {\"id\": \"Will Epperson\"}, {\"id\": \"Matthew Butler 0002\"}, {\"id\": \"Yanna Lin\"}, {\"id\": \"Reyk Hillert\"}, {\"id\": \"Dongyu Liu\"}, {\"id\": \"Michael S. Bernstein\"}, {\"id\": \"Yngve Sekse Kristiansen\"}, {\"id\": \"Michael E. Papka\"}, {\"id\": \"Fred Hohman\"}, {\"id\": \"Lisanne van Dijk\"}, {\"id\": \"Gene Golovchinsky\"}, {\"id\": \"Dominik Moritz\"}, {\"id\": \"Sehi L'Yi\"}, {\"id\": \"Achim Ebert\"}, {\"id\": \"Frank Keul\"}, {\"id\": \"Juraj P\\u00e1lenik\"}, {\"id\": \"Sana Malik\"}, {\"id\": \"Michael Wolverton\"}, {\"id\": \"Wenshuo Zhao\"}, {\"id\": \"Shichao Jia\"}, {\"id\": \"Heidrun Schumann\"}, {\"id\": \"Philipp Muigg\"}, {\"id\": \"Aryaman Bahukhandi\"}, {\"id\": \"Gunnar L\\u00e4th\\u00e9n\"}, {\"id\": \"Fuling Sun\"}, {\"id\": \"Ratanond Koonchanok\"}, {\"id\": \"Hannah Kim 0001\"}, {\"id\": \"Yihong Wu 0003\"}, {\"id\": \"Haekyu Park\"}, {\"id\": \"Laura A. Garrison\"}, {\"id\": \"Phoebe Moh\"}, {\"id\": \"Alex Endert\"}, {\"id\": \"Rui Zhang\"}, {\"id\": \"Liqi Cheng\"}, {\"id\": \"Gromit Yeuk-Yin Chan\"}, {\"id\": \"Hanghang Tong\"}, {\"id\": \"Jeremy G. Freeman\"}, {\"id\": \"Steven Mark Drucker\"}, {\"id\": \"Rebecca Kehlbeck\"}, {\"id\": \"Mitchell Gordon\"}, {\"id\": \"Xinyue Xu\"}, {\"id\": \"Renzhong Li\"}, {\"id\": \"Nan Cao 0001\"}, {\"id\": \"Mikayla Biggs\"}, {\"id\": \"Stefan Bruckner\"}, {\"id\": \"Yifan Wu 0005\"}, {\"id\": \"Thomas Kamps\"}, {\"id\": \"Thomas Butkiewicz\"}, {\"id\": \"Zhuochen Jin\"}, {\"id\": \"Christopher Collins 0001\"}, {\"id\": \"Hyeok Kim\"}, {\"id\": \"Fan Du\"}, {\"id\": \"Carla Floricel\"}, {\"id\": \"Arpit Narechania\"}, {\"id\": \"Huichen Will Wang\"}, {\"id\": \"Robert A. Lafrance\"}, {\"id\": \"David J. Israel\"}, {\"id\": \"Johanna Schmidt\"}, {\"id\": \"Simon Urbanek\"}, {\"id\": \"Wilmot Li\"}, {\"id\": \"Wei Shuai\"}, {\"id\": \"Tak Yeon Lee\"}, {\"id\": \"Anne Laprie\"}, {\"id\": \"Youfu Yan\"}, {\"id\": \"\\u00c7agatay Demiralp\"}, {\"id\": \"Jeffrey Heer\"}, {\"id\": \"Hanspeter Pfister\"}, {\"id\": \"Xinhuan Shu\"}, {\"id\": \"Austin P. Wright\"}, {\"id\": \"Ziao Liu\"}, {\"id\": \"Aritra Dasgupta\"}, {\"id\": \"Mat\\u00fas Straka\"}, {\"id\": \"Zeyu Li 0003\"}, {\"id\": \"Wolfgang Freiler\"}, {\"id\": \"Tera Marie Green\"}, {\"id\": \"Yoon Kim\"}, {\"id\": \"Samuel Reinders\"}, {\"id\": \"Mennatallah El-Assady\"}, {\"id\": \"Michael Oppermann\"}, {\"id\": \"Jason D. Hartline\"}, {\"id\": \"Fabian Sperrle\"}, {\"id\": \"Vidya Setlur\"}, {\"id\": \"Benjamin Rowland\"}, {\"id\": \"Glenn Sun\"}, {\"id\": \"Eston Schweickart\"}, {\"id\": \"Tian Gao\"}, {\"id\": \"Guadalupe Canahuate\"}, {\"id\": \"He Huang\"}, {\"id\": \"J. Xavier Prochaska\"}, {\"id\": \"Jiazhe Wang\"}, {\"id\": \"Thomas Spengler\"}, {\"id\": \"Davide Ceneda\"}, {\"id\": \"Halden Lin\"}, {\"id\": \"Dennis Chau\"}, {\"id\": \"Helwig Hauser\"}, {\"id\": \"Sam Yu-Te Lee\"}, {\"id\": \"Ke Xu\"}, {\"id\": \"Alan Lundgard\"}, {\"id\": \"Kay Hamacher\"}, {\"id\": \"Lingyun Yu 0001\"}, {\"id\": \"Subhajit Das 0002\"}, {\"id\": \"Siming Chen 0001\"}, {\"id\": \"Hao Yang 0007\"}, {\"id\": \"Zhutian Chen\"}, {\"id\": \"Aditeya Pandey\"}, {\"id\": \"Jessica Hullman\"}, {\"id\": \"Magnus Borga\"}, {\"id\": \"Jovan Popovic\"}, {\"id\": \"Duen Horng Chau\"}, {\"id\": \"Nils Gehlenborg\"}, {\"id\": \"Gordon Woodhull\"}, {\"id\": \"Steffen Koch 0001\"}, {\"id\": \"Paola Valdivia\"}, {\"id\": \"Xiao Xie\"}, {\"id\": \"Songheng Zhang\"}, {\"id\": \"Zhicheng Liu 0001\"}, {\"id\": \"Leni Yang\"}, {\"id\": \"Kim Marriott\"}, {\"id\": \"Alexandra La Cruz\"}, {\"id\": \"Yiwen Sun\"}, {\"id\": \"Thomas Ertl\"}, {\"id\": \"Yongkang Xiao\"}, {\"id\": \"R. Jordan Crouser\"}, {\"id\": \"Angus G. Forbes\"}, {\"id\": \"Lu Ying\"}, {\"id\": \"Zehua Zeng\"}, {\"id\": \"Leilani Battle\"}, {\"id\": \"Santhosh Dharmapuri\"}, {\"id\": \"Michelle A. Borkin\"}, {\"id\": \"Guande Wu\"}, {\"id\": \"Abhraneel Sarma\"}, {\"id\": \"Nilaksh Das\"}, {\"id\": \"Arjun Srinivasan\"}, {\"id\": \"Miguel Nunes\"}, {\"id\": \"Chunyao Qian\"}, {\"id\": \"Qing Chen 0001\"}, {\"id\": \"Shizhao Sun\"}, {\"id\": \"Ross Maciejewski\"}, {\"id\": \"Duen Horng (Polo) Chau\"}, {\"id\": \"Chin-Yew Lin\"}, {\"id\": \"Simon Breslav\"}, {\"id\": \"Tan Tang\"}, {\"id\": \"Dominik Fleischmann\"}, {\"id\": \"Ryan Wilson\"}, {\"id\": \"Lyndsey Franklin\"}, {\"id\": \"Andrew E. Johnson 0001\"}, {\"id\": \"Yun Wang 0012\"}, {\"id\": \"Bongshin Lee\"}, {\"id\": \"Wenchao Wu\"}, {\"id\": \"Yusheng Qi\"}, {\"id\": \"Kresimir Matkovic\"}, {\"id\": \"Yunjeong Chang\"}, {\"id\": \"Thomas Nocke\"}, {\"id\": \"Yong Wang 0021\"}, {\"id\": \"Joseph N. Burchett\"}, {\"id\": \"Minsuk Kahng\"}, {\"id\": \"Reiner Lenz\"}, {\"id\": \"Katja B\\u00fchler\"}, {\"id\": \"Klaus Reichenberger\"}, {\"id\": \"Fanny Chevalier\"}, {\"id\": \"Nan Chen\"}, {\"id\": \"Dongmei Zhang 0001\"}, {\"id\": \"Arnold K\\u00f6chl\"}, {\"id\": \"Xiaohan Jiao\"}, {\"id\": \"Steffen Oeltze\"}, {\"id\": \"Bill Howe\"}, {\"id\": \"Bharath Kalidindi\"}, {\"id\": \"Joe Bruce\"}, {\"id\": \"Omar Shaikh\"}, {\"id\": \"Helmut Doleisch\"}, {\"id\": \"Jiawan Zhang\"}, {\"id\": \"Xinyi He\"}, {\"id\": \"Hui Zhang 0051\"}, {\"id\": \"Ying Chen\"}, {\"id\": \"Magnus Heitzler\"}, {\"id\": \"Azam Khan\"}, {\"id\": \"Shunan Guo\"}, {\"id\": \"Ryan Wesslen\"}, {\"id\": \"Jinpeng Wang 0001\"}, {\"id\": \"Clifton David Fuller\"}, {\"id\": \"John Thompson 0002\"}, {\"id\": \"Rahul Duggal\"}, {\"id\": \"Po-Ming Law\"}, {\"id\": \"Alireza Karduni\"}, {\"id\": \"Ingrid Zukerman\"}, {\"id\": \"Russ Burtner\"}, {\"id\": \"Bei Chen\"}, {\"id\": \"Shuhan Liu\"}, {\"id\": \"M. Eduard Gr\\u00f6ller\"}, {\"id\": \"Yangqiu Song\"}, {\"id\": \"Lizhen Qu\"}, {\"id\": \"Bingchang Chen\"}, {\"id\": \"Michael Glueck\"}, {\"id\": \"Hans-J\\u00f6rg Schulz\"}, {\"id\": \"Quan Lin\"}, {\"id\": \"Catherine Plaisant\"}, {\"id\": \"Johannes Knittel\"}, {\"id\": \"Sol\\u00e9akh\\u00e9na Ken\"}, {\"id\": \"Yixuan Li\"}, {\"id\": \"Yu Hou\"}, {\"id\": \"Yixian Zheng\"}, {\"id\": \"Paula Kayongo\"}, {\"id\": \"Greg L. Nelson\"}, {\"id\": \"Alexis Pister\"}, {\"id\": \"Matthias Schlachter\"}, {\"id\": \"Lei Fang 0004\"}, {\"id\": \"Heidrun Steinmetz\"}, {\"id\": \"Silvia Miksch\"}, {\"id\": \"Shahid Latif\"}, {\"id\": \"Anders Persson\"}, {\"id\": \"Yunhai Wang\"}], \"edges\": [{\"source\": \"Daniel A. Keim\", \"target\": \"Stephen C. North\", \"value\": 5, \"filtered\": false}, {\"source\": \"Daniel A. Keim\", \"target\": \"Oliver Deussen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Mennatallah El-Assady\", \"value\": 3, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Fabian Sperrle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Christopher Collins 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Rebecca Kehlbeck\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Tatiana von Landesberger\", \"value\": 4, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Kay Hamacher\", \"value\": 2, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Olav Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Wenwen Dou\", \"value\": 12, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Remco Chang\", \"value\": 12, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Tera Marie Green\", \"value\": 2, \"filtered\": false}, {\"source\": \"William Ribarsky\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": false}, {\"source\": \"William Ribarsky\", \"target\": \"Thomas Butkiewicz\", \"value\": 3, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Zachary Wartell\", \"value\": 1, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Huamin Qu\", \"value\": 12, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yingcai Wu\", \"value\": 4, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yun Wang 0012\", \"value\": 5, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Haidong Zhang\", \"value\": 7, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Dongmei Zhang 0001\", \"value\": 5, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"He Huang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Jian-Guang Lou\", \"value\": 2, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Jian Zhao 0010\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yong Wang 0021\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Qianwen Wang\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Yang Shi 0007\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nick Cramer\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Joon-Yong Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Kristin A. Cook\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Michael Wolverton\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Junpeng Wang 0001\", \"value\": 4, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Liang Gou\", \"value\": 2, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nuo Chen\", \"target\": \"Shichao Jia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nuo Chen\", \"target\": \"Zeyu Li 0003\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nuo Chen\", \"target\": \"Jiawan Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jonathan P. Leidig\", \"target\": \"Santhosh Dharmapuri\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Nam Wook Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Wilmot Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Arvind Satyanarayan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Fan Du\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Sana Malik\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Eunyee Koh\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Ryan A. Rossi\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Hyeok Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Huichen Will Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Jochen Ehret\", \"value\": 2, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Lars Schuchardt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Alvitta Ottley\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Kay Hamacher\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Olav Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Bahador Saket\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bahador Saket\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bahador Saket\", \"target\": \"Alex Endert\", \"value\": 3, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yingcai Wu\", \"value\": 9, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Xiao Xie\", \"value\": 4, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Hui Zhang 0051\", \"value\": 4, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Zheng Zhou\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yuchen Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Aoyu Wu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Huamin Qu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yihong Wu 0003\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Moqi He\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"He Huang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Haidong Zhang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Dongmei Zhang 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Kanit Wongsuphasawat\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Jock D. Mackinlay\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Yingcai Wu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Fabian Beck 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Nam Wook Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nam Wook Kim\", \"target\": \"Hanspeter Pfister\", \"value\": 4, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Wilmot Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Fabian Beck 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Lars Schuchardt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Jiahang Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Yuqing Yang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Leslie M. Blaha\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Alex Endert\", \"value\": 6, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"John T. Stasko\", \"value\": 1, \"filtered\": false}, {\"source\": \"Emily Wall\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Ravish Chawla\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Emily Wall\", \"target\": \"Arpit Narechania\", \"value\": 3, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Alireza Karduni\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Carla Floricel\", \"value\": 3, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Nafiul Nipu\", \"value\": 2, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Andrew Wentzel\", \"value\": 4, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Guadalupe Canahuate\", \"value\": 4, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 2, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"\\u00c7agatay Demiralp\", \"value\": 1, \"filtered\": false}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Kristin A. Cook\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Andrew Wentzel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Reyk Hillert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Bernhard Preim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Remco Chang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Alvitta Ottley\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Alex Endert\", \"value\": 3, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Ravish Chawla\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Shuainan Ye\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Yingcai Wu\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xiangtong Chu\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Huamin Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Yun Wang 0012\", \"value\": 9, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Ke Xu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Dongmei Zhang 0001\", \"value\": 7, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"He Huang\", \"value\": 4, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Aoyu Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Huamin Qu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Kanit Wongsuphasawat\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Jeffrey Heer\", \"value\": 3, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Alan Lundgard\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"John T. Stasko\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Vidya Setlur\", \"value\": 2, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason Leigh\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Yiwen Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Dennis Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Khairi Reda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason Leigh\", \"target\": \"Michael E. Papka\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jamie Morgenstern\", \"target\": \"\\u00c1ngel Alexander Cabrera\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Will Epperson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Guadalupe Canahuate\", \"value\": 4, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 2, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Yuqing Yang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yingcai Wu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Huamin Qu\", \"value\": 7, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Haotian Li 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Pointner\", \"target\": \"Johanna Schmidt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Pointner\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Yang Shi 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Guadalupe Canahuate\", \"value\": 2, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Clifton David Fuller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Steffen Oeltze\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Philipp Muigg\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Reyk Hillert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Carlos Eduardo Scheidegger\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Simon Urbanek\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Catherine Plaisant\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Paolo Buono\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Paola Valdivia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Fanny Chevalier\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Peiran Ren\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Xinke Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Dongyu Liu\", \"value\": 3, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Huamin Qu\", \"value\": 13, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xiao Xie\", \"value\": 15, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Hui Zhang 0051\", \"value\": 12, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xinhuan Shu\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Shuainan Ye\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Zhutian Chen\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Haijun Xia\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Tan Tang\", \"value\": 9, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Renzhong Li\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xinke Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Shuhan Liu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Johannes Knittel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Steffen Koch 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Lingyun Yu 0001\", \"value\": 6, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Thomas Ertl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Ziyang Guo\", \"value\": 3, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Lu Ying\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yuzhe Luo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Lvkeshen Shen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Yihong Wu 0003\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Moqi He\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Wenshuo Zhao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ravish Chawla\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ravish Chawla\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ravish Chawla\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Dominik Moritz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Jeffrey Heer\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Jock D. Mackinlay\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuqing Yang 0001\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuqing Yang 0001\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junpeng Wang 0001\", \"target\": \"Liang Gou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junpeng Wang 0001\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Nan Cao 0001\", \"value\": 8, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Huamin Qu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Xiaohan Jiao\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Siming Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Tian Gao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Zhuochen Jin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Yifan Wu 0005\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Ziyang Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lars Schuchardt\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lars Schuchardt\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yangqiu Song\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Nan Cao 0001\", \"value\": 5, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Jian Zhao 0010\", \"value\": 3, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Haotian Li 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yong Wang 0021\", \"value\": 11, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Songheng Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Wenchao Wu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yixian Zheng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Zhutian Chen\", \"value\": 5, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yun Wang 0012\", \"value\": 4, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Qianwen Wang\", \"value\": 6, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Shuainan Ye\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Leni Yang\", \"value\": 4, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Yanna Lin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Siming Chen 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Nils Gehlenborg\", \"value\": 1, \"filtered\": false}, {\"source\": \"Eunyee Koh\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Fan Du\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Sana Malik\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Eunyee Koh\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Ryan A. Rossi\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kristin A. Cook\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Michael Wolverton\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Shuainan Ye\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Shuainan Ye\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Jessica Hullman\", \"value\": 3, \"filtered\": true}, {\"source\": \"Ziyang Guo\", \"target\": \"Yifan Wu 0005\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziyang Guo\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Thomas Butkiewicz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Wenwen Dou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Adam M. Smith 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Alvitta Ottley\", \"value\": 4, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Wenwen Dou\", \"value\": 5, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"R. Jordan Crouser\", \"value\": 5, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Thomas Butkiewicz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Tera Marie Green\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Jeremy G. Freeman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Yuzhe Luo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": false}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Will Epperson\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Jeffrey Heer\", \"value\": 2, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Paola Valdivia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Khairi Reda\", \"target\": \"Michael E. Papka\", \"value\": 3, \"filtered\": true}, {\"source\": \"Khairi Reda\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Khairi Reda\", \"target\": \"Ratanond Koonchanok\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helen Zhao 0001\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helen Zhao 0001\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Bongshin Lee\", \"value\": 2, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Zhicheng Liu 0001\", \"value\": 9, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Arjun Srinivasan\", \"value\": 3, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Steven Mark Drucker\", \"value\": 1, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Arpit Narechania\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yuge Zhang\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tamara Munzner\", \"target\": \"Hanspeter Pfister\", \"value\": 2, \"filtered\": false}, {\"source\": \"Tamara Munzner\", \"target\": \"Robert Kincaid\", \"value\": 3, \"filtered\": true}, {\"source\": \"Tamara Munzner\", \"target\": \"Michael Oppermann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leslie M. Blaha\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leslie M. Blaha\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liang Gou\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liang Gou\", \"target\": \"Jian Zhao 0010\", \"value\": 1, \"filtered\": false}, {\"source\": \"Liang Gou\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": false}, {\"source\": \"Liang Gou\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Moqi He\", \"target\": \"Yihong Wu 0003\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Wenshuo Zhao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Hanghang Tong\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Mat\\u00fas Straka\", \"value\": 2, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Alexandra La Cruz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 3, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Dominik Fleischmann\", \"value\": 3, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": false}, {\"source\": \"Mengyu Zhou\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mengyu Zhou\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mengyu Zhou\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Zhutian Chen\", \"value\": 2, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yong Wang 0021\", \"value\": 3, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Nils Gehlenborg\", \"value\": 5, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Sehi L'Yi\", \"value\": 2, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Oliver Deussen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Youfu Yan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Rui Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert Kincaid\", \"target\": \"Michael Oppermann\", \"value\": 1, \"filtered\": true}, {\"source\": \"Olav Lenz\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"Olav Lenz\", \"target\": \"Kay Hamacher\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Songheng Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Carlos Eduardo Scheidegger\", \"target\": \"Simon Urbanek\", \"value\": 1, \"filtered\": true}, {\"source\": \"Carlos Eduardo Scheidegger\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Jian Zhao 0010\", \"value\": 2, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Michael E. Papka\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Aryaman Bahukhandi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"He Huang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenwen Dou\", \"target\": \"Ryan Wesslen\", \"value\": 4, \"filtered\": false}, {\"source\": \"Wenwen Dou\", \"target\": \"Alireza Karduni\", \"value\": 3, \"filtered\": false}, {\"source\": \"Wenwen Dou\", \"target\": \"Thomas Butkiewicz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ignacio P\\u00e9rez-Messina\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ignacio P\\u00e9rez-Messina\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Christopher Collins 0001\", \"value\": 3, \"filtered\": false}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Fanny Chevalier\", \"value\": 5, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Michael Glueck\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Azam Khan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Simon Breslav\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haijun Xia\", \"target\": \"Zhutian Chen\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haijun Xia\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haijun Xia\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haijun Xia\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Oskar Elek\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oskar Elek\", \"target\": \"J. Xavier Prochaska\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oskar Elek\", \"target\": \"Angus G. Forbes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Yunhai Wang\", \"value\": 15, \"filtered\": false}, {\"source\": \"Oliver Deussen\", \"target\": \"Mennatallah El-Assady\", \"value\": 2, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Fabian Sperrle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Christopher Collins 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Rebecca Kehlbeck\", \"value\": 3, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Bongshin Lee\", \"value\": 2, \"filtered\": false}, {\"source\": \"Oliver Deussen\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Hyeok Kim\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Abhraneel Sarma\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steve DiPaola\", \"target\": \"Tera Marie Green\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steve DiPaola\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Shahid Latif\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": false}, {\"source\": \"Stefan Lindholm\", \"target\": \"Anders Persson\", \"value\": 2, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Gunnar L\\u00e4th\\u00e9n\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Magnus Borga\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Samuel Reinders\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yanna Lin\", \"target\": \"Dongyu Liu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yanna Lin\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yanna Lin\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongyu Liu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dongyu Liu\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongyu Liu\", \"target\": \"Aryaman Bahukhandi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael S. Bernstein\", \"target\": \"\\u00c7agatay Demiralp\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael S. Bernstein\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yngve Sekse Kristiansen\", \"target\": \"Laura A. Garrison\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yngve Sekse Kristiansen\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael E. Papka\", \"target\": \"Ratanond Koonchanok\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Haekyu Park\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Nilaksh Das\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Minsuk Kahng\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Duen Horng Chau\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fred Hohman\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Austin P. Wright\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Carla Floricel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gene Golovchinsky\", \"target\": \"Klaus Reichenberger\", \"value\": 2, \"filtered\": true}, {\"source\": \"Gene Golovchinsky\", \"target\": \"Thomas Kamps\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Jeffrey Heer\", \"value\": 5, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Bill Howe\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dominik Moritz\", \"target\": \"Hyeok Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Nils Gehlenborg\", \"value\": 5, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Achim Ebert\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Frank Keul\", \"target\": \"Kay Hamacher\", \"value\": 1, \"filtered\": true}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": false}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Helwig Hauser\", \"value\": 2, \"filtered\": true}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Thomas Spengler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": false}, {\"source\": \"Sana Malik\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Michael Wolverton\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Yihong Wu 0003\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shichao Jia\", \"target\": \"Zeyu Li 0003\", \"value\": 3, \"filtered\": true}, {\"source\": \"Shichao Jia\", \"target\": \"Jiawan Zhang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Thomas Nocke\", \"value\": 2, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Magnus Heitzler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Philipp Muigg\", \"target\": \"Helmut Doleisch\", \"value\": 5, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"Helwig Hauser\", \"value\": 3, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aryaman Bahukhandi\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Magnus Borga\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Xinyue Xu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hannah Kim 0001\", \"target\": \"Alex Endert\", \"value\": 4, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Nilaksh Das\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haekyu Park\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 3, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Austin P. Wright\", \"value\": 1, \"filtered\": true}, {\"source\": \"Laura A. Garrison\", \"target\": \"Stefan Bruckner\", \"value\": 2, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Arjun Srinivasan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Steven Mark Drucker\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Lyndsey Franklin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Subhajit Das 0002\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"Arpit Narechania\", \"value\": 4, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Mennatallah El-Assady\", \"value\": 1, \"filtered\": false}, {\"source\": \"Rui Zhang\", \"target\": \"Youfu Yan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rui Zhang\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rui Zhang\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gromit Yeuk-Yin Chan\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gromit Yeuk-Yin Chan\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Ross Maciejewski\", \"value\": 2, \"filtered\": false}, {\"source\": \"Hanghang Tong\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeremy G. Freeman\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Bongshin Lee\", \"value\": 3, \"filtered\": false}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Mennatallah El-Assady\", \"value\": 2, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Christopher Collins 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Fabian Sperrle\", \"value\": 1, \"filtered\": false}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Yunhai Wang\", \"value\": 2, \"filtered\": false}, {\"source\": \"Mitchell Gordon\", \"target\": \"Huichen Will Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mitchell Gordon\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mitchell Gordon\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Renzhong Li\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Christopher Collins 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Shunan Guo\", \"value\": 4, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Ke Xu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Zhuochen Jin\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Siming Chen 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Qing Chen 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Nan Chen\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Xiaohan Jiao\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Tian Gao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Carla Floricel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Bruckner\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 13, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Johanna Schmidt\", \"value\": 2, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Helwig Hauser\", \"value\": 2, \"filtered\": false}, {\"source\": \"Yifan Wu 0005\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yifan Wu 0005\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Kamps\", \"target\": \"Klaus Reichenberger\", \"value\": 2, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Xiaohan Jiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Tian Gao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Fanny Chevalier\", \"value\": 2, \"filtered\": false}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Mennatallah El-Assady\", \"value\": 4, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Fabian Sperrle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hyeok Kim\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hyeok Kim\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Carla Floricel\", \"target\": \"Guadalupe Canahuate\", \"value\": 2, \"filtered\": true}, {\"source\": \"Carla Floricel\", \"target\": \"Clifton David Fuller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arpit Narechania\", \"target\": \"Alireza Karduni\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Mennatallah El-Assady\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huichen Will Wang\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huichen Will Wang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert A. Lafrance\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert A. Lafrance\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"David J. Israel\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"David J. Israel\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Johanna Schmidt\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": false}, {\"source\": \"Johanna Schmidt\", \"target\": \"Silvia Miksch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Urbanek\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tak Yeon Lee\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tak Yeon Lee\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Miguel Nunes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Benjamin Rowland\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Youfu Yan\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Youfu Yan\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c7agatay Demiralp\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Bill Howe\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Nils Gehlenborg\", \"value\": 7, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Michelle A. Borkin\", \"value\": 2, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Fanny Chevalier\", \"value\": 1, \"filtered\": false}, {\"source\": \"Austin P. Wright\", \"target\": \"Nilaksh Das\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Omar Shaikh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziao Liu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziao Liu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aritra Dasgupta\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Alexandra La Cruz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Dominik Fleischmann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Zeyu Li 0003\", \"target\": \"Jiawan Zhang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Kresimir Matkovic\", \"value\": 2, \"filtered\": false}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tera Marie Green\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yoon Kim\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Fabian Sperrle\", \"value\": 4, \"filtered\": true}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Davide Ceneda\", \"value\": 2, \"filtered\": false}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason D. Hartline\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jason D. Hartline\", \"target\": \"Paula Kayongo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason D. Hartline\", \"target\": \"Glenn Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Sperrle\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Vidya Setlur\", \"target\": \"Arjun Srinivasan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Vidya Setlur\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Miguel Nunes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Glenn Sun\", \"target\": \"Paula Kayongo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Glenn Sun\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eston Schweickart\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eston Schweickart\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Xiaohan Jiao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guadalupe Canahuate\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Yun Wang 0012\", \"value\": 4, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Dongmei Zhang 0001\", \"value\": 4, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"J. Xavier Prochaska\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"J. Xavier Prochaska\", \"target\": \"Angus G. Forbes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiazhe Wang\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Spengler\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Davide Ceneda\", \"target\": \"Silvia Miksch\", \"value\": 3, \"filtered\": true}, {\"source\": \"Davide Ceneda\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Halden Lin\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Halden Lin\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dennis Chau\", \"target\": \"Yiwen Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dennis Chau\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Helmut Doleisch\", \"value\": 5, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Silvia Miksch\", \"value\": 3, \"filtered\": false}, {\"source\": \"Helwig Hauser\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Kresimir Matkovic\", \"value\": 10, \"filtered\": false}, {\"source\": \"Helwig Hauser\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Yun Wang 0012\", \"value\": 3, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ke Xu\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ke Xu\", \"target\": \"Xiaohan Jiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Tan Tang\", \"value\": 6, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Lu Ying\", \"value\": 2, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Subhajit Das 0002\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Jiawan Zhang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Siming Chen 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhutian Chen\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Yong Wang 0021\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Michelle A. Borkin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Nils Gehlenborg\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jessica Hullman\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jessica Hullman\", \"target\": \"Paula Kayongo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jessica Hullman\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Borga\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Borga\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jovan Popovic\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Duen Horng Chau\", \"target\": \"Minsuk Kahng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nils Gehlenborg\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Thomas Ertl\", \"value\": 13, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Johannes Knittel\", \"value\": 3, \"filtered\": true}, {\"source\": \"Paola Valdivia\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paola Valdivia\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Hui Zhang 0051\", \"value\": 9, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Songheng Zhang\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": true}, {\"source\": \"Songheng Zhang\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Bongshin Lee\", \"value\": 3, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leni Yang\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Leni Yang\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kim Marriott\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kim Marriott\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kim Marriott\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"Dominik Fleischmann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yiwen Sun\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Ross Maciejewski\", \"value\": 2, \"filtered\": false}, {\"source\": \"Thomas Ertl\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Johannes Knittel\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yongkang Xiao\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"R. Jordan Crouser\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"R. Jordan Crouser\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": false}, {\"source\": \"Angus G. Forbes\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lu Ying\", \"target\": \"Tan Tang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Zehua Zeng\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abhraneel Sarma\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nilaksh Das\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nilaksh Das\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nilaksh Das\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nilaksh Das\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chunyao Qian\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chunyao Qian\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shizhao Sun\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Minsuk Kahng\", \"value\": 3, \"filtered\": false}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Michael Glueck\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Fanny Chevalier\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Azam Khan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tan Tang\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tan Tang\", \"target\": \"Johannes Knittel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Fleischmann\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 5, \"filtered\": true}, {\"source\": \"Dominik Fleischmann\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Lyndsey Franklin\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yun Wang 0012\", \"target\": \"Dongmei Zhang 0001\", \"value\": 6, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yun Wang 0012\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"John Thompson 0002\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Bongshin Lee\", \"target\": \"Yunhai Wang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenchao Wu\", \"target\": \"Yixian Zheng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Wenchao Wu\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Wenchao Wu\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yusheng Qi\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": false}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yunjeong Chang\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Nocke\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Nocke\", \"target\": \"Magnus Heitzler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yong Wang 0021\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yong Wang 0021\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Minsuk Kahng\", \"target\": \"Omar Shaikh\", \"value\": 1, \"filtered\": false}, {\"source\": \"Reiner Lenz\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Michael Glueck\", \"value\": 5, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Azam Khan\", \"value\": 5, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Chen\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arnold K\\u00f6chl\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiaohan Jiao\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaohan Jiao\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Oeltze\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bill Howe\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joe Bruce\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Omar Shaikh\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helmut Doleisch\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hui Zhang 0051\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ying Chen\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Heitzler\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Azam Khan\", \"target\": \"Michael Glueck\", \"value\": 5, \"filtered\": true}, {\"source\": \"Ryan Wesslen\", \"target\": \"Alireza Karduni\", \"value\": 4, \"filtered\": true}, {\"source\": \"Po-Ming Law\", \"target\": \"Yixian Zheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ingrid Zukerman\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bei Chen\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuhan Liu\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"M. Eduard Gr\\u00f6ller\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"M. Eduard Gr\\u00f6ller\", \"target\": \"Yixian Zheng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans-J\\u00f6rg Schulz\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Catherine Plaisant\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sol\\u00e9akh\\u00e9na Ken\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}]}\nconst { Graph } = G6\nconst graph_13d6b349 = new Graph(\n{\ncontainer: 'network_a4d136b6-c3e1-40cb-9898-8d49a89297a5',\nautoFit: 'view',\ndata:data_13d6b349,\nlayout: {\ntype: 'force-atlas2',\npreventOverlap: true,\nkr: 20,\ncenter: [250, 250],\n},\nbehaviors: ['drag-canvas', 'zoom-canvas', 'drag-element'],\nedge: {\nstyle: {\nlineWidth: 2,\nopacity: d => d.filtered ? 1 : 0.4},\n},\nnode: {\nstyle: {\nlabelText: d => d.id,\n},\n},\n});\ngraph_13d6b349.render();\n</script>\n    \n        </body>\n        </html>\n        "
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nDataset path used: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nTotal papers processed: 83\nTotal unique authors (deduped): 311\n### End of facts\n### Begin of facts\nCleaned author list sample (first 30):\n- Abdallah Sherif Radwan Mohamed\n- Abhraneel Sarma\n- Achim Ebert\n- Adam M. Smith 0001\n- Aditeya Pandey\n- Alan Lundgard\n- Alex Endert\n- Alexandra La Cruz\n- Alexis Pister\n- Alireza Karduni\n- Alvitta Ottley\n- Anders Persson\n- Andrew E. Johnson 0001\n- Andrew Wentzel\n- Angus G. Forbes\n- Anne Laprie\n- Anushka Anand\n- Aoyu Wu\n- Aritra Dasgupta\n- Arjun Srinivasan\n- Arnold K\u00f6chl\n- Arpit Narechania\n- Arvind Satyanarayan\n- Aryaman Bahukhandi\n- Austin P. Wright\n- Azam Khan\n- Bahador Saket\n- Bei Chen\n- Benjamin Rowland\n- Bernhard Pointner\n### End of facts\n### Begin of facts\nAmbiguous author name examples (heuristic flags \u2014 needs manual review):\n- Eli T. Brown\n- Kristin A. Cook\n- Ryan A. Rossi\n- Daniel A. Keim\n- G. Elisabeta Marai\n- Jason D. Hartline\n- R. Jordan Crouser\n- Adam M. Smith 0001\n- Andrew E. Johnson 0001\n- Angus G. Forbes\n### End of facts\n### Begin of facts\nCo-authorship edge counts: total edges = 895\nEdge weight distribution (counts):\n weight=1: 822 edges\n weight=2: 60 edges\n weight=3: 12 edges\n weight=4: 1 edges\nRecommended minimum coauthorship weight threshold: 2\nEdges remaining at thresholds:\n >= 1: 895 edges\n >= 2: 73 edges\n >= 3: 13 edges\n### End of facts\n### Begin of facts\nTop authors by publication count (author \u2014 pubs \u2014 weighted_degree \u2014 betweenness):\n- Alex Endert \u2014 pubs:7 \u2014 wdeg:29.0 \u2014 betw:0.0166\n- Yingcai Wu \u2014 pubs:6 \u2014 wdeg:37.0 \u2014 betw:0.0049\n- Huamin Qu \u2014 pubs:6 \u2014 wdeg:26.0 \u2014 betw:0.0082\n- Nan Cao 0001 \u2014 pubs:5 \u2014 wdeg:27.0 \u2014 betw:0.0058\n- Jeffrey Heer \u2014 pubs:5 \u2014 wdeg:20.0 \u2014 betw:0.0093\n- Dongmei Zhang 0001 \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Haidong Zhang \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Dominik Moritz \u2014 pubs:4 \u2014 wdeg:19.0 \u2014 betw:0.0121\n- Jessica Hullman \u2014 pubs:4 \u2014 wdeg:14.0 \u2014 betw:0.0032\n- Lingyun Yu 0001 \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n### End of facts\n### Begin of facts\nTop bridging/central authors by betweenness (author \u2014 betweenness \u2014 degree \u2014 pubs):\n- Alex Endert \u2014 betw:0.0166 \u2014 deg:24 \u2014 pubs:7\n- Guande Wu \u2014 betw:0.0131 \u2014 deg:11 \u2014 pubs:2\n- Dominik Moritz \u2014 betw:0.0121 \u2014 deg:16 \u2014 pubs:4\n- Chenglong Wang \u2014 betw:0.0110 \u2014 deg:8 \u2014 pubs:2\n- Leilani Battle \u2014 betw:0.0109 \u2014 deg:12 \u2014 pubs:3\n- Bongshin Lee \u2014 betw:0.0099 \u2014 deg:11 \u2014 pubs:3\n- Jeffrey Heer \u2014 betw:0.0093 \u2014 deg:16 \u2014 pubs:5\n- Huamin Qu \u2014 betw:0.0082 \u2014 deg:22 \u2014 pubs:6\n- Ryan A. Rossi \u2014 betw:0.0065 \u2014 deg:9 \u2014 pubs:3\n- Emily Wall \u2014 betw:0.0065 \u2014 deg:10 \u2014 pubs:3\n### End of facts\n### Begin of facts\nCommunity detection summary (using greedy modularity on thresholded graph):\nNumber of communities: 267\nCommunity sizes (top 10): [10, 6, 6, 6, 6, 5, 5, 4, 3, 3]\n Community 0 (size 10): Xiao Xie, Yingcai Wu, Lingyun Yu 0001, Huamin Qu, Hui Zhang 0051, Lu Ying, Haotian Li 0001, Aoyu Wu, ...\n Community 1 (size 6): Emily Wall, Kristin A. Cook, Lyndsey Franklin, Alex Endert, Eli T. Brown, Nick Cramer\n Community 2 (size 6): Weiwei Cui, Jian-Guang Lou, He Huang, Yun Wang 0012, Dongmei Zhang 0001, Haidong Zhang\n Community 3 (size 6): G. Elisabeta Marai, Andrew Wentzel, Abdallah Sherif Radwan Mohamed, Carla Floricel, Clifton David Fuller, Guadalupe Canahuate\n Community 4 (size 6): Jessica Hullman, Eunyee Koh, Ryan A. Rossi, Jane Hoffswell, Jason D. Hartline, Hyeok Kim\n### End of facts\n### Begin of facts\nSuggested visualization parameters:\n- Minimum publications per author to display: 2 (recommended; increase to 3-5 to focus on cores)\n- Minimum coauthorship weight to display edges: 2 (recommended based on data; 822 edges are weight=1) \n- Layout: force-directed (e.g., spring) with node size ~ pub_count and edge width ~ weight\nShortlist of top candidate authors to highlight as bridges/cores:\n- Alex Endert \u2014 pubs:7 \u2014 wdeg:29.0 \u2014 betw:0.0166\n- Guande Wu \u2014 pubs:2 \u2014 wdeg:11.0 \u2014 betw:0.0131\n- Dominik Moritz \u2014 pubs:4 \u2014 wdeg:19.0 \u2014 betw:0.0121\n- Chenglong Wang \u2014 pubs:2 \u2014 wdeg:8.0 \u2014 betw:0.0110\n- Leilani Battle \u2014 pubs:3 \u2014 wdeg:14.0 \u2014 betw:0.0109\n- Bongshin Lee \u2014 pubs:3 \u2014 wdeg:11.0 \u2014 betw:0.0099\n- Jeffrey Heer \u2014 pubs:5 \u2014 wdeg:20.0 \u2014 betw:0.0093\n- Huamin Qu \u2014 pubs:6 \u2014 wdeg:26.0 \u2014 betw:0.0082\n- Ryan A. Rossi \u2014 pubs:3 \u2014 wdeg:12.0 \u2014 betw:0.0065\n- Emily Wall \u2014 pubs:3 \u2014 wdeg:11.0 \u2014 betw:0.0065\n- Yingcai Wu \u2014 pubs:6 \u2014 wdeg:37.0 \u2014 betw:0.0049\n- Nan Cao 0001 \u2014 pubs:5 \u2014 wdeg:27.0 \u2014 betw:0.0058\n- Dongmei Zhang 0001 \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Haidong Zhang \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Tan Tang \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n- Lingyun Yu 0001 \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n- Yun Wang 0012 \u2014 pubs:3 \u2014 wdeg:20.0 \u2014 betw:0.0017\n### End of facts\n\nSaved node metrics -> autovis_nodes_metrics.csv and edge list -> autovis_edge_list.csv\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                },
                {
                    "analysis_schema": {
                        "action": "present",
                        "information_needed": {
                            "question_text": "Which authors, papers, and institutions have the greatest influence in AutoVis (by citations and downloads), and how has author participation evolved over time in terms of newcomers versus long-term contributors?",
                            "primary_attributes": [
                                "AuthorNames-Deduped",
                                "CitationCount_CrossRef"
                            ],
                            "secondary_attributes": [
                                "AuthorAffiliation (derived institution)",
                                "Year"
                            ],
                            "transformation": [
                                "Aggregate: sum CitationCount_CrossRef (and AminerCitationCount) per author and per institution.",
                                "Derive first_year and last_year of publication per author; compute publications_count per author.",
                                "Classify authors by tenure: 'newcomer' if first_year >= max_year - 2 (configurable window), 'long-term' if publications_count >= 3 (configurable).",
                                "Rank: top-N authors/papers/institutions by aggregated citation counts and by Downloads_Xplore for papers."
                            ],
                            "expected_insight_types": [
                                "top (top authors, papers, and institutions by citations/downloads)",
                                "trend (newcomers vs long-term contributor counts over years)",
                                "distribution (distribution of citations across authors/papers)",
                                "outliers (highly cited or highly downloaded papers/authors)"
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n                "
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nTop 5 authors by CrossRef citations:\nJeffrey Heer: citations=551, aminer_citations=841, papers=5, years=2014-2024\nDominik Moritz: citations=480, aminer_citations=718, papers=4, years=2015-2023\nBill Howe: citations=469, aminer_citations=712, papers=2, years=2015-2018\nAlex Endert: citations=436, aminer_citations=519, papers=7, years=2014-2018\nKanit Wongsuphasawat: citations=292, aminer_citations=487, papers=1, years=2015-2015\n### End of facts\n### Begin of facts\nTop 5 papers by CrossRef citations:\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 citations=292, downloads=4307, DOI=10.1109/tvcg.2015.2467191\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 citations=177, downloads=3238, DOI=10.1109/tvcg.2018.2865240\nA Design Space of Visualization Tasks (2013) \u2014 citations=144, downloads=4884, DOI=10.1109/tvcg.2013.120\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 citations=121, downloads=2942, DOI=10.1109/tvcg.2018.2865145\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 citations=106, downloads=2108, DOI=10.1109/vast47406.2019.8986948\n### End of facts\n### Begin of facts\nTop 5 papers by Downloads_Xplore:\nA Design Space of Visualization Tasks (2013) \u2014 downloads=4884, citations=144, DOI=10.1109/tvcg.2013.120\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 downloads=4307, citations=292, DOI=10.1109/tvcg.2015.2467191\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 downloads=3724, citations=80, DOI=10.1109/tvcg.2020.3030403\nKG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation (2021) \u2014 downloads=3452, citations=69, DOI=10.1109/tvcg.2021.3114863\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 downloads=3238, citations=177, DOI=10.1109/tvcg.2018.2865240\n### End of facts\n### Begin of facts\nTop 5 institutions by aggregated CrossRef citations:\nUniversity of Washington: citations=2024, papers_counted=17\nGeorgia Institute of Technology: citations=1545, papers_counted=24\nTableau Research: citations=892, papers_counted=5\nMicrosoft Research Asia: citations=832, papers_counted=20\nAdobe Research: citations=539, papers_counted=17\n### End of facts\n### Begin of facts\nAuthor participation trend (newcomer vs long-term) by year:\n1995: newcomers=0, long-term=0, other=3\n2004: newcomers=0, long-term=0, other=11\n2006: newcomers=0, long-term=0, other=1\n2007: newcomers=0, long-term=0, other=5\n2008: newcomers=0, long-term=1, other=4\n2009: newcomers=0, long-term=0, other=4\n2010: newcomers=0, long-term=0, other=3\n2011: newcomers=0, long-term=1, other=8\n2012: newcomers=0, long-term=0, other=5\n2013: newcomers=0, long-term=0, other=4\n2014: newcomers=0, long-term=4, other=18\n2015: newcomers=0, long-term=4, other=15\n2016: newcomers=0, long-term=4, other=25\n2017: newcomers=0, long-term=3, other=5\n2018: newcomers=0, long-term=3, other=17\n2019: newcomers=0, long-term=4, other=16\n2020: newcomers=0, long-term=8, other=28\n2021: newcomers=0, long-term=17, other=66\n2022: newcomers=12, long-term=8, other=2\n2023: newcomers=28, long-term=12, other=13\n2024: newcomers=29, long-term=4, other=2\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                }
            ],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "Authorship and institutional analysis show a concentrated but connected landscape: a modest set of authors and groups produce many of the high-impact AutoVis contributions, while a long tail of one\u2011off collaborators expands application breadth. Collaboration clusters form around established labs and industrial research groups; authors such as Alex Endert, Huamin Qu, Yingcai Wu, Jeffrey Heer, Dominik Moritz and Nan Cao recur as both high-output and high-impact contributors, and several authors serve as structural bridges connecting different sub-communities (high betweenness). Institutions with outsized aggregated citation counts include University of Washington, Georgia Tech, Microsoft Research Asia, Tableau Research, and Adobe Research, reflecting a mix of academic and industrial leadership."
                },
                {
                    "id": 1,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "antv",
                        "specification": "\n        <!DOCTYPE html>\n        <html>\n        <head>\n            <meta charset=\"UTF-8\">\n        </head>\n        <body>\n            \n    <div id=\"network_a4d136b6-c3e1-40cb-9898-8d49a89297a5\" style=\"width: 100%px; height: 100%px;\"></div>\n    <script src=\"https://unpkg.com/@antv/g6@5/dist/g6.min.js\"></script>\n    <script>\nconst data_13d6b349 = {\"nodes\": [{\"id\": \"Daniel A. Keim\"}, {\"id\": \"Sebastian Bremm\"}, {\"id\": \"William Ribarsky\"}, {\"id\": \"Weiwei Cui\"}, {\"id\": \"Nick Cramer\"}, {\"id\": \"Han-Wei Shen\"}, {\"id\": \"Nuo Chen\"}, {\"id\": \"Jonathan P. Leidig\"}, {\"id\": \"Mira Dontcheva\"}, {\"id\": \"Jane Hoffswell\"}, {\"id\": \"Hans Hagen\"}, {\"id\": \"Richard Souvenir\"}, {\"id\": \"Tatiana von Landesberger\"}, {\"id\": \"Bahador Saket\"}, {\"id\": \"Dazhen Deng\"}, {\"id\": \"Jian-Guang Lou\"}, {\"id\": \"Anushka Anand\"}, {\"id\": \"Zheng Zhou\"}, {\"id\": \"Nam Wook Kim\"}, {\"id\": \"Jochen Ehret\"}, {\"id\": \"Kan Ren\"}, {\"id\": \"Emily Wall\"}, {\"id\": \"G. Elisabeta Marai\"}, {\"id\": \"Joon-Yong Lee\"}, {\"id\": \"Nafiul Nipu\"}, {\"id\": \"Walter Schubert\"}, {\"id\": \"Yuchen Yang\"}, {\"id\": \"Eli T. Brown\"}, {\"id\": \"Xiangtong Chu\"}, {\"id\": \"Haidong Zhang\"}, {\"id\": \"Arvind Satyanarayan\"}, {\"id\": \"Jason Leigh\"}, {\"id\": \"Jamie Morgenstern\"}, {\"id\": \"Andrew Wentzel\"}, {\"id\": \"Jiahang Xu\"}, {\"id\": \"Aoyu Wu\"}, {\"id\": \"Bernhard Pointner\"}, {\"id\": \"Danqing Shi\"}, {\"id\": \"Abdallah Sherif Radwan Mohamed\"}, {\"id\": \"Bernhard Preim\"}, {\"id\": \"Stephen C. North\"}, {\"id\": \"Jean-Daniel Fekete\"}, {\"id\": \"Peiran Ren\"}, {\"id\": \"Yingcai Wu\"}, {\"id\": \"Ravish Chawla\"}, {\"id\": \"Kanit Wongsuphasawat\"}, {\"id\": \"Yuqing Yang 0001\"}, {\"id\": \"Junpeng Wang 0001\"}, {\"id\": \"Yang Shi 0007\"}, {\"id\": \"Michalis Mamakos\"}, {\"id\": \"Lars Schuchardt\"}, {\"id\": \"Xinke Wu\"}, {\"id\": \"Huamin Qu\"}, {\"id\": \"Eunyee Koh\"}, {\"id\": \"Kristin A. Cook\"}, {\"id\": \"Shuainan Ye\"}, {\"id\": \"Ziyang Guo\"}, {\"id\": \"Zachary Wartell\"}, {\"id\": \"Chenglong Wang\"}, {\"id\": \"Remco Chang\"}, {\"id\": \"Lvkeshen Shen\"}, {\"id\": \"Alvitta Ottley\"}, {\"id\": \"\\u00c1ngel Alexander Cabrera\"}, {\"id\": \"Jock D. Mackinlay\"}, {\"id\": \"Chen Chen 0080\"}, {\"id\": \"Paolo Buono\"}, {\"id\": \"Khairi Reda\"}, {\"id\": \"Helen Zhao 0001\"}, {\"id\": \"John T. Stasko\"}, {\"id\": \"Yuge Zhang\"}, {\"id\": \"Tamara Munzner\"}, {\"id\": \"Samuel H. Payne\"}, {\"id\": \"Leslie M. Blaha\"}, {\"id\": \"Liang Gou\"}, {\"id\": \"Moqi He\"}, {\"id\": \"Zui Chen\"}, {\"id\": \"Adam M. Smith 0001\"}, {\"id\": \"Junran Yang\"}, {\"id\": \"Zhe Xu 0007\"}, {\"id\": \"Milos Sr\\u00e1mek\"}, {\"id\": \"Mengyu Zhou\"}, {\"id\": \"Qianwen Wang\"}, {\"id\": \"Robert Kincaid\"}, {\"id\": \"Olav Lenz\"}, {\"id\": \"Haotian Li 0001\"}, {\"id\": \"Carlos Eduardo Scheidegger\"}, {\"id\": \"Kwan-Liu Ma\"}, {\"id\": \"Xiaoyu Zhang 0014\"}, {\"id\": \"Wenwen Dou\"}, {\"id\": \"Ignacio P\\u00e9rez-Messina\"}, {\"id\": \"Jian Zhao 0010\"}, {\"id\": \"Haijun Xia\"}, {\"id\": \"Oskar Elek\"}, {\"id\": \"Oliver Deussen\"}, {\"id\": \"Ryan A. Rossi\"}, {\"id\": \"Steve DiPaola\"}, {\"id\": \"Yuzhe Luo\"}, {\"id\": \"Fabian Beck 0001\"}, {\"id\": \"Stefan Lindholm\"}, {\"id\": \"Will Epperson\"}, {\"id\": \"Matthew Butler 0002\"}, {\"id\": \"Yanna Lin\"}, {\"id\": \"Reyk Hillert\"}, {\"id\": \"Dongyu Liu\"}, {\"id\": \"Michael S. Bernstein\"}, {\"id\": \"Yngve Sekse Kristiansen\"}, {\"id\": \"Michael E. Papka\"}, {\"id\": \"Fred Hohman\"}, {\"id\": \"Lisanne van Dijk\"}, {\"id\": \"Gene Golovchinsky\"}, {\"id\": \"Dominik Moritz\"}, {\"id\": \"Sehi L'Yi\"}, {\"id\": \"Achim Ebert\"}, {\"id\": \"Frank Keul\"}, {\"id\": \"Juraj P\\u00e1lenik\"}, {\"id\": \"Sana Malik\"}, {\"id\": \"Michael Wolverton\"}, {\"id\": \"Wenshuo Zhao\"}, {\"id\": \"Shichao Jia\"}, {\"id\": \"Heidrun Schumann\"}, {\"id\": \"Philipp Muigg\"}, {\"id\": \"Aryaman Bahukhandi\"}, {\"id\": \"Gunnar L\\u00e4th\\u00e9n\"}, {\"id\": \"Fuling Sun\"}, {\"id\": \"Ratanond Koonchanok\"}, {\"id\": \"Hannah Kim 0001\"}, {\"id\": \"Yihong Wu 0003\"}, {\"id\": \"Haekyu Park\"}, {\"id\": \"Laura A. Garrison\"}, {\"id\": \"Phoebe Moh\"}, {\"id\": \"Alex Endert\"}, {\"id\": \"Rui Zhang\"}, {\"id\": \"Liqi Cheng\"}, {\"id\": \"Gromit Yeuk-Yin Chan\"}, {\"id\": \"Hanghang Tong\"}, {\"id\": \"Jeremy G. Freeman\"}, {\"id\": \"Steven Mark Drucker\"}, {\"id\": \"Rebecca Kehlbeck\"}, {\"id\": \"Mitchell Gordon\"}, {\"id\": \"Xinyue Xu\"}, {\"id\": \"Renzhong Li\"}, {\"id\": \"Nan Cao 0001\"}, {\"id\": \"Mikayla Biggs\"}, {\"id\": \"Stefan Bruckner\"}, {\"id\": \"Yifan Wu 0005\"}, {\"id\": \"Thomas Kamps\"}, {\"id\": \"Thomas Butkiewicz\"}, {\"id\": \"Zhuochen Jin\"}, {\"id\": \"Christopher Collins 0001\"}, {\"id\": \"Hyeok Kim\"}, {\"id\": \"Fan Du\"}, {\"id\": \"Carla Floricel\"}, {\"id\": \"Arpit Narechania\"}, {\"id\": \"Huichen Will Wang\"}, {\"id\": \"Robert A. Lafrance\"}, {\"id\": \"David J. Israel\"}, {\"id\": \"Johanna Schmidt\"}, {\"id\": \"Simon Urbanek\"}, {\"id\": \"Wilmot Li\"}, {\"id\": \"Wei Shuai\"}, {\"id\": \"Tak Yeon Lee\"}, {\"id\": \"Anne Laprie\"}, {\"id\": \"Youfu Yan\"}, {\"id\": \"\\u00c7agatay Demiralp\"}, {\"id\": \"Jeffrey Heer\"}, {\"id\": \"Hanspeter Pfister\"}, {\"id\": \"Xinhuan Shu\"}, {\"id\": \"Austin P. Wright\"}, {\"id\": \"Ziao Liu\"}, {\"id\": \"Aritra Dasgupta\"}, {\"id\": \"Mat\\u00fas Straka\"}, {\"id\": \"Zeyu Li 0003\"}, {\"id\": \"Wolfgang Freiler\"}, {\"id\": \"Tera Marie Green\"}, {\"id\": \"Yoon Kim\"}, {\"id\": \"Samuel Reinders\"}, {\"id\": \"Mennatallah El-Assady\"}, {\"id\": \"Michael Oppermann\"}, {\"id\": \"Jason D. Hartline\"}, {\"id\": \"Fabian Sperrle\"}, {\"id\": \"Vidya Setlur\"}, {\"id\": \"Benjamin Rowland\"}, {\"id\": \"Glenn Sun\"}, {\"id\": \"Eston Schweickart\"}, {\"id\": \"Tian Gao\"}, {\"id\": \"Guadalupe Canahuate\"}, {\"id\": \"He Huang\"}, {\"id\": \"J. Xavier Prochaska\"}, {\"id\": \"Jiazhe Wang\"}, {\"id\": \"Thomas Spengler\"}, {\"id\": \"Davide Ceneda\"}, {\"id\": \"Halden Lin\"}, {\"id\": \"Dennis Chau\"}, {\"id\": \"Helwig Hauser\"}, {\"id\": \"Sam Yu-Te Lee\"}, {\"id\": \"Ke Xu\"}, {\"id\": \"Alan Lundgard\"}, {\"id\": \"Kay Hamacher\"}, {\"id\": \"Lingyun Yu 0001\"}, {\"id\": \"Subhajit Das 0002\"}, {\"id\": \"Siming Chen 0001\"}, {\"id\": \"Hao Yang 0007\"}, {\"id\": \"Zhutian Chen\"}, {\"id\": \"Aditeya Pandey\"}, {\"id\": \"Jessica Hullman\"}, {\"id\": \"Magnus Borga\"}, {\"id\": \"Jovan Popovic\"}, {\"id\": \"Duen Horng Chau\"}, {\"id\": \"Nils Gehlenborg\"}, {\"id\": \"Gordon Woodhull\"}, {\"id\": \"Steffen Koch 0001\"}, {\"id\": \"Paola Valdivia\"}, {\"id\": \"Xiao Xie\"}, {\"id\": \"Songheng Zhang\"}, {\"id\": \"Zhicheng Liu 0001\"}, {\"id\": \"Leni Yang\"}, {\"id\": \"Kim Marriott\"}, {\"id\": \"Alexandra La Cruz\"}, {\"id\": \"Yiwen Sun\"}, {\"id\": \"Thomas Ertl\"}, {\"id\": \"Yongkang Xiao\"}, {\"id\": \"R. Jordan Crouser\"}, {\"id\": \"Angus G. Forbes\"}, {\"id\": \"Lu Ying\"}, {\"id\": \"Zehua Zeng\"}, {\"id\": \"Leilani Battle\"}, {\"id\": \"Santhosh Dharmapuri\"}, {\"id\": \"Michelle A. Borkin\"}, {\"id\": \"Guande Wu\"}, {\"id\": \"Abhraneel Sarma\"}, {\"id\": \"Nilaksh Das\"}, {\"id\": \"Arjun Srinivasan\"}, {\"id\": \"Miguel Nunes\"}, {\"id\": \"Chunyao Qian\"}, {\"id\": \"Qing Chen 0001\"}, {\"id\": \"Shizhao Sun\"}, {\"id\": \"Ross Maciejewski\"}, {\"id\": \"Duen Horng (Polo) Chau\"}, {\"id\": \"Chin-Yew Lin\"}, {\"id\": \"Simon Breslav\"}, {\"id\": \"Tan Tang\"}, {\"id\": \"Dominik Fleischmann\"}, {\"id\": \"Ryan Wilson\"}, {\"id\": \"Lyndsey Franklin\"}, {\"id\": \"Andrew E. Johnson 0001\"}, {\"id\": \"Yun Wang 0012\"}, {\"id\": \"Bongshin Lee\"}, {\"id\": \"Wenchao Wu\"}, {\"id\": \"Yusheng Qi\"}, {\"id\": \"Kresimir Matkovic\"}, {\"id\": \"Yunjeong Chang\"}, {\"id\": \"Thomas Nocke\"}, {\"id\": \"Yong Wang 0021\"}, {\"id\": \"Joseph N. Burchett\"}, {\"id\": \"Minsuk Kahng\"}, {\"id\": \"Reiner Lenz\"}, {\"id\": \"Katja B\\u00fchler\"}, {\"id\": \"Klaus Reichenberger\"}, {\"id\": \"Fanny Chevalier\"}, {\"id\": \"Nan Chen\"}, {\"id\": \"Dongmei Zhang 0001\"}, {\"id\": \"Arnold K\\u00f6chl\"}, {\"id\": \"Xiaohan Jiao\"}, {\"id\": \"Steffen Oeltze\"}, {\"id\": \"Bill Howe\"}, {\"id\": \"Bharath Kalidindi\"}, {\"id\": \"Joe Bruce\"}, {\"id\": \"Omar Shaikh\"}, {\"id\": \"Helmut Doleisch\"}, {\"id\": \"Jiawan Zhang\"}, {\"id\": \"Xinyi He\"}, {\"id\": \"Hui Zhang 0051\"}, {\"id\": \"Ying Chen\"}, {\"id\": \"Magnus Heitzler\"}, {\"id\": \"Azam Khan\"}, {\"id\": \"Shunan Guo\"}, {\"id\": \"Ryan Wesslen\"}, {\"id\": \"Jinpeng Wang 0001\"}, {\"id\": \"Clifton David Fuller\"}, {\"id\": \"John Thompson 0002\"}, {\"id\": \"Rahul Duggal\"}, {\"id\": \"Po-Ming Law\"}, {\"id\": \"Alireza Karduni\"}, {\"id\": \"Ingrid Zukerman\"}, {\"id\": \"Russ Burtner\"}, {\"id\": \"Bei Chen\"}, {\"id\": \"Shuhan Liu\"}, {\"id\": \"M. Eduard Gr\\u00f6ller\"}, {\"id\": \"Yangqiu Song\"}, {\"id\": \"Lizhen Qu\"}, {\"id\": \"Bingchang Chen\"}, {\"id\": \"Michael Glueck\"}, {\"id\": \"Hans-J\\u00f6rg Schulz\"}, {\"id\": \"Quan Lin\"}, {\"id\": \"Catherine Plaisant\"}, {\"id\": \"Johannes Knittel\"}, {\"id\": \"Sol\\u00e9akh\\u00e9na Ken\"}, {\"id\": \"Yixuan Li\"}, {\"id\": \"Yu Hou\"}, {\"id\": \"Yixian Zheng\"}, {\"id\": \"Paula Kayongo\"}, {\"id\": \"Greg L. Nelson\"}, {\"id\": \"Alexis Pister\"}, {\"id\": \"Matthias Schlachter\"}, {\"id\": \"Lei Fang 0004\"}, {\"id\": \"Heidrun Steinmetz\"}, {\"id\": \"Silvia Miksch\"}, {\"id\": \"Shahid Latif\"}, {\"id\": \"Anders Persson\"}, {\"id\": \"Yunhai Wang\"}], \"edges\": [{\"source\": \"Daniel A. Keim\", \"target\": \"Stephen C. North\", \"value\": 5, \"filtered\": false}, {\"source\": \"Daniel A. Keim\", \"target\": \"Oliver Deussen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Mennatallah El-Assady\", \"value\": 3, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Fabian Sperrle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Christopher Collins 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Daniel A. Keim\", \"target\": \"Rebecca Kehlbeck\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Tatiana von Landesberger\", \"value\": 4, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Kay Hamacher\", \"value\": 2, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Olav Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sebastian Bremm\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Wenwen Dou\", \"value\": 12, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Remco Chang\", \"value\": 12, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Tera Marie Green\", \"value\": 2, \"filtered\": false}, {\"source\": \"William Ribarsky\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": false}, {\"source\": \"William Ribarsky\", \"target\": \"Thomas Butkiewicz\", \"value\": 3, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Zachary Wartell\", \"value\": 1, \"filtered\": true}, {\"source\": \"William Ribarsky\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Huamin Qu\", \"value\": 12, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yingcai Wu\", \"value\": 4, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yun Wang 0012\", \"value\": 5, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Haidong Zhang\", \"value\": 7, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Dongmei Zhang 0001\", \"value\": 5, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"He Huang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Jian-Guang Lou\", \"value\": 2, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Jian Zhao 0010\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Yong Wang 0021\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Qianwen Wang\", \"value\": 2, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Weiwei Cui\", \"target\": \"Yang Shi 0007\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": false}, {\"source\": \"Weiwei Cui\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nick Cramer\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Joon-Yong Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Kristin A. Cook\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Michael Wolverton\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nick Cramer\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Junpeng Wang 0001\", \"value\": 4, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Liang Gou\", \"value\": 2, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Han-Wei Shen\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nuo Chen\", \"target\": \"Shichao Jia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nuo Chen\", \"target\": \"Zeyu Li 0003\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nuo Chen\", \"target\": \"Jiawan Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jonathan P. Leidig\", \"target\": \"Santhosh Dharmapuri\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Nam Wook Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Wilmot Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mira Dontcheva\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Arvind Satyanarayan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Fan Du\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Sana Malik\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Eunyee Koh\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Ryan A. Rossi\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jane Hoffswell\", \"target\": \"Hyeok Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jane Hoffswell\", \"target\": \"Huichen Will Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans Hagen\", \"target\": \"Jochen Ehret\", \"value\": 2, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Lars Schuchardt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hans Hagen\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Alvitta Ottley\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Richard Souvenir\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Kay Hamacher\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Olav Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tatiana von Landesberger\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Bahador Saket\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bahador Saket\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bahador Saket\", \"target\": \"Alex Endert\", \"value\": 3, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yingcai Wu\", \"value\": 9, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Xiao Xie\", \"value\": 4, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Hui Zhang 0051\", \"value\": 4, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Zheng Zhou\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yuchen Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Aoyu Wu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Huamin Qu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dazhen Deng\", \"target\": \"Yihong Wu 0003\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dazhen Deng\", \"target\": \"Moqi He\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"He Huang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Haidong Zhang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Dongmei Zhang 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian-Guang Lou\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Kanit Wongsuphasawat\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Jock D. Mackinlay\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anushka Anand\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Yingcai Wu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zheng Zhou\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Fabian Beck 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zheng Zhou\", \"target\": \"Nam Wook Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nam Wook Kim\", \"target\": \"Hanspeter Pfister\", \"value\": 4, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Wilmot Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nam Wook Kim\", \"target\": \"Fabian Beck 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Lars Schuchardt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jochen Ehret\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Jiahang Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kan Ren\", \"target\": \"Yuqing Yang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Leslie M. Blaha\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Alex Endert\", \"value\": 6, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"John T. Stasko\", \"value\": 1, \"filtered\": false}, {\"source\": \"Emily Wall\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Ravish Chawla\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Eli T. Brown\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Emily Wall\", \"target\": \"Arpit Narechania\", \"value\": 3, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Alireza Karduni\", \"value\": 1, \"filtered\": true}, {\"source\": \"Emily Wall\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Carla Floricel\", \"value\": 3, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Nafiul Nipu\", \"value\": 2, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Andrew Wentzel\", \"value\": 4, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Guadalupe Canahuate\", \"value\": 4, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 2, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"G. Elisabeta Marai\", \"target\": \"\\u00c7agatay Demiralp\", \"value\": 1, \"filtered\": false}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Kristin A. Cook\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joon-Yong Lee\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Andrew Wentzel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nafiul Nipu\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Reyk Hillert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Walter Schubert\", \"target\": \"Bernhard Preim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuchen Yang\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Remco Chang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Alvitta Ottley\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Alex Endert\", \"value\": 3, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Hannah Kim 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Ravish Chawla\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eli T. Brown\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Shuainan Ye\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Yingcai Wu\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xiangtong Chu\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiangtong Chu\", \"target\": \"Huamin Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Yun Wang 0012\", \"value\": 9, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Ke Xu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Dongmei Zhang 0001\", \"value\": 7, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"He Huang\", \"value\": 4, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Aoyu Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Huamin Qu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Chunyao Qian\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haidong Zhang\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haidong Zhang\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Kanit Wongsuphasawat\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Jeffrey Heer\", \"value\": 3, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Alan Lundgard\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"John T. Stasko\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Vidya Setlur\", \"value\": 2, \"filtered\": false}, {\"source\": \"Arvind Satyanarayan\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason Leigh\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Yiwen Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Dennis Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason Leigh\", \"target\": \"Khairi Reda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason Leigh\", \"target\": \"Michael E. Papka\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jamie Morgenstern\", \"target\": \"\\u00c1ngel Alexander Cabrera\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Will Epperson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jamie Morgenstern\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Guadalupe Canahuate\", \"value\": 4, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Abdallah Sherif Radwan Mohamed\", \"value\": 2, \"filtered\": true}, {\"source\": \"Andrew Wentzel\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiahang Xu\", \"target\": \"Yuqing Yang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yingcai Wu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Huamin Qu\", \"value\": 7, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aoyu Wu\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Haotian Li 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aoyu Wu\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Pointner\", \"target\": \"Johanna Schmidt\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Pointner\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Yang Shi 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Danqing Shi\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Carla Floricel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Guadalupe Canahuate\", \"value\": 2, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Lisanne van Dijk\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abdallah Sherif Radwan Mohamed\", \"target\": \"Clifton David Fuller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Steffen Oeltze\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Philipp Muigg\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bernhard Preim\", \"target\": \"Reyk Hillert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Carlos Eduardo Scheidegger\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Simon Urbanek\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stephen C. North\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Catherine Plaisant\", \"value\": 3, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Paolo Buono\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Paola Valdivia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Fanny Chevalier\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jean-Daniel Fekete\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Peiran Ren\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Xinke Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Peiran Ren\", \"target\": \"Yingcai Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Dongyu Liu\", \"value\": 3, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Huamin Qu\", \"value\": 13, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xiao Xie\", \"value\": 15, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Hui Zhang 0051\", \"value\": 12, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xinhuan Shu\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Shuainan Ye\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Zhutian Chen\", \"value\": 5, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Haijun Xia\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Tan Tang\", \"value\": 9, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Renzhong Li\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Xinke Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Shuhan Liu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Johannes Knittel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Steffen Koch 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Lingyun Yu 0001\", \"value\": 6, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Thomas Ertl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Ziyang Guo\", \"value\": 3, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Lu Ying\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yuzhe Luo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Lvkeshen Shen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Yihong Wu 0003\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Moqi He\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yingcai Wu\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Wenshuo Zhao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yingcai Wu\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ravish Chawla\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ravish Chawla\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ravish Chawla\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Dominik Moritz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Jeffrey Heer\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Jock D. Mackinlay\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kanit Wongsuphasawat\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuqing Yang 0001\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuqing Yang 0001\", \"target\": \"Yuge Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junpeng Wang 0001\", \"target\": \"Liang Gou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junpeng Wang 0001\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Nan Cao 0001\", \"value\": 8, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Huamin Qu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yang Shi 0007\", \"target\": \"Xiaohan Jiao\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Siming Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Tian Gao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Zhuochen Jin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yang Shi 0007\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Yifan Wu 0005\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Ziyang Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michalis Mamakos\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lars Schuchardt\", \"target\": \"Achim Ebert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lars Schuchardt\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Renzhong Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinke Wu\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yangqiu Song\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Nan Cao 0001\", \"value\": 5, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Jian Zhao 0010\", \"value\": 3, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Haotian Li 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yong Wang 0021\", \"value\": 11, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Songheng Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Wenchao Wu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yixian Zheng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Zhutian Chen\", \"value\": 5, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Yun Wang 0012\", \"value\": 4, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Qianwen Wang\", \"value\": 6, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Shuainan Ye\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Xinhuan Shu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Mengyu Zhou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Leni Yang\", \"value\": 4, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Yanna Lin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Siming Chen 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huamin Qu\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huamin Qu\", \"target\": \"Nils Gehlenborg\", \"value\": 1, \"filtered\": false}, {\"source\": \"Eunyee Koh\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Fan Du\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Sana Malik\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Vidya Setlur\", \"value\": 1, \"filtered\": false}, {\"source\": \"Eunyee Koh\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Ryan A. Rossi\", \"value\": 2, \"filtered\": true}, {\"source\": \"Eunyee Koh\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kristin A. Cook\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Samuel H. Payne\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Michael Wolverton\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kristin A. Cook\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Shuainan Ye\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Haijun Xia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuainan Ye\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Shuainan Ye\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ziyang Guo\", \"target\": \"Jessica Hullman\", \"value\": 3, \"filtered\": true}, {\"source\": \"Ziyang Guo\", \"target\": \"Yifan Wu 0005\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziyang Guo\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Thomas Butkiewicz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Wenwen Dou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zachary Wartell\", \"target\": \"Remco Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Adam M. Smith 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chenglong Wang\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Alvitta Ottley\", \"value\": 4, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Wenwen Dou\", \"value\": 5, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"R. Jordan Crouser\", \"value\": 5, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Thomas Butkiewicz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Remco Chang\", \"target\": \"Subhajit Das 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Tera Marie Green\", \"value\": 1, \"filtered\": false}, {\"source\": \"Remco Chang\", \"target\": \"Jeremy G. Freeman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Yuzhe Luo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lvkeshen Shen\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Helen Zhao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alvitta Ottley\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": false}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Will Epperson\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c1ngel Alexander Cabrera\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jock D. Mackinlay\", \"target\": \"Jeffrey Heer\", \"value\": 2, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chen Chen 0080\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paolo Buono\", \"target\": \"Paola Valdivia\", \"value\": 1, \"filtered\": true}, {\"source\": \"Khairi Reda\", \"target\": \"Michael E. Papka\", \"value\": 3, \"filtered\": true}, {\"source\": \"Khairi Reda\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Khairi Reda\", \"target\": \"Ratanond Koonchanok\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helen Zhao 0001\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helen Zhao 0001\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Bongshin Lee\", \"value\": 2, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Zhicheng Liu 0001\", \"value\": 9, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Arjun Srinivasan\", \"value\": 3, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Steven Mark Drucker\", \"value\": 1, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Alex Endert\", \"value\": 2, \"filtered\": true}, {\"source\": \"John T. Stasko\", \"target\": \"Arpit Narechania\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": false}, {\"source\": \"John T. Stasko\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yuge Zhang\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tamara Munzner\", \"target\": \"Hanspeter Pfister\", \"value\": 2, \"filtered\": false}, {\"source\": \"Tamara Munzner\", \"target\": \"Robert Kincaid\", \"value\": 3, \"filtered\": true}, {\"source\": \"Tamara Munzner\", \"target\": \"Michael Oppermann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel H. Payne\", \"target\": \"Robert A. Lafrance\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leslie M. Blaha\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leslie M. Blaha\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liang Gou\", \"target\": \"Hao Yang 0007\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liang Gou\", \"target\": \"Jian Zhao 0010\", \"value\": 1, \"filtered\": false}, {\"source\": \"Liang Gou\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": false}, {\"source\": \"Liang Gou\", \"target\": \"Kwan-Liu Ma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Moqi He\", \"target\": \"Yihong Wu 0003\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Wenshuo Zhao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Moqi He\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Fuling Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Xinyue Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zui Chen\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Adam M. Smith 0001\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Junran Yang\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Hanghang Tong\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhe Xu 0007\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Mat\\u00fas Straka\", \"value\": 2, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Alexandra La Cruz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 3, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Dominik Fleischmann\", \"value\": 3, \"filtered\": true}, {\"source\": \"Milos Sr\\u00e1mek\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": false}, {\"source\": \"Mengyu Zhou\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mengyu Zhou\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mengyu Zhou\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Zhutian Chen\", \"value\": 2, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yong Wang 0021\", \"value\": 3, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Nils Gehlenborg\", \"value\": 5, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Sehi L'Yi\", \"value\": 2, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Oliver Deussen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Qianwen Wang\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Youfu Yan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qianwen Wang\", \"target\": \"Rui Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert Kincaid\", \"target\": \"Michael Oppermann\", \"value\": 1, \"filtered\": true}, {\"source\": \"Olav Lenz\", \"target\": \"Frank Keul\", \"value\": 1, \"filtered\": true}, {\"source\": \"Olav Lenz\", \"target\": \"Kay Hamacher\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Songheng Zhang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Yanna Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haotian Li 0001\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Carlos Eduardo Scheidegger\", \"target\": \"Simon Urbanek\", \"value\": 1, \"filtered\": true}, {\"source\": \"Carlos Eduardo Scheidegger\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Jian Zhao 0010\", \"value\": 2, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Xiaoyu Zhang 0014\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Michael E. Papka\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Aryaman Bahukhandi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kwan-Liu Ma\", \"target\": \"Dongyu Liu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"He Huang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaoyu Zhang 0014\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenwen Dou\", \"target\": \"Ryan Wesslen\", \"value\": 4, \"filtered\": false}, {\"source\": \"Wenwen Dou\", \"target\": \"Alireza Karduni\", \"value\": 3, \"filtered\": false}, {\"source\": \"Wenwen Dou\", \"target\": \"Thomas Butkiewicz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ignacio P\\u00e9rez-Messina\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ignacio P\\u00e9rez-Messina\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Christopher Collins 0001\", \"value\": 3, \"filtered\": false}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Fanny Chevalier\", \"value\": 5, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Michael Glueck\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Azam Khan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Simon Breslav\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jian Zhao 0010\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haijun Xia\", \"target\": \"Zhutian Chen\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haijun Xia\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haijun Xia\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haijun Xia\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Oskar Elek\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oskar Elek\", \"target\": \"J. Xavier Prochaska\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oskar Elek\", \"target\": \"Angus G. Forbes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Yunhai Wang\", \"value\": 15, \"filtered\": false}, {\"source\": \"Oliver Deussen\", \"target\": \"Mennatallah El-Assady\", \"value\": 2, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Fabian Sperrle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Christopher Collins 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Rebecca Kehlbeck\", \"value\": 3, \"filtered\": true}, {\"source\": \"Oliver Deussen\", \"target\": \"Bongshin Lee\", \"value\": 2, \"filtered\": false}, {\"source\": \"Oliver Deussen\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Hyeok Kim\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Abhraneel Sarma\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Gromit Yeuk-Yin Chan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ryan A. Rossi\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steve DiPaola\", \"target\": \"Tera Marie Green\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steve DiPaola\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yuzhe Luo\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Shahid Latif\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Yoon Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Beck 0001\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": false}, {\"source\": \"Stefan Lindholm\", \"target\": \"Anders Persson\", \"value\": 2, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Gunnar L\\u00e4th\\u00e9n\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Lindholm\", \"target\": \"Magnus Borga\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Fred Hohman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Will Epperson\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Samuel Reinders\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Matthew Butler 0002\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yanna Lin\", \"target\": \"Dongyu Liu\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yanna Lin\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yanna Lin\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Wolfgang Freiler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Reyk Hillert\", \"target\": \"Helmut Doleisch\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongyu Liu\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dongyu Liu\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongyu Liu\", \"target\": \"Aryaman Bahukhandi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael S. Bernstein\", \"target\": \"\\u00c7agatay Demiralp\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael S. Bernstein\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yngve Sekse Kristiansen\", \"target\": \"Laura A. Garrison\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yngve Sekse Kristiansen\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael E. Papka\", \"target\": \"Ratanond Koonchanok\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Haekyu Park\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Nilaksh Das\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Minsuk Kahng\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 3, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Duen Horng Chau\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fred Hohman\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Austin P. Wright\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fred Hohman\", \"target\": \"Dominik Moritz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Carla Floricel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Mikayla Biggs\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lisanne van Dijk\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gene Golovchinsky\", \"target\": \"Klaus Reichenberger\", \"value\": 2, \"filtered\": true}, {\"source\": \"Gene Golovchinsky\", \"target\": \"Thomas Kamps\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Jeffrey Heer\", \"value\": 5, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Bill Howe\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dominik Moritz\", \"target\": \"Hyeok Kim\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dominik Moritz\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Nils Gehlenborg\", \"value\": 5, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sehi L'Yi\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Achim Ebert\", \"target\": \"Heidrun Steinmetz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Frank Keul\", \"target\": \"Kay Hamacher\", \"value\": 1, \"filtered\": true}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Stefan Bruckner\", \"value\": 1, \"filtered\": false}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Helwig Hauser\", \"value\": 2, \"filtered\": true}, {\"source\": \"Juraj P\\u00e1lenik\", \"target\": \"Thomas Spengler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Phoebe Moh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sana Malik\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": false}, {\"source\": \"Sana Malik\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Michael Wolverton\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Michael Wolverton\", \"target\": \"Alex Endert\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Yihong Wu 0003\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenshuo Zhao\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shichao Jia\", \"target\": \"Zeyu Li 0003\", \"value\": 3, \"filtered\": true}, {\"source\": \"Shichao Jia\", \"target\": \"Jiawan Zhang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Thomas Nocke\", \"value\": 2, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Magnus Heitzler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Heidrun Schumann\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Philipp Muigg\", \"target\": \"Helmut Doleisch\", \"value\": 5, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"Helwig Hauser\", \"value\": 3, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Philipp Muigg\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Aryaman Bahukhandi\", \"target\": \"Sam Yu-Te Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gunnar L\\u00e4th\\u00e9n\", \"target\": \"Magnus Borga\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Xinyue Xu\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fuling Sun\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hannah Kim 0001\", \"target\": \"Alex Endert\", \"value\": 4, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yihong Wu 0003\", \"target\": \"Liqi Cheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Nilaksh Das\", \"value\": 2, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Haekyu Park\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 3, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Haekyu Park\", \"target\": \"Austin P. Wright\", \"value\": 1, \"filtered\": true}, {\"source\": \"Laura A. Garrison\", \"target\": \"Stefan Bruckner\", \"value\": 2, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Phoebe Moh\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Arjun Srinivasan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Steven Mark Drucker\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Quan Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Lyndsey Franklin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Subhajit Das 0002\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Duen Horng Chau\", \"value\": 1, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"David J. Israel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"Arpit Narechania\", \"value\": 4, \"filtered\": false}, {\"source\": \"Alex Endert\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alex Endert\", \"target\": \"Mennatallah El-Assady\", \"value\": 1, \"filtered\": false}, {\"source\": \"Rui Zhang\", \"target\": \"Youfu Yan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rui Zhang\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rui Zhang\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Ziao Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Liqi Cheng\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gromit Yeuk-Yin Chan\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Gromit Yeuk-Yin Chan\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Ross Maciejewski\", \"value\": 2, \"filtered\": false}, {\"source\": \"Hanghang Tong\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanghang Tong\", \"target\": \"Nan Cao 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeremy G. Freeman\", \"target\": \"R. Jordan Crouser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Bongshin Lee\", \"value\": 3, \"filtered\": false}, {\"source\": \"Steven Mark Drucker\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Mennatallah El-Assady\", \"value\": 2, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Christopher Collins 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Fabian Sperrle\", \"value\": 1, \"filtered\": false}, {\"source\": \"Rebecca Kehlbeck\", \"target\": \"Yunhai Wang\", \"value\": 2, \"filtered\": false}, {\"source\": \"Mitchell Gordon\", \"target\": \"Huichen Will Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mitchell Gordon\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mitchell Gordon\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Nan Cao 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinyue Xu\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Renzhong Li\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Renzhong Li\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Christopher Collins 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Shunan Guo\", \"value\": 4, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Ke Xu\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Zhuochen Jin\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Siming Chen 0001\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Qing Chen 0001\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Jiazhe Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Nan Chen\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Cao 0001\", \"target\": \"Xiaohan Jiao\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Tian Gao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Wei Shuai\", \"value\": 1, \"filtered\": true}, {\"source\": \"Nan Cao 0001\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Carla Floricel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Guadalupe Canahuate\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mikayla Biggs\", \"target\": \"Clifton David Fuller\", \"value\": 1, \"filtered\": true}, {\"source\": \"Stefan Bruckner\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 13, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Johanna Schmidt\", \"value\": 2, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": false}, {\"source\": \"Stefan Bruckner\", \"target\": \"Helwig Hauser\", \"value\": 2, \"filtered\": false}, {\"source\": \"Yifan Wu 0005\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yifan Wu 0005\", \"target\": \"Jason D. Hartline\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Kamps\", \"target\": \"Klaus Reichenberger\", \"value\": 2, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Fan Du\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhuochen Jin\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Xiaohan Jiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhuochen Jin\", \"target\": \"Tian Gao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Fanny Chevalier\", \"value\": 2, \"filtered\": false}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Mennatallah El-Assady\", \"value\": 4, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Fabian Sperrle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Christopher Collins 0001\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hyeok Kim\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hyeok Kim\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Shunan Guo\", \"value\": 2, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Tak Yeon Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fan Du\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Fan Du\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": false}, {\"source\": \"Carla Floricel\", \"target\": \"Guadalupe Canahuate\", \"value\": 2, \"filtered\": true}, {\"source\": \"Carla Floricel\", \"target\": \"Clifton David Fuller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": false}, {\"source\": \"Arpit Narechania\", \"target\": \"Alireza Karduni\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Ryan Wesslen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arpit Narechania\", \"target\": \"Mennatallah El-Assady\", \"value\": 1, \"filtered\": false}, {\"source\": \"Huichen Will Wang\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Huichen Will Wang\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert A. Lafrance\", \"target\": \"Aritra Dasgupta\", \"value\": 1, \"filtered\": true}, {\"source\": \"Robert A. Lafrance\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"David J. Israel\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": true}, {\"source\": \"David J. Israel\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Johanna Schmidt\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": false}, {\"source\": \"Johanna Schmidt\", \"target\": \"Silvia Miksch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Urbanek\", \"target\": \"Gordon Woodhull\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wilmot Li\", \"target\": \"Hanspeter Pfister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wei Shuai\", \"target\": \"Guande Wu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tak Yeon Lee\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tak Yeon Lee\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Miguel Nunes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Benjamin Rowland\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Anne Laprie\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Youfu Yan\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"Youfu Yan\", \"target\": \"Yongkang Xiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"\\u00c7agatay Demiralp\", \"target\": \"Jeffrey Heer\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Bill Howe\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Zhicheng Liu 0001\", \"value\": 2, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Halden Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jeffrey Heer\", \"target\": \"Zehua Zeng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jeffrey Heer\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Nils Gehlenborg\", \"value\": 7, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Michelle A. Borkin\", \"value\": 2, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Eston Schweickart\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Zhutian Chen\", \"value\": 4, \"filtered\": false}, {\"source\": \"Hanspeter Pfister\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Lingyun Yu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xinhuan Shu\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Xinhuan Shu\", \"target\": \"Fanny Chevalier\", \"value\": 1, \"filtered\": false}, {\"source\": \"Austin P. Wright\", \"target\": \"Nilaksh Das\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Omar Shaikh\", \"value\": 1, \"filtered\": true}, {\"source\": \"Austin P. Wright\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziao Liu\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ziao Liu\", \"target\": \"Hui Zhang 0051\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aritra Dasgupta\", \"target\": \"Ryan Wilson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Alexandra La Cruz\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Mat\\u00fas Straka\", \"target\": \"Dominik Fleischmann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Zeyu Li 0003\", \"target\": \"Jiawan Zhang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Kresimir Matkovic\", \"value\": 2, \"filtered\": false}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": false}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Wolfgang Freiler\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tera Marie Green\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yoon Kim\", \"target\": \"Shahid Latif\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Samuel Reinders\", \"target\": \"Kim Marriott\", \"value\": 1, \"filtered\": true}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Fabian Sperrle\", \"value\": 4, \"filtered\": true}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Davide Ceneda\", \"value\": 2, \"filtered\": false}, {\"source\": \"Mennatallah El-Assady\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jason D. Hartline\", \"target\": \"Jessica Hullman\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jason D. Hartline\", \"target\": \"Paula Kayongo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jason D. Hartline\", \"target\": \"Glenn Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fabian Sperrle\", \"target\": \"Davide Ceneda\", \"value\": 1, \"filtered\": false}, {\"source\": \"Vidya Setlur\", \"target\": \"Arjun Srinivasan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Vidya Setlur\", \"target\": \"Aditeya Pandey\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Miguel Nunes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Benjamin Rowland\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Glenn Sun\", \"target\": \"Paula Kayongo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Glenn Sun\", \"target\": \"Jessica Hullman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eston Schweickart\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Eston Schweickart\", \"target\": \"Jovan Popovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Xiaohan Jiao\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tian Gao\", \"target\": \"Ke Xu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guadalupe Canahuate\", \"target\": \"Clifton David Fuller\", \"value\": 3, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Yun Wang 0012\", \"value\": 4, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Dongmei Zhang 0001\", \"value\": 4, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"He Huang\", \"target\": \"Chin-Yew Lin\", \"value\": 1, \"filtered\": true}, {\"source\": \"J. Xavier Prochaska\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"J. Xavier Prochaska\", \"target\": \"Angus G. Forbes\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jiazhe Wang\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Spengler\", \"target\": \"Helwig Hauser\", \"value\": 1, \"filtered\": true}, {\"source\": \"Davide Ceneda\", \"target\": \"Silvia Miksch\", \"value\": 3, \"filtered\": true}, {\"source\": \"Davide Ceneda\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": false}, {\"source\": \"Halden Lin\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Halden Lin\", \"target\": \"Bill Howe\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dennis Chau\", \"target\": \"Yiwen Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dennis Chau\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Helmut Doleisch\", \"value\": 5, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Silvia Miksch\", \"value\": 3, \"filtered\": false}, {\"source\": \"Helwig Hauser\", \"target\": \"Steffen Oeltze\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helwig Hauser\", \"target\": \"Kresimir Matkovic\", \"value\": 10, \"filtered\": false}, {\"source\": \"Helwig Hauser\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Yun Wang 0012\", \"value\": 3, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Leni Yang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ke Xu\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ke Xu\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ke Xu\", \"target\": \"Xiaohan Jiao\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Tan Tang\", \"value\": 6, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Steffen Koch 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Thomas Ertl\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Lu Ying\", \"value\": 2, \"filtered\": true}, {\"source\": \"Lingyun Yu 0001\", \"target\": \"Xiao Xie\", \"value\": 1, \"filtered\": true}, {\"source\": \"Subhajit Das 0002\", \"target\": \"Bharath Kalidindi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Jiawan Zhang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Siming Chen 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Siming Chen 0001\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhutian Chen\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Yong Wang 0021\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Xiao Xie\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhutian Chen\", \"target\": \"Hui Zhang 0051\", \"value\": 2, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Michelle A. Borkin\", \"value\": 2, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Arjun Srinivasan\", \"value\": 1, \"filtered\": true}, {\"source\": \"Aditeya Pandey\", \"target\": \"Nils Gehlenborg\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jessica Hullman\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": false}, {\"source\": \"Jessica Hullman\", \"target\": \"Paula Kayongo\", \"value\": 2, \"filtered\": true}, {\"source\": \"Jessica Hullman\", \"target\": \"Abhraneel Sarma\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Borga\", \"target\": \"Reiner Lenz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Borga\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Jovan Popovic\", \"target\": \"Zhicheng Liu 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Duen Horng Chau\", \"target\": \"Minsuk Kahng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Nils Gehlenborg\", \"target\": \"Michelle A. Borkin\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Thomas Ertl\", \"value\": 13, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Ross Maciejewski\", \"value\": 1, \"filtered\": false}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Koch 0001\", \"target\": \"Johannes Knittel\", \"value\": 3, \"filtered\": true}, {\"source\": \"Paola Valdivia\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Paola Valdivia\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Hui Zhang 0051\", \"value\": 9, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Lu Ying\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Tan Tang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiao Xie\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Songheng Zhang\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": true}, {\"source\": \"Songheng Zhang\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Bongshin Lee\", \"value\": 3, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"John Thompson 0002\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": false}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Zhicheng Liu 0001\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Leni Yang\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Leni Yang\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": false}, {\"source\": \"Kim Marriott\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kim Marriott\", \"target\": \"Bongshin Lee\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kim Marriott\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 2, \"filtered\": true}, {\"source\": \"Alexandra La Cruz\", \"target\": \"Dominik Fleischmann\", \"value\": 2, \"filtered\": true}, {\"source\": \"Yiwen Sun\", \"target\": \"Andrew E. Johnson 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Ross Maciejewski\", \"value\": 2, \"filtered\": false}, {\"source\": \"Thomas Ertl\", \"target\": \"Tan Tang\", \"value\": 2, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Ertl\", \"target\": \"Johannes Knittel\", \"value\": 3, \"filtered\": true}, {\"source\": \"Yongkang Xiao\", \"target\": \"Yu Hou\", \"value\": 1, \"filtered\": true}, {\"source\": \"R. Jordan Crouser\", \"target\": \"Lyndsey Franklin\", \"value\": 1, \"filtered\": true}, {\"source\": \"R. Jordan Crouser\", \"target\": \"Leilani Battle\", \"value\": 1, \"filtered\": false}, {\"source\": \"Angus G. Forbes\", \"target\": \"Joseph N. Burchett\", \"value\": 1, \"filtered\": true}, {\"source\": \"Lu Ying\", \"target\": \"Tan Tang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Zehua Zeng\", \"target\": \"Leilani Battle\", \"value\": 2, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Qing Chen 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Guande Wu\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Abhraneel Sarma\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nilaksh Das\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nilaksh Das\", \"target\": \"Minsuk Kahng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nilaksh Das\", \"target\": \"Duen Horng (Polo) Chau\", \"value\": 2, \"filtered\": true}, {\"source\": \"Nilaksh Das\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Kresimir Matkovic\", \"value\": 1, \"filtered\": true}, {\"source\": \"Miguel Nunes\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chunyao Qian\", \"target\": \"Shizhao Sun\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chunyao Qian\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Yusheng Qi\", \"value\": 1, \"filtered\": true}, {\"source\": \"Qing Chen 0001\", \"target\": \"Nan Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shizhao Sun\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Minsuk Kahng\", \"value\": 3, \"filtered\": false}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Omar Shaikh\", \"value\": 2, \"filtered\": true}, {\"source\": \"Duen Horng (Polo) Chau\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Yun Wang 0012\", \"value\": 1, \"filtered\": true}, {\"source\": \"Chin-Yew Lin\", \"target\": \"Dongmei Zhang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Michael Glueck\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Fanny Chevalier\", \"value\": 2, \"filtered\": true}, {\"source\": \"Simon Breslav\", \"target\": \"Azam Khan\", \"value\": 2, \"filtered\": true}, {\"source\": \"Tan Tang\", \"target\": \"Shuhan Liu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Tan Tang\", \"target\": \"Johannes Knittel\", \"value\": 2, \"filtered\": true}, {\"source\": \"Dominik Fleischmann\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 5, \"filtered\": true}, {\"source\": \"Dominik Fleischmann\", \"target\": \"Arnold K\\u00f6chl\", \"value\": 2, \"filtered\": true}, {\"source\": \"Lyndsey Franklin\", \"target\": \"Joe Bruce\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yun Wang 0012\", \"target\": \"Dongmei Zhang 0001\", \"value\": 6, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Yong Wang 0021\", \"value\": 1, \"filtered\": false}, {\"source\": \"Yun Wang 0012\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yun Wang 0012\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"John Thompson 0002\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Catherine Plaisant\", \"value\": 1, \"filtered\": false}, {\"source\": \"Bongshin Lee\", \"target\": \"Yunhai Wang\", \"value\": 3, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Yunjeong Chang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Ingrid Zukerman\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bongshin Lee\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Wenchao Wu\", \"target\": \"Yixian Zheng\", \"value\": 3, \"filtered\": true}, {\"source\": \"Wenchao Wu\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Wenchao Wu\", \"target\": \"Po-Ming Law\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yusheng Qi\", \"target\": \"Yixuan Li\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": false}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Kresimir Matkovic\", \"target\": \"Katja B\\u00fchler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yunjeong Chang\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Nocke\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Thomas Nocke\", \"target\": \"Magnus Heitzler\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yong Wang 0021\", \"target\": \"Yangqiu Song\", \"value\": 1, \"filtered\": true}, {\"source\": \"Yong Wang 0021\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Minsuk Kahng\", \"target\": \"Omar Shaikh\", \"value\": 1, \"filtered\": false}, {\"source\": \"Reiner Lenz\", \"target\": \"Anders Persson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}, {\"source\": \"Katja B\\u00fchler\", \"target\": \"Sol\\u00e9akh\\u00e9na Ken\", \"value\": 1, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Michael Glueck\", \"value\": 5, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Azam Khan\", \"value\": 5, \"filtered\": true}, {\"source\": \"Fanny Chevalier\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": false}, {\"source\": \"Nan Chen\", \"target\": \"Shunan Guo\", \"value\": 1, \"filtered\": false}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Bei Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Xinyi He\", \"value\": 1, \"filtered\": true}, {\"source\": \"Dongmei Zhang 0001\", \"target\": \"Jinpeng Wang 0001\", \"value\": 1, \"filtered\": true}, {\"source\": \"Arnold K\\u00f6chl\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 4, \"filtered\": true}, {\"source\": \"Xiaohan Jiao\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Xiaohan Jiao\", \"target\": \"Ying Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Steffen Oeltze\", \"target\": \"Helmut Doleisch\", \"value\": 2, \"filtered\": true}, {\"source\": \"Bill Howe\", \"target\": \"Greg L. Nelson\", \"value\": 1, \"filtered\": true}, {\"source\": \"Joe Bruce\", \"target\": \"Russ Burtner\", \"value\": 1, \"filtered\": true}, {\"source\": \"Omar Shaikh\", \"target\": \"Rahul Duggal\", \"value\": 1, \"filtered\": true}, {\"source\": \"Helmut Doleisch\", \"target\": \"M. Eduard Gr\\u00f6ller\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hui Zhang 0051\", \"target\": \"Yunhai Wang\", \"value\": 1, \"filtered\": false}, {\"source\": \"Ying Chen\", \"target\": \"Bingchang Chen\", \"value\": 1, \"filtered\": true}, {\"source\": \"Magnus Heitzler\", \"target\": \"Hans-J\\u00f6rg Schulz\", \"value\": 1, \"filtered\": true}, {\"source\": \"Azam Khan\", \"target\": \"Michael Glueck\", \"value\": 5, \"filtered\": true}, {\"source\": \"Ryan Wesslen\", \"target\": \"Alireza Karduni\", \"value\": 4, \"filtered\": true}, {\"source\": \"Po-Ming Law\", \"target\": \"Yixian Zheng\", \"value\": 1, \"filtered\": true}, {\"source\": \"Ingrid Zukerman\", \"target\": \"Lizhen Qu\", \"value\": 1, \"filtered\": true}, {\"source\": \"Bei Chen\", \"target\": \"Lei Fang 0004\", \"value\": 1, \"filtered\": true}, {\"source\": \"Shuhan Liu\", \"target\": \"Johannes Knittel\", \"value\": 1, \"filtered\": true}, {\"source\": \"M. Eduard Gr\\u00f6ller\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"M. Eduard Gr\\u00f6ller\", \"target\": \"Yixian Zheng\", \"value\": 1, \"filtered\": false}, {\"source\": \"Hans-J\\u00f6rg Schulz\", \"target\": \"Silvia Miksch\", \"value\": 1, \"filtered\": false}, {\"source\": \"Catherine Plaisant\", \"target\": \"Alexis Pister\", \"value\": 1, \"filtered\": true}, {\"source\": \"Sol\\u00e9akh\\u00e9na Ken\", \"target\": \"Matthias Schlachter\", \"value\": 1, \"filtered\": true}]}\nconst { Graph } = G6\nconst graph_13d6b349 = new Graph(\n{\ncontainer: 'network_a4d136b6-c3e1-40cb-9898-8d49a89297a5',\nautoFit: 'view',\ndata:data_13d6b349,\nlayout: {\ntype: 'force-atlas2',\npreventOverlap: true,\nkr: 20,\ncenter: [250, 250],\n},\nbehaviors: ['drag-canvas', 'zoom-canvas', 'drag-element'],\nedge: {\nstyle: {\nlineWidth: 2,\nopacity: d => d.filtered ? 1 : 0.4},\n},\nnode: {\nstyle: {\nlabelText: d => d.id,\n},\n},\n});\ngraph_13d6b349.render();\n</script>\n    \n        </body>\n        </html>\n        "
                    },
                    "facts": "### Begin of facts\nDataset path used: outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv\nTotal papers processed: 83\nTotal unique authors (deduped): 311\n### End of facts\n### Begin of facts\nCleaned author list sample (first 30):\n- Abdallah Sherif Radwan Mohamed\n- Abhraneel Sarma\n- Achim Ebert\n- Adam M. Smith 0001\n- Aditeya Pandey\n- Alan Lundgard\n- Alex Endert\n- Alexandra La Cruz\n- Alexis Pister\n- Alireza Karduni\n- Alvitta Ottley\n- Anders Persson\n- Andrew E. Johnson 0001\n- Andrew Wentzel\n- Angus G. Forbes\n- Anne Laprie\n- Anushka Anand\n- Aoyu Wu\n- Aritra Dasgupta\n- Arjun Srinivasan\n- Arnold K\u00f6chl\n- Arpit Narechania\n- Arvind Satyanarayan\n- Aryaman Bahukhandi\n- Austin P. Wright\n- Azam Khan\n- Bahador Saket\n- Bei Chen\n- Benjamin Rowland\n- Bernhard Pointner\n### End of facts\n### Begin of facts\nAmbiguous author name examples (heuristic flags \u2014 needs manual review):\n- Eli T. Brown\n- Kristin A. Cook\n- Ryan A. Rossi\n- Daniel A. Keim\n- G. Elisabeta Marai\n- Jason D. Hartline\n- R. Jordan Crouser\n- Adam M. Smith 0001\n- Andrew E. Johnson 0001\n- Angus G. Forbes\n### End of facts\n### Begin of facts\nCo-authorship edge counts: total edges = 895\nEdge weight distribution (counts):\n weight=1: 822 edges\n weight=2: 60 edges\n weight=3: 12 edges\n weight=4: 1 edges\nRecommended minimum coauthorship weight threshold: 2\nEdges remaining at thresholds:\n >= 1: 895 edges\n >= 2: 73 edges\n >= 3: 13 edges\n### End of facts\n### Begin of facts\nTop authors by publication count (author \u2014 pubs \u2014 weighted_degree \u2014 betweenness):\n- Alex Endert \u2014 pubs:7 \u2014 wdeg:29.0 \u2014 betw:0.0166\n- Yingcai Wu \u2014 pubs:6 \u2014 wdeg:37.0 \u2014 betw:0.0049\n- Huamin Qu \u2014 pubs:6 \u2014 wdeg:26.0 \u2014 betw:0.0082\n- Nan Cao 0001 \u2014 pubs:5 \u2014 wdeg:27.0 \u2014 betw:0.0058\n- Jeffrey Heer \u2014 pubs:5 \u2014 wdeg:20.0 \u2014 betw:0.0093\n- Dongmei Zhang 0001 \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Haidong Zhang \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Dominik Moritz \u2014 pubs:4 \u2014 wdeg:19.0 \u2014 betw:0.0121\n- Jessica Hullman \u2014 pubs:4 \u2014 wdeg:14.0 \u2014 betw:0.0032\n- Lingyun Yu 0001 \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n### End of facts\n### Begin of facts\nTop bridging/central authors by betweenness (author \u2014 betweenness \u2014 degree \u2014 pubs):\n- Alex Endert \u2014 betw:0.0166 \u2014 deg:24 \u2014 pubs:7\n- Guande Wu \u2014 betw:0.0131 \u2014 deg:11 \u2014 pubs:2\n- Dominik Moritz \u2014 betw:0.0121 \u2014 deg:16 \u2014 pubs:4\n- Chenglong Wang \u2014 betw:0.0110 \u2014 deg:8 \u2014 pubs:2\n- Leilani Battle \u2014 betw:0.0109 \u2014 deg:12 \u2014 pubs:3\n- Bongshin Lee \u2014 betw:0.0099 \u2014 deg:11 \u2014 pubs:3\n- Jeffrey Heer \u2014 betw:0.0093 \u2014 deg:16 \u2014 pubs:5\n- Huamin Qu \u2014 betw:0.0082 \u2014 deg:22 \u2014 pubs:6\n- Ryan A. Rossi \u2014 betw:0.0065 \u2014 deg:9 \u2014 pubs:3\n- Emily Wall \u2014 betw:0.0065 \u2014 deg:10 \u2014 pubs:3\n### End of facts\n### Begin of facts\nCommunity detection summary (using greedy modularity on thresholded graph):\nNumber of communities: 267\nCommunity sizes (top 10): [10, 6, 6, 6, 6, 5, 5, 4, 3, 3]\n Community 0 (size 10): Xiao Xie, Yingcai Wu, Lingyun Yu 0001, Huamin Qu, Hui Zhang 0051, Lu Ying, Haotian Li 0001, Aoyu Wu, ...\n Community 1 (size 6): Emily Wall, Kristin A. Cook, Lyndsey Franklin, Alex Endert, Eli T. Brown, Nick Cramer\n Community 2 (size 6): Weiwei Cui, Jian-Guang Lou, He Huang, Yun Wang 0012, Dongmei Zhang 0001, Haidong Zhang\n Community 3 (size 6): G. Elisabeta Marai, Andrew Wentzel, Abdallah Sherif Radwan Mohamed, Carla Floricel, Clifton David Fuller, Guadalupe Canahuate\n Community 4 (size 6): Jessica Hullman, Eunyee Koh, Ryan A. Rossi, Jane Hoffswell, Jason D. Hartline, Hyeok Kim\n### End of facts\n### Begin of facts\nSuggested visualization parameters:\n- Minimum publications per author to display: 2 (recommended; increase to 3-5 to focus on cores)\n- Minimum coauthorship weight to display edges: 2 (recommended based on data; 822 edges are weight=1) \n- Layout: force-directed (e.g., spring) with node size ~ pub_count and edge width ~ weight\nShortlist of top candidate authors to highlight as bridges/cores:\n- Alex Endert \u2014 pubs:7 \u2014 wdeg:29.0 \u2014 betw:0.0166\n- Guande Wu \u2014 pubs:2 \u2014 wdeg:11.0 \u2014 betw:0.0131\n- Dominik Moritz \u2014 pubs:4 \u2014 wdeg:19.0 \u2014 betw:0.0121\n- Chenglong Wang \u2014 pubs:2 \u2014 wdeg:8.0 \u2014 betw:0.0110\n- Leilani Battle \u2014 pubs:3 \u2014 wdeg:14.0 \u2014 betw:0.0109\n- Bongshin Lee \u2014 pubs:3 \u2014 wdeg:11.0 \u2014 betw:0.0099\n- Jeffrey Heer \u2014 pubs:5 \u2014 wdeg:20.0 \u2014 betw:0.0093\n- Huamin Qu \u2014 pubs:6 \u2014 wdeg:26.0 \u2014 betw:0.0082\n- Ryan A. Rossi \u2014 pubs:3 \u2014 wdeg:12.0 \u2014 betw:0.0065\n- Emily Wall \u2014 pubs:3 \u2014 wdeg:11.0 \u2014 betw:0.0065\n- Yingcai Wu \u2014 pubs:6 \u2014 wdeg:37.0 \u2014 betw:0.0049\n- Nan Cao 0001 \u2014 pubs:5 \u2014 wdeg:27.0 \u2014 betw:0.0058\n- Dongmei Zhang 0001 \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Haidong Zhang \u2014 pubs:4 \u2014 wdeg:25.0 \u2014 betw:0.0019\n- Tan Tang \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n- Lingyun Yu 0001 \u2014 pubs:3 \u2014 wdeg:21.0 \u2014 betw:0.0000\n- Yun Wang 0012 \u2014 pubs:3 \u2014 wdeg:20.0 \u2014 betw:0.0017\n### End of facts\n\nSaved node metrics -> autovis_nodes_metrics.csv and edge list -> autovis_edge_list.csv\n",
                    "text": "The co-authorship network highlights core clusters and bridges: there are dense local communities (e.g., a cluster around Yingcai Wu / Huamin Qu / Zhejiang University collaborators focused on automatic glyphs, dashboard recommendation and RL approaches) and bridging authors (Alex Endert, Dominik Moritz, Jeff Heer) who connect mixed-initiative, recommendation, and perception-focused strands. Metrics show Alex Endert leading in publication count within this AutoVis subset and high weighted degree; Yingcai Wu and Huamin Qu combine high publication counts with strong local connectivity; Dominik Moritz and Jeffrey Heer appear among the most-cited authors and help anchor the Draco/Voyager lineage. Community detection reveals many small, topic-coherent groups and a handful of mid\u2011sized communities that align with methodological families (mixed\u2011initiative visual analytics, visualization recommendation/Draco, NLG/infographic automation, ML-driven interpretability)."
                },
                {
                    "id": 2,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n                "
                    },
                    "facts": "### Begin of facts\nTop 5 authors by CrossRef citations:\nJeffrey Heer: citations=551, aminer_citations=841, papers=5, years=2014-2024\nDominik Moritz: citations=480, aminer_citations=718, papers=4, years=2015-2023\nBill Howe: citations=469, aminer_citations=712, papers=2, years=2015-2018\nAlex Endert: citations=436, aminer_citations=519, papers=7, years=2014-2018\nKanit Wongsuphasawat: citations=292, aminer_citations=487, papers=1, years=2015-2015\n### End of facts\n### Begin of facts\nTop 5 papers by CrossRef citations:\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 citations=292, downloads=4307, DOI=10.1109/tvcg.2015.2467191\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 citations=177, downloads=3238, DOI=10.1109/tvcg.2018.2865240\nA Design Space of Visualization Tasks (2013) \u2014 citations=144, downloads=4884, DOI=10.1109/tvcg.2013.120\nAugmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication (2018) \u2014 citations=121, downloads=2942, DOI=10.1109/tvcg.2018.2865145\nFAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning (2019) \u2014 citations=106, downloads=2108, DOI=10.1109/vast47406.2019.8986948\n### End of facts\n### Begin of facts\nTop 5 papers by Downloads_Xplore:\nA Design Space of Visualization Tasks (2013) \u2014 downloads=4884, citations=144, DOI=10.1109/tvcg.2013.120\nVoyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations (2015) \u2014 downloads=4307, citations=292, DOI=10.1109/tvcg.2015.2467191\nCalliope: Automatic Visual Data Story Generation from a Spreadsheet (2020) \u2014 downloads=3724, citations=80, DOI=10.1109/tvcg.2020.3030403\nKG4Vis: A Knowledge Graph-Based Approach for Visualization Recommendation (2021) \u2014 downloads=3452, citations=69, DOI=10.1109/tvcg.2021.3114863\nFormalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco (2018) \u2014 downloads=3238, citations=177, DOI=10.1109/tvcg.2018.2865240\n### End of facts\n### Begin of facts\nTop 5 institutions by aggregated CrossRef citations:\nUniversity of Washington: citations=2024, papers_counted=17\nGeorgia Institute of Technology: citations=1545, papers_counted=24\nTableau Research: citations=892, papers_counted=5\nMicrosoft Research Asia: citations=832, papers_counted=20\nAdobe Research: citations=539, papers_counted=17\n### End of facts\n### Begin of facts\nAuthor participation trend (newcomer vs long-term) by year:\n1995: newcomers=0, long-term=0, other=3\n2004: newcomers=0, long-term=0, other=11\n2006: newcomers=0, long-term=0, other=1\n2007: newcomers=0, long-term=0, other=5\n2008: newcomers=0, long-term=1, other=4\n2009: newcomers=0, long-term=0, other=4\n2010: newcomers=0, long-term=0, other=3\n2011: newcomers=0, long-term=1, other=8\n2012: newcomers=0, long-term=0, other=5\n2013: newcomers=0, long-term=0, other=4\n2014: newcomers=0, long-term=4, other=18\n2015: newcomers=0, long-term=4, other=15\n2016: newcomers=0, long-term=4, other=25\n2017: newcomers=0, long-term=3, other=5\n2018: newcomers=0, long-term=3, other=17\n2019: newcomers=0, long-term=4, other=16\n2020: newcomers=0, long-term=8, other=28\n2021: newcomers=0, long-term=17, other=66\n2022: newcomers=12, long-term=8, other=2\n2023: newcomers=28, long-term=12, other=13\n2024: newcomers=29, long-term=4, other=2\n### End of facts\n",
                    "text": "Impact concentrations: top papers by CrossRef citations (Voyager, Draco, Task Design Space, interactive data facts, FAIRVIS) are also among the most-downloaded, indicating both scholarly influence and practitioner interest. Institution\u2011level aggregates reinforce the central role of a few hubs (University of Washington and Georgia Tech notably) while industry research (Tableau, Microsoft Research Asia, Adobe) supplies high-download, application-oriented work. Author timelines show many recent newcomers (large counts of first-time contributors after 2020) alongside long-term contributors who repeatedly publish in AutoVis themes; this mixture supports rapid innovation but raises reproducibility and evaluation coordination challenges."
                }
            ]
        },
        {
            "section_number": 6,
            "section_name": "Interpretation, open questions, and community recommendations",
            "section_size": "short",
            "section_description": "Synthesis of findings: interpret why AutoVis interest evolved (e.g., tooling like D3, web platforms, evaluation difficulty), identify gaps (evaluation standards, reproducibility, dataset benchmarks), and propose concrete next steps for research and community actions (benchmarks, shared datasets, mixed-initiative studies, interdisciplinary collaboration). Include suggested visualizations or artifacts the community should maintain.",
            "analyses": [
                {
                    "analysis_schema": {
                        "action": "explore",
                        "information_needed": {
                            "question_text": "To what extent do AutoVis papers report empirical evaluation, provide reproducibility artifacts (code/data), or release benchmarks/datasets?",
                            "key_uncertainty": "Which keywords and dataset attributes reliably indicate evaluation and reproducibility (for example, 'user study', 'evaluation', 'benchmark', 'dataset', 'code', 'open source', 'reproduc'), and whether existing structured fields (GraphicsReplicabilityStamp, Downloads_Xplore) align with textual signals?",
                            "expected_outputs": [
                                "A short list of reliable keyword regexes to detect: evaluation/user study, benchmark/dataset release, and code/open-source artifacts (applied over Title, Abstract, AuthorKeywords).",
                                "Counts and percentages of AutoVis papers matching each category (evaluation, benchmark/dataset, code/open-source, reproducibility stamp) to quantify gaps.",
                                "Recommended thresholds/filters (e.g., Downloads_Xplore cutoff, mapping of GraphicsReplicabilityStamp values) and any sample paper identifiers for manual checks to validate the heuristics."
                            ]
                        }
                    },
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_69bfe2b6\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 13}, \"title\": {\"fontSize\": 14}}, \"vconcat\": [{\"layer\": [{\"data\": {\"values\": [{\"category\": \"Evaluation / user study\", \"count\": 48, \"pct\": 57.8, \"label\": \"48 (57.8%)\"}, {\"category\": \"Benchmark / dataset release\", \"count\": 12, \"pct\": 14.5, \"label\": \"12 (14.5%)\"}, {\"category\": \"Code / open-source artifacts\", \"count\": 8, \"pct\": 9.6, \"label\": \"8 (9.6%)\"}, {\"category\": \"GraphicsReplicabilityStamp present\", \"count\": 0, \"pct\": 0.0, \"label\": \"0 (0.0%)\"}, {\"category\": \"Any of (eval/dataset/code)\", \"count\": 51, \"pct\": 61.4, \"label\": \"51 (61.4%)\"}]}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"category\", \"legend\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"category\", \"sort\": \"-y\", \"title\": \"Category\", \"type\": \"nominal\"}, \"y\": {\"field\": \"count\", \"title\": \"Count of papers\", \"type\": \"quantitative\"}}}, {\"data\": {\"values\": [{\"category\": \"Evaluation / user study\", \"count\": 48, \"pct\": 57.8, \"label\": \"48 (57.8%)\"}, {\"category\": \"Benchmark / dataset release\", \"count\": 12, \"pct\": 14.5, \"label\": \"12 (14.5%)\"}, {\"category\": \"Code / open-source artifacts\", \"count\": 8, \"pct\": 9.6, \"label\": \"8 (9.6%)\"}, {\"category\": \"GraphicsReplicabilityStamp present\", \"count\": 0, \"pct\": 0.0, \"label\": \"0 (0.0%)\"}, {\"category\": \"Any of (eval/dataset/code)\", \"count\": 51, \"pct\": 61.4, \"label\": \"51 (61.4%)\"}]}, \"mark\": {\"type\": \"text\", \"color\": \"black\", \"dy\": -8}, \"encoding\": {\"text\": {\"field\": \"label\", \"type\": \"nominal\"}, \"x\": {\"field\": \"category\", \"sort\": \"-y\", \"type\": \"nominal\"}, \"y\": {\"field\": \"count\", \"type\": \"quantitative\"}}}], \"height\": 250, \"width\": 700}, {\"layer\": [{\"data\": {\"values\": [{\"label\": \"Evaluation only\", \"count\": 34}, {\"label\": \"Dataset only\", \"count\": 1}, {\"label\": \"Code only\", \"count\": 2}, {\"label\": \"Eval & Code\", \"count\": 3}, {\"label\": \"Eval & Dataset\", \"count\": 8}, {\"label\": \"Code & Dataset\", \"count\": 0}, {\"label\": \"All three\", \"count\": 3}, {\"label\": \"None of the three\", \"count\": 32}]}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"label\", \"legend\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"label\", \"sort\": \"-x\", \"title\": \"Overlap category\", \"type\": \"nominal\"}}}, {\"data\": {\"values\": [{\"label\": \"Evaluation only\", \"count\": 34}, {\"label\": \"Dataset only\", \"count\": 1}, {\"label\": \"Code only\", \"count\": 2}, {\"label\": \"Eval & Code\", \"count\": 3}, {\"label\": \"Eval & Dataset\", \"count\": 8}, {\"label\": \"Code & Dataset\", \"count\": 0}, {\"label\": \"All three\", \"count\": 3}, {\"label\": \"None of the three\", \"count\": 32}]}, \"mark\": {\"type\": \"text\", \"color\": \"black\", \"dx\": 5}, \"encoding\": {\"text\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"label\", \"sort\": \"-x\", \"type\": \"nominal\"}}}], \"height\": 300, \"width\": 700}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\"};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_69bfe2b6\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "knowledge": {
                        "facts": "### Begin of facts\nReliable regexes to detect categories (apply over Title+Abstract+AuthorKeywords):\n- Evaluation / user study regex: 'user[ -]?study|user[ -]?evaluation|usability (?:study|evaluation)|experiment|empirical|evaluate(?:d|ion)?|controlled study|pilot study|field study|mturk|amazon mechanical turk|crowd?sourc(?:e|ed)'\n- Benchmark / dataset release regex: 'benchmark|data ?set|dataset|corpus|corpora|data release|data released|we (?:release|released)|public dataset|open dataset|shared dataset|dataset available|data available'\n- Code / open-source artifact regex: 'code|source ?code|open[ -]?source|github\\\\.com|gitlab\\\\.com|bitbucket\\\\.org|repository|implementation available|artifact(?: available)?|software available|demo web|online demo|package|pip|npm|docker'\nNotes: case-insensitive; search for whole phrases like \"user study\", \"dataset\", \"github.com\", \"open source\", etc.\n### End of facts\n### Begin of facts\nAutoVis papers found: 80\nPapers with evaluation/user-study signals: 37 (46.2%)\nPapers with benchmark/dataset signals: 30 (37.5%)\nPapers with code/open-source signals: 8 (10.0%)\nPapers with non-empty GraphicsReplicabilityStamp: 0 (0.0%)\n### End of facts\n### Begin of facts\nDownloads_Xplore distribution (AutoVis subset):\n- median: 937, 75th pct: 1791, 90th pct: 2673, max: 4884\nRecommended Downloads_Xplore cutoff suggestions for manual triage:\n- Consider > median as moderately visible; >75th percentile as high visibility; >90th percentile as top-tier papers to prioritize for manual checks.\n\nGraphicsReplicabilityStamp values observed (value:count):\n- \"\": 80\n\nRecommendation for stamp handling:\n- Treat non-empty stamp as evidence of some reported reproducibility information.\n- Do NOT assume exact meaning of codes (e.g., \"TT\"); map stamp tokens to semantics after manual inspection (common approach: T* => tools/artifacts, R* => raw data, M* => methods).\n### End of facts\n### Begin of facts\nSample papers (Title, DOI, Year, Downloads_Xplore, CitationCount_CrossRef) for manual validation:\n\n-- Evaluation samples --\n- ('Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco', '10.1109/tvcg.2018.2865240', '2018', 3238.0, 177.0)\n- ('Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication', '10.1109/tvcg.2018.2865145', '2018', 2942.0, 121.0)\n- ('Finding Waldo: Learning about Users from their Interactions', '10.1109/tvcg.2014.2346575', '2014', 2226.0, 95.0)\n\n-- Benchmark/dataset samples --\n- ('Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication', '10.1109/tvcg.2018.2865145', '2018', 2942.0, 121.0)\n- ('FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning', '10.1109/vast47406.2019.8986948', '2019', 2108.0, 106.0)\n- ('Calliope: Automatic Visual Data Story Generation from a Spreadsheet', '10.1109/tvcg.2020.3030403', '2020', 3724.0, 80.0)\n\n-- Code/open-source samples --\n- ('Data-Driven Guides: Supporting Expressive Design for Information Graphics', '10.1109/tvcg.2016.2598620', '2016', 2245.0, 92.0)\n- ('Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations', '10.1109/tvcg.2016.2598543', '2016', 2188.0, 33.0)\n- ('NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks', '10.1109/tvcg.2021.3114858', '2021', 830.0, 15.0)\n### End of facts\n"
                    },
                    "global_filter_state": {
                        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
                        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
                        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
                    }
                }
            ],
            "content": [
                {
                    "id": 0,
                    "type": "introduction",
                    "text": "Interpreting the overall picture: AutoVis rose as enabling technologies and cultural practices converged \u2014 web toolchains and public codebases lowered barriers, machine learning and LLMs offered powerful automation, and mixed-initiative paradigms gave researchers a usable human-in-the-loop framing. Interest spiked around high-visibility systems (Voyager, Draco) that demonstrated both utility and extensibility, and later waves reflect new capabilities (NLG, reinforcement learning, knowledge-graph approaches). Gaps persist in standardization: many papers include evaluations but relatively few publish code, datasets, or reproducibility stamps, and evaluation tasks are heterogeneous. To accelerate impact and trust, the community should prioritize shared benchmarks and curated artifacts, reproducible artifact requirements, and cross-lab evaluation protocols. Concretely, maintainable artifacts the community should sustain include a canonical milestone list (award + citation\u2011based), a central AutoVis dataset index linking papers to code and datasets, benchmark tasks for NL2VIS and mixed-initiative evaluation, and a living leaderboard or artifact repository that journals and conferences can point to for replication checks."
                },
                {
                    "id": 1,
                    "type": "visualisation",
                    "visualisation": {
                        "library": "altair",
                        "specification": "\n  <div id=\"vis_69bfe2b6\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"axis\": {\"labelFontSize\": 12, \"titleFontSize\": 13}, \"title\": {\"fontSize\": 14}}, \"vconcat\": [{\"layer\": [{\"data\": {\"values\": [{\"category\": \"Evaluation / user study\", \"count\": 48, \"pct\": 57.8, \"label\": \"48 (57.8%)\"}, {\"category\": \"Benchmark / dataset release\", \"count\": 12, \"pct\": 14.5, \"label\": \"12 (14.5%)\"}, {\"category\": \"Code / open-source artifacts\", \"count\": 8, \"pct\": 9.6, \"label\": \"8 (9.6%)\"}, {\"category\": \"GraphicsReplicabilityStamp present\", \"count\": 0, \"pct\": 0.0, \"label\": \"0 (0.0%)\"}, {\"category\": \"Any of (eval/dataset/code)\", \"count\": 51, \"pct\": 61.4, \"label\": \"51 (61.4%)\"}]}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"category\", \"legend\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"category\", \"sort\": \"-y\", \"title\": \"Category\", \"type\": \"nominal\"}, \"y\": {\"field\": \"count\", \"title\": \"Count of papers\", \"type\": \"quantitative\"}}}, {\"data\": {\"values\": [{\"category\": \"Evaluation / user study\", \"count\": 48, \"pct\": 57.8, \"label\": \"48 (57.8%)\"}, {\"category\": \"Benchmark / dataset release\", \"count\": 12, \"pct\": 14.5, \"label\": \"12 (14.5%)\"}, {\"category\": \"Code / open-source artifacts\", \"count\": 8, \"pct\": 9.6, \"label\": \"8 (9.6%)\"}, {\"category\": \"GraphicsReplicabilityStamp present\", \"count\": 0, \"pct\": 0.0, \"label\": \"0 (0.0%)\"}, {\"category\": \"Any of (eval/dataset/code)\", \"count\": 51, \"pct\": 61.4, \"label\": \"51 (61.4%)\"}]}, \"mark\": {\"type\": \"text\", \"color\": \"black\", \"dy\": -8}, \"encoding\": {\"text\": {\"field\": \"label\", \"type\": \"nominal\"}, \"x\": {\"field\": \"category\", \"sort\": \"-y\", \"type\": \"nominal\"}, \"y\": {\"field\": \"count\", \"type\": \"quantitative\"}}}], \"height\": 250, \"width\": 700}, {\"layer\": [{\"data\": {\"values\": [{\"label\": \"Evaluation only\", \"count\": 34}, {\"label\": \"Dataset only\", \"count\": 1}, {\"label\": \"Code only\", \"count\": 2}, {\"label\": \"Eval & Code\", \"count\": 3}, {\"label\": \"Eval & Dataset\", \"count\": 8}, {\"label\": \"Code & Dataset\", \"count\": 0}, {\"label\": \"All three\", \"count\": 3}, {\"label\": \"None of the three\", \"count\": 32}]}, \"mark\": {\"type\": \"bar\"}, \"encoding\": {\"color\": {\"field\": \"label\", \"legend\": null, \"type\": \"nominal\"}, \"x\": {\"field\": \"count\", \"title\": \"Count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"label\", \"sort\": \"-x\", \"title\": \"Overlap category\", \"type\": \"nominal\"}}}, {\"data\": {\"values\": [{\"label\": \"Evaluation only\", \"count\": 34}, {\"label\": \"Dataset only\", \"count\": 1}, {\"label\": \"Code only\", \"count\": 2}, {\"label\": \"Eval & Code\", \"count\": 3}, {\"label\": \"Eval & Dataset\", \"count\": 8}, {\"label\": \"Code & Dataset\", \"count\": 0}, {\"label\": \"All three\", \"count\": 3}, {\"label\": \"None of the three\", \"count\": 32}]}, \"mark\": {\"type\": \"text\", \"color\": \"black\", \"dx\": 5}, \"encoding\": {\"text\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"x\": {\"field\": \"count\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"label\", \"sort\": \"-x\", \"type\": \"nominal\"}}}], \"height\": 300, \"width\": 700}], \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\"};\n      var embedOpt = {\"mode\": \"vega-lite\"};\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis_69bfe2b6\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n\n  </script>\n"
                    },
                    "facts": "### Begin of facts\nReliable regexes to detect categories (apply over Title+Abstract+AuthorKeywords):\n- Evaluation / user study regex: 'user[ -]?study|user[ -]?evaluation|usability (?:study|evaluation)|experiment|empirical|evaluate(?:d|ion)?|controlled study|pilot study|field study|mturk|amazon mechanical turk|crowd?sourc(?:e|ed)'\n- Benchmark / dataset release regex: 'benchmark|data ?set|dataset|corpus|corpora|data release|data released|we (?:release|released)|public dataset|open dataset|shared dataset|dataset available|data available'\n- Code / open-source artifact regex: 'code|source ?code|open[ -]?source|github\\\\.com|gitlab\\\\.com|bitbucket\\\\.org|repository|implementation available|artifact(?: available)?|software available|demo web|online demo|package|pip|npm|docker'\nNotes: case-insensitive; search for whole phrases like \"user study\", \"dataset\", \"github.com\", \"open source\", etc.\n### End of facts\n### Begin of facts\nAutoVis papers found: 80\nPapers with evaluation/user-study signals: 37 (46.2%)\nPapers with benchmark/dataset signals: 30 (37.5%)\nPapers with code/open-source signals: 8 (10.0%)\nPapers with non-empty GraphicsReplicabilityStamp: 0 (0.0%)\n### End of facts\n### Begin of facts\nDownloads_Xplore distribution (AutoVis subset):\n- median: 937, 75th pct: 1791, 90th pct: 2673, max: 4884\nRecommended Downloads_Xplore cutoff suggestions for manual triage:\n- Consider > median as moderately visible; >75th percentile as high visibility; >90th percentile as top-tier papers to prioritize for manual checks.\n\nGraphicsReplicabilityStamp values observed (value:count):\n- \"\": 80\n\nRecommendation for stamp handling:\n- Treat non-empty stamp as evidence of some reported reproducibility information.\n- Do NOT assume exact meaning of codes (e.g., \"TT\"); map stamp tokens to semantics after manual inspection (common approach: T* => tools/artifacts, R* => raw data, M* => methods).\n### End of facts\n### Begin of facts\nSample papers (Title, DOI, Year, Downloads_Xplore, CitationCount_CrossRef) for manual validation:\n\n-- Evaluation samples --\n- ('Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco', '10.1109/tvcg.2018.2865240', '2018', 3238.0, 177.0)\n- ('Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication', '10.1109/tvcg.2018.2865145', '2018', 2942.0, 121.0)\n- ('Finding Waldo: Learning about Users from their Interactions', '10.1109/tvcg.2014.2346575', '2014', 2226.0, 95.0)\n\n-- Benchmark/dataset samples --\n- ('Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication', '10.1109/tvcg.2018.2865145', '2018', 2942.0, 121.0)\n- ('FAIRVIS: Visual Analytics for Discovering Intersectional Bias in Machine Learning', '10.1109/vast47406.2019.8986948', '2019', 2108.0, 106.0)\n- ('Calliope: Automatic Visual Data Story Generation from a Spreadsheet', '10.1109/tvcg.2020.3030403', '2020', 3724.0, 80.0)\n\n-- Code/open-source samples --\n- ('Data-Driven Guides: Supporting Expressive Design for Information Graphics', '10.1109/tvcg.2016.2598620', '2016', 2245.0, 92.0)\n- ('Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data Based on User-Authored Annotations', '10.1109/tvcg.2016.2598543', '2016', 2188.0, 33.0)\n- ('NeuroCartography: Scalable Automatic Visual Summarization of Concepts in Deep Neural Networks', '10.1109/tvcg.2021.3114858', '2021', 830.0, 15.0)\n### End of facts\n",
                    "text": "A small evidence-summary chart shows that while a majority of papers include some evaluation signal (roughly 46\u201358% depending on detection), far fewer release datasets (\u224814%) or code artifacts (\u224810%), and none in our filtered subset carry a GraphicsReplicabilityStamp. Overlap analysis indicates many papers provide evaluation but not code or dataset artifacts; only a small minority provide the full triad (evaluation + data + code). From these numbers we recommend concrete next steps: require artifact deposition on submission (code + data + runnable examples), run community-held benchmarking challenges for NL2VIS and mixed-initiative tasks, adopt lightweight replicability stamps for Vis publications, and fund collaborative, multi-site user studies to produce more comparable evaluation evidence that future meta-analyses can depend on."
                }
            ]
        }
    ],
    "global_filter_state": {
        "description": "Select all papers that mention automated visualization concepts (automatic/automated vis, visualization recommendation, mixed-initiative, visualization generation, vis generation, agent) in their Title, Abstract, or AuthorKeywords to support analysis of what happened to research on automated visualization. Returns all columns and orders results by Year ascending.",
        "sql_query": "SELECT *\nFROM Papers\nWHERE (\n  LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automatic vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%automated vis%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization recommendation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%mixed-initiative%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%visualization generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%vis generation%'\n  OR LOWER(COALESCE(AuthorKeywords, '') || ' ' || COALESCE(Abstract, '') || ' ' || COALESCE(Title, '')) LIKE '%agent%'\n)\nORDER BY Year ASC;",
        "dataset_path": "outputs_sync/vis_report/thread_20250827_213106/dataset_global_filtered.csv"
    }
}